{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32437641-9b2e-44e8-9df6-cc08a9e6322a",
   "metadata": {},
   "source": [
    "# MDP 马尔可夫决策过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d6b76-f399-47be-9405-9c38c4cb2f19",
   "metadata": {},
   "source": [
    "本章给大家介绍马尔可夫决策过程。\n",
    "\n",
    "* 在介绍马尔可夫决策过程之前，先介绍它的简化版本：马尔可夫链以及马尔可夫奖励过程，通过跟这两种过程的比较，我们可以更容易理解马尔可夫决策过程。\n",
    "* 第二部分会介绍马尔可夫决策过程中的 `policy evaluation`，就是当给定一个决策过后，怎么去计算它的价值函数。\n",
    "* 第三部分会介绍马尔可夫决策过程的控制，具体有两种算法：`policy iteration` 和 `value iteration`。\n",
    "\n",
    "![](img/2.2.png)\n",
    "\n",
    "上图介绍了在强化学习里面 agent 跟 environment 之间的交互，agent 在得到环境的状态过后，它会采取动作，它会把这个采取的动作返还给环境。环境在得到 agent 的动作过后，它会进入下一个状态，把下一个状态传回 agent。在强化学习中，agent 跟环境就是这样进行交互的，这个交互过程是可以通过马尔可夫决策过程来表示的，所以马尔可夫决策过程是强化学习里面的一个基本框架。\n",
    "\n",
    "在马尔可夫决策过程中，它的环境是全部可以观测的(`fully observable`)。但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成一个 MDP 的问题。\n",
    "\n",
    "在介绍马尔可夫决策过程(Markov Decision Process，MDP)之前，先给大家梳理一下马尔可夫过程(Markov Process，MP)、马尔可夫奖励过程(Markov Reward Processes，MRP)。这两个过程是马尔可夫决策过程的一个基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02aa30-5f4c-4e07-a6b1-d5409a66fdc4",
   "metadata": {},
   "source": [
    "## Markov Process(MP)\n",
    "\n",
    "### Markov Property\n",
    "\n",
    "如果一个状态转移是符合马尔可夫的，那就是说一个状态的下一个状态只取决于它当前状态，而跟它当前状态之前的状态都没有关系。\n",
    "\n",
    "我们设状态的历史为 $h_{t}=\\left\\{s_{1}, s_{2}, s_{3}, \\ldots, s_{t}\\right\\}$（$h_t$ 包含了之前的所有状态），如果一个状态转移是符合马尔可夫的，也就是满足如下条件：\n",
    "$$\n",
    "p\\left(s_{t+1} \\mid s_{t}\\right) =p\\left(s_{t+1} \\mid h_{t}\\right) \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p\\left(s_{t+1} \\mid s_{t}, a_{t}\\right) =p\\left(s_{t+1} \\mid h_{t}, a_{t}\\right) \\tag{2}\n",
    "$$\n",
    "\n",
    "从当前 $s_t$ 转移到 $s_{t+1}$ 这个状态，它是直接就等于它之前所有的状态转移到 $s_{t+1}$。如果某一个过程满足`马尔可夫性质(Markov Property)`，就是说未来的转移跟过去是独立的，它只取决于现在。**马尔可夫性质是所有马尔可夫过程的基础。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6e4d3-b15c-427f-a952-0625047d5031",
   "metadata": {},
   "source": [
    "### Markov Process/Markov Chain\n",
    "\n",
    "![](img/2.5.png ':size=500')\n",
    "\n",
    "首先看一看`马尔可夫链(Markov Chain)`。举个例子，这个图里面有四个状态，这四个状态从 $s_1,s_2,s_3,s_4$ 之间互相转移。比如说从 $s_1$ 开始，\n",
    "\n",
    "*  $s_1$ 有 0.1 的概率继续存活在 $s_1$ 状态，\n",
    "* 有 0.2 的概率转移到 $s_2$， \n",
    "* 有 0.7 的概率转移到 $s_4$ 。\n",
    "\n",
    "如果 $s_4$ 是我们当前状态的话，\n",
    "\n",
    "* 它有 0.3 的概率转移到 $s_2$ ，\n",
    "* 有 0.2 的概率转移到 $s_3$ ，\n",
    "* 有 0.5 的概率留在这里。\n",
    "\n",
    "我们可以用`状态转移矩阵(State Transition Matrix)` $P$ 来描述状态转移 $p\\left(s_{t+1}=s^{\\prime} \\mid s_{t}=s\\right)$，如下式所示。\n",
    "$$\n",
    "P=\\left[\\begin{array}{cccc}\n",
    "P\\left(s_{1} \\mid s_{1}\\right) & P\\left(s_{2} \\mid s_{1}\\right) & \\ldots & P\\left(s_{N} \\mid s_{1}\\right) \\\\\n",
    "P\\left(s_{1} \\mid s_{2}\\right) & P\\left(s_{2} \\mid s_{2}\\right) & \\ldots & P\\left(s_{N} \\mid s_{2}\\right) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "P\\left(s_{1} \\mid s_{N}\\right) & P\\left(s_{2} \\mid s_{N}\\right) & \\ldots & P\\left(s_{N} \\mid s_{N}\\right)\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "状态转移矩阵类似于一个 conditional probability，当我们知道当前我们在 $s_t$ 这个状态过后，到达下面所有状态的一个概念。所以它每一行其实描述了是从一个节点到达所有其它节点的概率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7bf0bf-2bab-4739-afc8-39fbc7857ad8",
   "metadata": {},
   "source": [
    "### Example of MP\n",
    "\n",
    "![](img/2.6.png)\n",
    "\n",
    "上图是一个马尔可夫链的例子，我们这里有七个状态。比如说从 $s_1$ 开始到 $s_2$ ，它有 0.4 的概率，然后它有 0.6 的概率继续存活在它当前的状态。 $s_2$ 有 0.4 的概率到左边，有 0.4 的概率到 $s_3$ ，另外有 0.2 的概率存活在现在的状态，所以给定了这个状态转移的马尔可夫链后，我们可以对这个链进行采样，这样就会得到一串的轨迹。\n",
    "\n",
    "下面我们有三个轨迹，都是从同一个起始点开始。假设还是从 $s_3$ 这个状态开始，\n",
    "\n",
    "* 第一条链先到了 $s_4$， 又到了 $s_5$，又往右到了 $s_6$ ，然后继续存活在 $s_6$ 状态。\n",
    "* 第二条链从 $s_3$ 开始，先往左走到了 $s_2$ 。然后它又往右走，又回到了$s_3$ ，然后它又往左走，然后再往左走到了 $s_1$ 。\n",
    "* 通过对这个状态的采样，我们生成了很多这样的轨迹。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d7705-9006-41b7-921f-dbc9f43e6536",
   "metadata": {},
   "source": [
    "## Markov Reward Process(MRP)\n",
    "\n",
    "**`马尔可夫奖励过程(Markov Reward Process, MRP)` 是马尔可夫链再加上了一个奖励函数。**在 MRP 中，转移矩阵和状态都是跟马尔可夫链一样的，多了一个`奖励函数(reward function)`。**奖励函数 $R$ 是一个期望**，就是说当你到达某一个状态的时候，可以获得多大的奖励，然后这里另外定义了一个 discount factor $\\gamma$ 。如果状态数是有限的，$R$ 可以是一个向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bec5c-58f6-4bd1-8e73-3961c9e78225",
   "metadata": {},
   "source": [
    "### Example of MRP\n",
    "\n",
    "![](img/2.8.png)\n",
    "\n",
    "这里是我们刚才看的马尔可夫链，如果把奖励也放上去的话，就是说到达每一个状态，我们都会获得一个奖励。这里我们可以设置对应的奖励，比如说到达 $s_1$ 状态的时候，可以获得 5 的奖励，到达 $s_7$ 的时候，可以得到 10 的奖励，其它状态没有任何奖励。因为这里状态是有限的，所以我们可以用向量 $R=[5,0,0,0,0,0,10]$ 来表示这个奖励函数，这个向量表示了每个点的奖励大小。\n",
    "\n",
    "我们通过一个形象的例子来理解 MRP。我们把一个纸船放到河流之中，那么它就会随着这个河流而流动，它自身是没有动力的。所以你可以把 MRP 看成是一个随波逐流的例子，当我们从某一个点开始的时候，这个纸船就会随着事先定义好的状态转移进行流动，它到达每个状态过后，我们就有可能获得一些奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359110d-682d-4886-a243-90221f2e6f7e",
   "metadata": {},
   "source": [
    "### Return and Value function\n",
    "\n",
    "这里我们进一步定义一些概念。\n",
    "\n",
    "*  `Horizon` 是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的。\n",
    "\n",
    "* `Return(回报)` 说的是把奖励进行折扣后所获得的收益。Return 可以定义为奖励的逐步叠加，如下式所示：\n",
    "\n",
    "$$\n",
    "G_{t}=R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\gamma^{3} R_{t+4}+\\ldots+\\gamma^{T-t-1} R_{T}\n",
    "$$\n",
    "\n",
    "这里有一个叠加系数，越往后得到的奖励，折扣得越多。这说明我们其实更希望得到现有的奖励，未来的奖励就要把它打折扣。\n",
    "\n",
    "* 当我们有了 return 过后，就可以定义一个状态的价值了，就是 `state value function`。对于 MRP，state value function 被定义成是 return 的期望，如下式所示：\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  V_{t}(s) &=\\mathbb{E}\\left[G_{t} \\mid s_{t}=s\\right] \\\\\n",
    "  &=\\mathbb{E}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots+\\gamma^{T-t-1} R_{T} \\mid s_{t}=s\\right]\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "$G_t$ 是之前定义的 `discounted return`，我们这里取了一个期望，期望就是说从这个状态开始，你有可能获得多大的价值。所以这个期望也可以看成是对未来可能获得奖励的当前价值的一个表现，就是当你进入某一个状态过后，你现在就有多大的价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5cd253-74c2-4343-8d29-786e1c807bf6",
   "metadata": {},
   "source": [
    "### Why Discount Factor\n",
    "\n",
    "**这里解释一下为什么需要 discount factor。**\n",
    "\n",
    "* 有些马尔可夫过程是带环的，它并没有终结，我们想避免这个无穷的奖励。\n",
    "* 我们并没有建立一个完美的模拟环境的模型，也就是说，我们对未来的评估不一定是准确的，我们不一定完全信任我们的模型，因为这种不确定性，所以我们对未来的预估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。\n",
    "* 如果这个奖励是有实际价值的，我们可能是更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。\n",
    "* 在人的行为里面来说的话，大家也是想得到即时奖励。\n",
    "* 有些时候可以把这个系数设为 0，$\\gamma=0$：我们就只关注了它当前的奖励。我们也可以把它设为 1，$\\gamma=1$：对未来并没有折扣，未来获得的奖励跟当前获得的奖励是一样的。\n",
    "\n",
    "Discount factor 可以作为强化学习 agent 的一个超参数来进行调整，然后就会得到不同行为的 agent。\n",
    "\n",
    "![](img/2.11.png)\n",
    "\n",
    "这里我们再来看一看，在这个 MRP 里面，如何计算它的价值。这个 MRP 依旧是这个状态转移。它的奖励函数是定义成这样，它在进入第一个状态的时候会得到 5 的奖励，进入第七个状态的时候会得到 10 的奖励，其它状态都没有奖励。\n",
    "\n",
    "我们现在可以计算每一个轨迹得到的奖励，比如我们对于这个 $s_4,s_5,s_6,s_7$ 轨迹的奖励进行计算，这里折扣系数是 0.5。\n",
    "\n",
    "* 在 $s_4$ 的时候，奖励为零。\n",
    "\n",
    "* 下一个状态 $s_5$ 的时候，因为我们已经到了下一步了，所以我们要把 $s_5$ 进行一个折扣，$s_5$ 本身也是没有奖励的。\n",
    "* 然后是到 $s_6$，也没有任何奖励，折扣系数应该是 $\\frac{1}{4}$ 。\n",
    "* 到达 $s_7$ 后，我们获得了一个奖励，但是因为 $s_7$ 这个状态是未来才获得的奖励，所以我们要进行三次折扣。\n",
    "\n",
    "所以对于这个轨迹，它的 return 就是一个 1.25，类似地，我们可以得到其它轨迹的 return 。\n",
    "\n",
    "这里就引出了一个问题，当我们有了一些轨迹的实际 return，怎么计算它的价值函数。比如说我们想知道 $s_4$ 状态的价值，就是当你进入 $s_4$ 后，它的价值到底如何。一个可行的做法就是说我们可以产生很多轨迹，然后把这里的轨迹都叠加起来。比如我们可以从 $s_4$ 开始，采样生成很多轨迹，都把它的 return 计算出来，然后可以直接把它取一个平均作为你进入 $s_4$ 它的价值。这其实是一种计算价值函数的办法，通过这个蒙特卡罗采样的办法计算 $s_4$ 的状态。接下来会进一步介绍蒙特卡罗算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c803b43-49c1-4510-9c24-a6a7e7d77aee",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "\n",
    "![](img/2.12.png)\n",
    "\n",
    "但是这里我们采取了另外一种计算方法，我们从这个价值函数里面推导出 `Bellman Equation（贝尔曼等式）`，如下式所示：\n",
    "$$\n",
    "V(s)=\\underbrace{R(s)}_{\\text {Immediate reward }}+\\underbrace{\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s\\right) V\\left(s^{\\prime}\\right)}_{\\text {Discounted sum of future reward }}\n",
    "$$\n",
    "其中：\n",
    "\n",
    "*  $s'$ 可以看成未来的所有状态。\n",
    "* 转移 $P(s'|s)$  是指从当前状态转移到未来状态的概率。\n",
    "* $V(s')$ 代表的是未来某一个状态的价值。我们从当前这个位置开始，有一定的概率去到未来的所有状态，所以我们要把这个概率也写上去，这个转移矩阵也写上去，然后我们就得到了未来状态，然后再乘以一个 $\\gamma$，这样就可以把未来的奖励打折扣。\n",
    "* 第二部分可以看成是未来奖励的折扣总和(Discounted sum of future reward)。\n",
    "\n",
    "**Bellman Equation 定义了当前状态跟未来状态之间的这个关系。**\n",
    "\n",
    "未来打了折扣的奖励加上当前立刻可以得到的奖励，就组成了这个 Bellman Equation。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e464dd-282e-460b-bfe8-70cf38edf2eb",
   "metadata": {},
   "source": [
    "#### Law of Total Expectation\n",
    "\n",
    "在推导 Bellman equation 之前，我们可以仿照`Law of Total Expectation(全期望公式)`的证明过程来证明下面的式子：\n",
    "$$\n",
    "\\mathbb{E}[V(s_{t+1})|s_t]=\\mathbb{E}[\\mathbb{E}[G_{t+1}|s_{t+1}]|s_t]=E[G_{t+1}|s_t]\n",
    "$$\n",
    "\n",
    "> Law of total expectation 也被称为 law of iterated expectations(LIE)。如果 $A_i$ 是样本空间的有限或可数的划分(partition)，则全期望公式可以写成如下形式：\n",
    "> $$\n",
    "> \\mathrm{E}(X)=\\sum_{i} \\mathrm{E}\\left(X \\mid A_{i}\\right) \\mathrm{P}\\left(A_{i}\\right)\n",
    "> $$\n",
    "\n",
    "**证明：**\n",
    "\n",
    "为了记号简洁并且易读，我们丢掉了下标，令 $s=s_t,g'=G_{t+1},s'=s_{t+1}$。我们可以根据条件期望的定义来重写这个回报的期望为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[G_{t+1} \\mid s_{t+1}\\right] &=\\mathbb{E}\\left[g^{\\prime} \\mid s^{\\prime}\\right] \\\\\n",
    "&=\\sum_{g^{\\prime}} g^{\\prime}~p\\left(g^{\\prime} \\mid s^{\\prime}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "> 如果 $X$ 和 $Y$ 都是离散型随机变量，则条件期望（Conditional Expectation）$E(X|Y=y)$的定义如下式所示：\n",
    "> $$\n",
    "> \\mathrm{E}(X \\mid Y=y)=\\sum_{x} x P(X=x \\mid Y=y)\n",
    "> $$\n",
    "\n",
    "令 $s_t=s$，我们对 $\\mathbb{E}\\left[G_{t+1} \\mid s_{t+1}\\right]$ 求期望可得：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[\\mathbb{E}\\left[G_{t+1} \\mid s_{t+1}\\right] \\mid s_{t}\\right] \n",
    "&=\\mathbb{E} \\left[\\mathbb{E}\\left[g^{\\prime} \\mid s^{\\prime}\\right] \\mid s\\right]\\\\\n",
    "&=\\mathbb{E} \\left[\\sum_{g^{\\prime}} g^{\\prime}~p\\left(g^{\\prime} \\mid s^{\\prime}\\right)\\mid s\\right]\\\\\n",
    "&= \\sum_{s^{\\prime}}\\sum_{g^{\\prime}} g^{\\prime}~p\\left(g^{\\prime} \\mid s^{\\prime},s\\right)p(s^{\\prime} \\mid s)\\\\\n",
    "&=\\sum_{s^{\\prime}} \\sum_{g^{\\prime}} \\frac{g^{\\prime} p\\left(g^{\\prime} \\mid s^{\\prime}, s\\right) p\\left(s^{\\prime} \\mid s\\right) p(s)}{p(s)} \\\\\n",
    "&=\\sum_{s^{\\prime}} \\sum_{g^{\\prime}} \\frac{g^{\\prime} p\\left(g^{\\prime} \\mid s^{\\prime}, s\\right) p\\left(s^{\\prime}, s\\right)}{p(s)} \\\\\n",
    "&=\\sum_{s^{\\prime}} \\sum_{g^{\\prime}} \\frac{g^{\\prime} p\\left(g^{\\prime}, s^{\\prime}, s\\right)}{p(s)} \\\\\n",
    "&=\\sum_{s^{\\prime}} \\sum_{g^{\\prime}} g^{\\prime} p\\left(g^{\\prime}, s^{\\prime} \\mid s\\right) \\\\\n",
    "&=\\sum_{g^{\\prime}} \\sum_{s^{\\prime}} g^{\\prime} p\\left(g^{\\prime}, s^{\\prime} \\mid s\\right) \\\\\n",
    "&=\\sum_{g^{\\prime}} g^{\\prime} p\\left(g^{\\prime} \\mid s\\right) \\\\\n",
    "&=\\mathbb{E}\\left[g^{\\prime} \\mid s\\right]=\\mathbb{E}\\left[G_{t+1} \\mid s_{t}\\right]\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c144db-1963-4282-ad94-891a719bb540",
   "metadata": {},
   "source": [
    "#### Bellman Equation Derivation\n",
    "\n",
    "Bellman equation 的推导过程如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V(s)&=\\mathbb{E}\\left[G_{t} \\mid s_{t}=s\\right]\\\\\n",
    "&=\\mathbb{E}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid s_{t}=s\\right]  \\\\\n",
    "&=\\mathbb{E}\\left[R_{t+1}|s_t=s\\right] +\\gamma \\mathbb{E}\\left[R_{t+2}+\\gamma R_{t+3}+\\gamma^{2} R_{t+4}+\\ldots \\mid s_{t}=s\\right]\\\\\n",
    "&=R(s)+\\gamma \\mathbb{E}[G_{t+1}|s_t=s] \\\\\n",
    "&=R(s)+\\gamma \\mathbb{E}[V(s_{t+1})|s_t=s]\\\\\n",
    "&=R(s)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s\\right) V\\left(s^{\\prime}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    ">Bellman Equation 就是当前状态与未来状态的迭代关系，表示当前状态的值函数可以通过下个状态的值函数来计算。Bellman Equation 因其提出者、动态规划创始人 Richard Bellman 而得名 ，也叫作“动态规划方程”。\n",
    "\n",
    "**Bellman Equation 定义了状态之间的迭代关系，如下式所示。**\n",
    "$$\n",
    "V(s)=R(s)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s\\right) V\\left(s^{\\prime}\\right)\n",
    "$$\n",
    "![](img/2.13.png)\n",
    "\n",
    "假设有一个马尔可夫转移矩阵是右边这个样子，Bellman Equation 描述的就是当前状态到未来状态的一个转移。假设我们当前是在 $s_1$， 那么它只可能去到三个未来的状态：有 0.1 的概率留在它当前这个位置，有 0.2 的概率去到 $s_2$ 状态，有 0.7 的概率去到 $s_4$ 的状态，所以我们要把这个转移乘以它未来的状态的价值，再加上它的 immediate reward 就会得到它当前状态的价值。**所以 Bellman Equation 定义的就是当前状态跟未来状态的一个迭代的关系。**\n",
    "\n",
    "我们可以把 Bellman Equation 写成一种矩阵的形式，如下式所示。\n",
    "$$\n",
    "\\left[\\begin{array}{c}\n",
    "V\\left(s_{1}\\right) \\\\\n",
    "V\\left(s_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "V\\left(s_{N}\\right)\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "R\\left(s_{1}\\right) \\\\\n",
    "R\\left(s_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "R\\left(s_{N}\\right)\n",
    "\\end{array}\\right]+\\gamma\\left[\\begin{array}{cccc}\n",
    "P\\left(s_{1} \\mid s_{1}\\right) & P\\left(s_{2} \\mid s_{1}\\right) & \\ldots & P\\left(s_{N} \\mid s_{1}\\right) \\\\\n",
    "P\\left(s_{1} \\mid s_{2}\\right) & P\\left(s_{2} \\mid s_{2}\\right) & \\ldots & P\\left(s_{N} \\mid s_{2}\\right) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "P\\left(s_{1} \\mid s_{N}\\right) & P\\left(s_{2} \\mid s_{N}\\right) & \\ldots & P\\left(s_{N} \\mid s_{N}\\right)\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "V\\left(s_{1}\\right) \\\\\n",
    "V\\left(s_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "V\\left(s_{N}\\right)\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "首先有这个转移矩阵。我们当前这个状态是一个向量  $[V(s_1),V(s_2),\\cdots,V(s_N)]^T$。我们可以写成迭代的形式。我们每一行来看的话，$V$ 这个向量乘以了转移矩阵里面的某一行，再加上它当前可以得到的 reward，就会得到它当前的价值。\n",
    "\n",
    "当我们把 Bellman Equation 写成矩阵形式后，可以直接求解：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V &= R+ \\gamma PV \\\\\n",
    "IV &= R+ \\gamma PV \\\\\n",
    "(I-\\gamma P)V &=R \\\\\n",
    "V&=(I-\\gamma P)^{-1}R\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "我们可以直接得到一个`解析解(analytic solution)`:\n",
    "$$\n",
    "V=(I-\\gamma P)^{-1} R\n",
    "$$\n",
    "我们可以通过矩阵求逆把这个 V 的这个价值直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是 $O(N^3)$。所以当状态非常多的时候，比如说从十个状态到一千个状态，到一百万个状态。那么当我们有一百万个状态的时候，这个转移矩阵就会是个一百万乘以一百万的矩阵，这样一个大矩阵的话求逆是非常困难的，**所以这种通过解析解去求解的方法只适用于很小量的 MRP。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f91ab-07c0-42ab-86dc-f02835ab45bc",
   "metadata": {},
   "source": [
    "### Iterative Algorithm for Computing Value of a MRP\n",
    "\n",
    "接下来我们来求解这个价值函数。**我们可以通过迭代的方法来解这种状态非常多的 MRP(large MRPs)，**比如说：\n",
    "\n",
    "* 动态规划的方法，\n",
    "* 蒙特卡罗的办法(通过采样的办法去计算它)，\n",
    "* 时序差分学习(Temporal-Difference Learning)的办法。 `Temporal-Difference Learning` 叫 `TD Leanring`，它是动态规划和蒙特卡罗的一个结合。\n",
    "\n",
    "![](img/2.16.png)\n",
    "\n",
    "**首先我们用蒙特卡罗(Monte Carlo)的办法来计算它的价值函数。**蒙特卡罗就是说当得到一个 MRP 过后，我们可以从某一个状态开始，把这个小船放进去，让它随波逐流，这样就会产生一个轨迹。产生了一个轨迹过后，就会得到一个奖励，那么就直接把它的折扣的奖励 $g$ 算出来。算出来过后就可以把它积累起来，得到 return $G_t$。 当积累到一定的轨迹数量过后，直接用 $G_t$ 除以轨迹数量，就会得到它的价值。\n",
    "\n",
    "比如说我们要算 $s_4$ 状态的价值。\n",
    "\n",
    "* 我们就可以从 $s_4$ 状态开始，随机产生很多轨迹，就是说产生很多小船，把小船扔到这个转移矩阵里面去，然后它就会随波逐流，产生轨迹。\n",
    "* 每个轨迹都会得到一个 return，我们得到大量的 return，比如说一百个、一千个 return ，然后直接取一个平均，那么就可以等价于现在 $s_4$ 这个价值，因为 $s_4$ 的价值 $V(s_4)$  定义了你未来可能得到多少的奖励。这就是蒙特卡罗采样的方法。\n",
    "\n",
    "![](img/2.17.png)\n",
    "\n",
    "**我们也可以用这个动态规划的办法**，一直去迭代它的 Bellman equation，让它最后收敛，我们就可以得到它的一个状态。所以在这里算法二就是一个迭代的算法，通过 `bootstrapping(自举)`的办法，然后去不停地迭代这个 Bellman Equation。当这个最后更新的状态跟你上一个状态变化并不大的时候，更新就可以停止，我们就可以输出最新的 $V'(s)$ 作为它当前的状态。所以这里就是把 Bellman Equation 变成一个 Bellman Update，这样就可以得到它的一个价值。\n",
    "\n",
    "动态规划的方法基于后继状态值的估计来更新状态值的估计（算法二中的第 3 行用 $V'$ 来更新 $V$ ）。也就是说，它们根据其他估算值来更新估算值。我们称这种基本思想为 bootstrapping。\n",
    "\n",
    ">Bootstrap 本意是“解靴带”；这里是在使用徳国文学作品《吹牛大王历险记》中解靴带自助(拔靴自助)的典故，因此将其译为“自举”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97bb28-708a-4d40-b6b4-776e2f0d19d5",
   "metadata": {},
   "source": [
    "## Markov Decision Process(MDP) 马尔科夫决策过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab9f19-4722-48af-a732-4950794d0f47",
   "metadata": {},
   "source": [
    "### MDP\n",
    "\n",
    "**相对于 MRP，`马尔可夫决策过程(Markov Decision Process)`多了一个 `decision`，其它的定义跟 MRP 都是类似的**:\n",
    "\n",
    "* 这里多了一个决策，多了一个动作。\n",
    "* 状态转移也多了一个条件，变成了 $P\\left(s_{t+1}=s^{\\prime} \\mid s_{t}=s, a_{t}=a\\right)$。你采取某一种动作，然后你未来的状态会不同。未来的状态不仅是依赖于你当前的状态，也依赖于在当前状态 agent 采取的这个动作。\n",
    "* 对于这个价值函数，它也是多了一个条件，多了一个你当前的这个动作，变成了 $R\\left(s_{t}=s, a_{t}=a\\right)=\\mathbb{E}\\left[r_{t} \\mid s_{t}=s, a_{t}=a\\right]$。你当前的状态以及你采取的动作会决定你在当前可能得到的奖励多少。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9134cf-5e1e-4d0f-a430-aaed37b02777",
   "metadata": {},
   "source": [
    "### Policy in MDP\n",
    "\n",
    "* Policy 定义了在某一个状态应该采取什么样的动作。\n",
    "\n",
    "* 知道当前状态过后，我们可以把当前状态带入 policy function，然后就会得到一个概率，即 \n",
    "$$\n",
    "\\pi(a \\mid s)=P\\left(a_{t}=a \\mid s_{t}=s\\right)\n",
    "$$\n",
    "\n",
    "概率就代表了在所有可能的动作里面怎样采取行动，比如可能有 0.7 的概率往左走，有 0.3 的概率往右走，这是一个概率的表示。\n",
    "\n",
    "* 另外这个策略也可能是确定的，它有可能是直接输出一个值。或者就直接告诉你当前应该采取什么样的动作，而不是一个动作的概率。\n",
    "\n",
    "* 假设这个概率函数应该是稳定的(stationary)，不同时间点，你采取的动作其实都是对这个 policy function 进行采样。\n",
    "\n",
    "我们可以将 MRP 转换成 MDP。已知一个 MDP 和一个 policy $\\pi$ 的时候，我们可以把 MDP 转换成 MRP。\n",
    "\n",
    "在 MDP 里面，转移函数 $P(s'|s,a)$  是基于它当前状态以及它当前的 action。因为我们现在已知它 policy function，就是说在每一个状态，我们知道它可能采取的动作的概率，那么就可以直接把这个 action 进行加和，直接把这个 a 去掉，那我们就可以得到对于 MRP 的一个转移，这里就没有 action。\n",
    "\n",
    "$$\n",
    " P^{\\pi}\\left(s^{\\prime} \\mid s\\right)=\\sum_{a \\in A} \\pi(a \\mid s) P\\left(s^{\\prime} \\mid s, a\\right)\n",
    "$$\n",
    "\n",
    "对于这个奖励函数，我们也可以把 action 拿掉，这样就会得到一个类似于 MRP 的奖励函数。\n",
    "\n",
    "$$\n",
    "R^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s) R(s, a)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe1c0a-09cf-46ff-bfae-15b4cc82e459",
   "metadata": {},
   "source": [
    "### Comparison of MP/MRP and MDP\n",
    "\n",
    "![](img/2.21.png)\n",
    "\n",
    "\n",
    "\n",
    "**这里我们看一看，MDP 里面的状态转移跟 MRP 以及 MP 的一个差异。**\n",
    "\n",
    "* 马尔可夫过程的转移是直接就决定。比如当前状态是 s，那么就直接通过这个转移概率决定了下一个状态是什么。\n",
    "* 但对于 MDP，它的中间多了一层这个动作 a ，就是说在你当前这个状态的时候，首先要决定的是采取某一种动作，那么你会到了某一个黑色的节点。到了这个黑色的节点，因为你有一定的不确定性，当你当前状态决定过后以及你当前采取的动作过后，你到未来的状态其实也是一个概率分布。**所以在这个当前状态跟未来状态转移过程中这里多了一层决策性，这是 MDP 跟之前的马尔可夫过程很不同的一个地方。**在马尔可夫决策过程中，动作是由 agent 决定，所以多了一个 component，agent 会采取动作来决定未来的状态转移。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892d3ef-5496-4f33-9dd1-1a8b22e77990",
   "metadata": {},
   "source": [
    "### Value function for MDP\n",
    "\n",
    "顺着 MDP 的定义，我们可以把 `状态-价值函数(state-value function)`，就是在 MDP 里面的价值函数也进行一个定义，它的定义是跟 MRP 是类似的，如式 (3)  所示：\n",
    "$$\n",
    "v^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid s_{t}=s\\right] \\tag{3}\n",
    "$$\n",
    "但是这里 expectation over policy，就是这个期望是基于你采取的这个 policy ，就当你的 policy 决定过后，**我们通过对这个 policy 进行采样来得到一个期望，那么就可以计算出它的这个价值函数。**\n",
    "\n",
    "这里我们另外引入了一个 `Q 函数(Q-function)`。Q 函数也被称为 `action-value function`。**Q 函数定义的是在某一个状态采取某一个动作，它有可能得到的这个 return 的一个期望**，如式 (4) 所示：\n",
    "$$\n",
    "q^{\\pi}(s, a)=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid s_{t}=s, A_{t}=a\\right] \\tag{4}\n",
    "$$\n",
    "这里期望其实也是 over policy function。所以你需要对这个 policy function 进行一个加和，然后得到它的这个价值。\n",
    "**对 Q 函数中的动作函数进行加和，就可以得到价值函数**，如式 (5) 所示：\n",
    "$$\n",
    "v^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s) q^{\\pi}(s, a) \\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267560a-653b-4b44-9a8f-35468c4d3167",
   "metadata": {},
   "source": [
    "#### Q-function Bellman Equation\n",
    "\n",
    "此处我们给出 Q 函数的 Bellman equation：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q(s,a)&=\\mathbb{E}\\left[G_{t} \\mid s_{t}=s,a_{t}=a\\right]\\\\\n",
    "&=\\mathbb{E}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid s_{t}=s,a_{t}=a\\right]  \\\\\n",
    "&=\\mathbb{E}\\left[R_{t+1}|s_{t}=s,a_{t}=a\\right] +\\gamma \\mathbb{E}\\left[R_{t+2}+\\gamma R_{t+3}+\\gamma^{2} R_{t+4}+\\ldots \\mid s_{t}=s,a_{t}=a\\right]\\\\\n",
    "&=R(s,a)+\\gamma \\mathbb{E}[G_{t+1}|s_{t}=s,a_{t}=a] \\\\\n",
    "&=R(s,a)+\\gamma \\mathbb{E}[V(s_{t+1})|s_{t}=s,a_{t}=a]\\\\\n",
    "&=R(s,a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s,a\\right) V\\left(s^{\\prime}\\right)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9860a57-6f7c-4be2-aaea-bb2cf7cd77ae",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation\n",
    "\n",
    "**我们可以把状态-价值函数和 Q 函数拆解成两个部分：即时奖励(immediate reward) 和后续状态的折扣价值(discounted value of successor state)。**\n",
    "\n",
    "通过对状态-价值函数进行一个分解，我们就可以得到一个类似于之前 MRP 的 Bellman Equation，这里叫 `Bellman Expectation Equation`，如式 (6) 所示：\n",
    "$$\n",
    "v^{\\pi}(s)=E_{\\pi}\\left[R_{t+1}+\\gamma v^{\\pi}\\left(s_{t+1}\\right) \\mid s_{t}=s\\right] \\tag{6}\n",
    "$$\n",
    "对于 Q 函数，我们也可以做类似的分解，也可以得到 Q 函数的 Bellman Expectation Equation，如式 (7) 所示：\n",
    "$$\n",
    "q^{\\pi}(s, a)=E_{\\pi}\\left[R_{t+1}+\\gamma q^{\\pi}\\left(s_{t+1}, A_{t+1}\\right) \\mid s_{t}=s, A_{t}=a\\right] \\tag{7}\n",
    "$$\n",
    "**Bellman expectation equation 定义了你当前状态跟未来状态之间的一个关联。**\n",
    "\n",
    "我们进一步进行一个简单的分解。\n",
    "\n",
    "我们先给出等式 (8)：\n",
    "$$\n",
    "v^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s) q^{\\pi}(s, a) \\tag{8}\n",
    "$$\n",
    "再给出等式 (9)：\n",
    "$$\n",
    "q^{\\pi}(s, a)=R_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right) \\tag{9}\n",
    "$$\n",
    "**等式 (8) 和等式 (9) 代表了价值函数跟 Q 函数之间的一个关联。**\n",
    "\n",
    "也可以把等式 (9) 插入等式 (8) 中，得到等式 (10)：\n",
    "$$\n",
    "v^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right)\\right) \\tag{10}\n",
    "$$\n",
    "**等式 (10) 代表了当前状态的价值跟未来状态价值之间的一个关联。**\n",
    "\n",
    "我们把等式 (8) 插入到等式 (9)，就可以得到等式 (11)：\n",
    "$$\n",
    "q^{\\pi}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) \\tag{11}\n",
    "$$\n",
    "**等式 (11) 代表了当前时刻的 Q 函数跟未来时刻的 Q 函数之间的一个关联。**\n",
    "\n",
    "**等式  (10) 和等式 (11)  是 Bellman expectation equation 的另一种形式。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9becd-8049-471c-a5f2-245ec2356093",
   "metadata": {},
   "source": [
    "### Backup Diagram\n",
    "\n",
    "![](img/2.25.png)\n",
    "\n",
    "这里有一个概念叫 `backup`。Backup 类似于 bootstrapping 之间这个迭代关系，就对于某一个状态，它的当前价值是跟它的未来价值线性相关的。\n",
    "\n",
    "我们把上面这样的图称为 `backup diagram(备份图)`，因为它们图示的关系构成了更新或备份操作的基础，而这些操作是强化学习方法的核心。这些操作将价值信息从一个状态（或状态-动作对）的后继状态（或状态-动作对）转移回它。\n",
    "\n",
    "每一个空心圆圈代表一个状态，每一个实心圆圈代表一个状态-动作对。\n",
    "\n",
    "\n",
    "$$\n",
    "v^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right)\\right) \\tag{12}\n",
    "$$\n",
    "如式 (12) 所示，我们这里有两层加和：\n",
    "\n",
    "* 第一层加和就是这个叶子节点，往上走一层的话，我们就可以把未来的价值($s'$ 的价值) backup 到黑色的节点。\n",
    "* 第二层加和是对 action 进行加和。得到黑色节点的价值过后，再往上 backup 一层，就会推到根节点的价值，即当前状态的价值。\n",
    "\n",
    "![](img/state_value_function_backup.png ':size=650')\n",
    "\n",
    "上图是状态-价值函数的计算分解图，上图 B 计算公式为\n",
    "$$\n",
    "v^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s) q^{\\pi}(s, a) \\tag{i}\n",
    "$$\n",
    "上图 B 给出了状态-价值函数与 Q 函数之间的关系。上图 C 计算 Q 函数为\n",
    "$$\n",
    "q^{\\pi}(s,a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right) \\tag{ii}\n",
    "$$\n",
    "\n",
    "将式 (ii) 代入式 (i) 可得：\n",
    "$$\n",
    "v^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right)\\right)\n",
    "$$\n",
    "**所以 backup diagram 定义了未来下一时刻的状态-价值函数跟上一时刻的状态-价值函数之间的关联。**\n",
    "\n",
    "![](img/2.26.png)\n",
    "\n",
    "对于 Q 函数，我们也可以进行这样的一个推导。现在的根节点是这个 Q 函数的一个节点。Q 函数对应于黑色的节点。我们下一时刻的 Q 函数是叶子节点，有四个黑色节点。\n",
    "$$\n",
    "q^{\\pi}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) \\tag{13}\n",
    "$$\n",
    "如式 (13) 所示，我们这里也有两个加和：\n",
    "\n",
    "* 第一层加和是先把这个叶子节点从黑色节点推到这个白色的节点，进了它的这个状态。\n",
    "* 当我们到达某一个状态过后，再对这个白色节点进行一个加和，这样就把它重新推回到当前时刻的一个 Q 函数。\n",
    "\n",
    "![](img/q_function_backup.png ':size=650')\n",
    "\n",
    "在上图 C 中，\n",
    "$$\n",
    "v^{\\pi}\\left(s^{\\prime}\\right)=\\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) \\tag{iii}\n",
    "$$\n",
    "将式 (iii) 代入式 (ii) 可得到 Q 函数：\n",
    "$$\n",
    "q^{\\pi}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\n",
    "$$\n",
    "**所以这个等式就决定了未来 Q 函数跟当前 Q 函数之间的这个关联。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0640c861-04cf-410f-9c99-37fe863b5298",
   "metadata": {},
   "source": [
    "### Policy Evaluation(Prediction)\n",
    "\n",
    "* 当我们知道一个 MDP 以及要采取的策略 $\\pi$ ，计算价值函数 $v^{\\pi}(s)$ 的过程就是 `policy evaluation`。就像我们在评估这个策略，我们会得到多大的奖励。\n",
    "* **Policy evaluation 在有些地方也被叫做 `(value) prediction`，也就是预测你当前采取的这个策略最终会产生多少的价值。**\n",
    "\n",
    "![](img/2.28.png)\n",
    "\n",
    "* MDP，你其实可以把它想象成一个摆渡的人在这个船上面，她可以控制这个船的移动，这样就避免了这个船随波逐流。因为在每一个时刻，这个人会决定采取什么样的一个动作，这样会把这个船进行导向。\n",
    "\n",
    "* MRP 跟 MP 的话，这个纸的小船会随波逐流，然后产生轨迹。\n",
    "* MDP 的不同就是有一个 agent 去控制这个船，这样我们就可以尽可能多地获得奖励。\n",
    "\n",
    "![](img/2.29.png)\n",
    "\n",
    "我们再看下 policy evaluation 的例子，怎么在决策过程里面计算它每一个状态的价值。\n",
    "\n",
    "* 假设环境里面有两种动作：往左走和往右走。\n",
    "* 现在的奖励函数有两个变量：动作和状态。但我们这里规定，不管你采取什么动作，只要到达状态 $s_1$，就有 5 的奖励。只要你到达状态 $s_7$ 了，就有 10 的奖励，中间没有任何奖励。\n",
    "* 假设我们现在采取的一个策略，这个策略是说不管在任何状态，我们采取的策略都是往左走。假设价值折扣因子是零，那么对于确定性策略(deterministic policy)，最后估算出的价值函数是一致的，即\n",
    "\n",
    "$$\n",
    "V^{\\pi}=[5,0,0,0,0,0,10]\n",
    "$$\n",
    "\n",
    "Q: 怎么得到这个结果？\n",
    "\n",
    "A: 我们可以直接在去 run 下面这个 iterative equation：\n",
    "$$\n",
    "v_{k}^{\\pi}(s)=r(s, \\pi(s))+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, \\pi(s)\\right) v_{k-1}^{\\pi}\\left(s^{\\prime}\\right)\n",
    "$$\n",
    "就把 Bellman expectation equation 拿到这边来，然后不停地迭代，最后它会收敛。收敛过后，它的值就是它每一个状态的价值。\n",
    "\n",
    "![](img/2.30.png)\n",
    "\n",
    "再来看一个例子(practice 1)，如果折扣因子是 0.5，我们可以通过下面这个等式进行迭代：\n",
    "$$\n",
    "v_{t}^{\\pi}(s)=\\sum_{a} P(\\pi(s)=a)\\left(r(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v_{t-1}^{\\pi}\\left(s^{\\prime}\\right)\\right)\n",
    "$$\n",
    "然后就会得到它的状态价值。\n",
    "\n",
    "另外一个例子(practice 2)，就是说我们现在采取的 policy 在每个状态下，有 0.5 的概率往左走，有 0.5 的概率往右走，那么放到这个状态里面去如何计算。其实也是把这个 Bellman expectation equation 拿出来，然后进行迭代就可以算出来了。一开始的时候，我们可以初始化，不同的 $v(s')$ 都会有一个值，放到 Bellman expectation equation 里面去迭代，然后就可以算出它的状态价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529cdcf-9d61-4e31-b331-a2c05bcd0af2",
   "metadata": {},
   "source": [
    "### Prediction and Control\n",
    "\n",
    "![](img/2.31.png)\n",
    "\n",
    "MDP 的 `prediction` 和 `control` 是 MDP 里面的核心问题。\n",
    "\n",
    "* 预测问题：\n",
    "  * 输入：MDP $<S,A,P,R,\\gamma>$ 和 policy $\\pi$  或者 MRP $<S,P^{\\pi},R^{\\pi},\\gamma>$。\n",
    "  * 输出：value function $v^{\\pi}$。\n",
    "  * Prediction 是说给定一个 MDP 以及一个 policy $\\pi$ ，去计算它的 value function，就对于每个状态，它的价值函数是多少。\n",
    "\n",
    "* 控制问题：\n",
    "  * 输入：MDP  $<S,A,P,R,\\gamma>$。\n",
    "  * 输出：最佳价值函数(optimal value function) $v^*$ 和最佳策略(optimal policy) $\\pi^*$。\n",
    "  * Control 就是说我们去寻找一个最佳的策略，然后同时输出它的最佳价值函数以及最佳策略。\n",
    "* 在 MDP 里面，prediction 和 control 都可以通过动态规划去解决。\n",
    "* 要强调的是，这两者的区别就在于，\n",
    "  * 预测问题是**给定一个 policy**，我们要确定它的 value function 是多少。\n",
    "  * 而控制问题是在**没有 policy 的前提下**，我们要确定最优的 value function 以及对应的决策方案。\n",
    "* **实际上，这两者是递进的关系，在强化学习中，我们通过解决预测问题，进而解决控制问题。**\n",
    "\n",
    "![](img/prediction_example.png)\n",
    "\n",
    "**举一个例子来说明 prediction 与 control 的区别。**\n",
    "\n",
    "首先是**预测问题**：\n",
    "\n",
    "* 在上图的方格中，我们规定从 A $\\to$ A' 可以得到 +10 的奖励，从 B $\\to$ B' 可以得到 +5 的奖励，其它步骤的奖励为 -1。\n",
    "* 现在，我们给定一个 policy：在任何状态中，它的行为模式都是随机的，也就是上下左右的概率各 25%。\n",
    "* 预测问题要做的就是，在这种决策模式下，我们的 value function 是什么。上图 b 是对应的 value function。\n",
    "\n",
    "![](img/control_example.png)\n",
    "\n",
    "\n",
    "\n",
    "接着是**控制问题**：\n",
    "\n",
    "* 在控制问题中，问题背景与预测问题相同，唯一的区别就是：不再限制 policy。也就是说行为模式是未知的，我们要自己确定。\n",
    "* 所以我们通过解决控制问题，求得每一个状态的最优的 value function（如上图 b 所示），也得到了最优的 policy（如上图 c 所示）。\n",
    "\n",
    "* 控制问题要做的就是，给定同样的条件，在所有可能的策略下最优的价值函数是什么？最优策略是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4e72e-2ffb-4aa4-a78a-007d79ec7666",
   "metadata": {},
   "source": [
    "### Dynamic Programming\n",
    "\n",
    "`动态规划(Dynamic Programming，DP)`适合解决满足如下两个性质的问题：\n",
    "\n",
    "* `最优子结构(optimal substructure)`。最优子结构意味着，我们的问题可以拆分成一个个的小问题，通过解决这个小问题，最后，我们能够通过组合小问题的答案，得到大问题的答案，即最优的解。\n",
    "* `重叠子问题(Overlapping subproblems)`。重叠子问题意味着，子问题出现多次，并且子问题的解决方案能够被重复使用。\n",
    "\n",
    "MDP 是满足动态规划的要求的，\n",
    "\n",
    "* 在 Bellman equation 里面，我们可以把它分解成一个递归的结构。当我们把它分解成一个递归的结构的时候，如果我们的子问题子状态能得到一个值，那么它的未来状态因为跟子状态是直接相连的，那我们也可以继续推算出来。\n",
    "* 价值函数就可以储存并重用它的最佳的解。\n",
    "\n",
    "动态规划应用于 MDP 的规划问题(planning)而不是学习问题(learning)，我们必须对环境是完全已知的(Model-Based)，才能做动态规划，直观的说，就是要知道状态转移概率和对应的奖励才行\n",
    "\n",
    "动态规划能够完成预测问题和控制问题的求解，是解 MDP prediction 和 control 一个非常有效的方式。\n",
    "\n",
    "### Policy Evaluation on MDP\n",
    "\n",
    "**Policy evaluation 就是给定一个 MDP 和一个 policy，我们可以获得多少的价值。**就对于当前这个策略，我们可以得到多大的 value function。\n",
    "\n",
    "这里有一个方法是说，我们直接把这个 `Bellman Expectation Backup` 拿过来，变成一个迭代的过程，这样反复迭代直到收敛。这个迭代过程可以看作是 `synchronous backup` 的过程。\n",
    "\n",
    "> 同步备份(synchronous backup)是指每一次的迭代都会完全更新所有的状态，这样对于程序资源需求特别大。异步备份(asynchronous backup)的思想就是通过某种方式，使得每一次迭代不需要更新所有的状态，因为事实上，很多的状态也不需要被更新。\n",
    "\n",
    "$$\n",
    "v_{t+1}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} P\\left(s^{\\prime} \\mid s, a\\right) v_{t}\\left(s^{\\prime}\\right)\\right) \\tag{14}\n",
    "$$\n",
    "* 等式 (14) 说的是说我们可以把 Bellman Expectation Backup 转换成一个动态规划的迭代。\n",
    "* 当我们得到上一时刻的 $v_t$ 的时候，就可以通过这个递推的关系来推出下一时刻的值。\n",
    "* 反复去迭代它，最后它的值就是从 $v_1,v_2$ 到最后收敛过后的这个值。这个值就是当前给定的 policy 对应的价值函数。\n",
    "\n",
    "Policy evaluation 的核心思想就是把如下式所示的 Bellman expectation backup 拿出来反复迭代，然后就会得到一个收敛的价值函数的值。\n",
    "$$\n",
    "v_{t+1}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} P\\left(s^{\\prime} \\mid s, a\\right) v_{t}\\left(s^{\\prime}\\right)\\right) \\tag{15}\n",
    "$$\n",
    "因为已经给定了这个函数的 policy  function，那我们可以直接把它简化成一个 MRP 的表达形式，这样的话，形式就更简洁一些，就相当于我们把这个 $a$  去掉，如下式所示：\n",
    "$$\n",
    "v_{t+1}(s)=R^{\\pi}(s)+\\gamma P^{\\pi}\\left(s^{\\prime} \\mid s\\right) v_{t}\\left(s^{\\prime}\\right) \\tag{16}\n",
    "$$\n",
    "这样它就只有价值函数跟转移函数了。通过去迭代这个更简化的一个函数，我们也可以得到它每个状态的价值。因为不管是在 MRP 以及 MDP，它的价值函数包含的这个变量都是只跟这个状态有关，就相当于进入某一个状态，未来可能得到多大的价值。\n",
    "\n",
    "![](img/2.35.png)\n",
    "\n",
    "* 比如现在的环境是一个 small gridworld。这个 agent 的目的是从某一个状态开始，然后到达终点状态。它的终止状态就是左上角跟右下角，这里总共有 14 个状态，因为我们把每个位置用一个状态来表示。\n",
    "* 这个 agent 采取的动作，它的 policy function 就直接先给定了，它在每一个状态都是随机游走，它们在每一个状态就是上下左右行走。它在边缘状态的时候，比如说在第四号状态的时候，它往左走的话，它是依然存在第四号状态，我们加了这个限制。\n",
    "\n",
    "* 这里我们给的奖励函数就是说你每走一步，就会得到 -1 的奖励，所以 agent 需要尽快地到达终止状态。\n",
    "* 状态之间的转移也是确定的。比如从第六号状态往上走，它就会直接到达第二号状态。很多时候有些环境是 `概率性的(probabilistic)`， 就是说 agent 在第六号状态，它选择往上走的时候，有可能地板是滑的，然后它可能滑到第三号状态或者第一号状态，这就是有概率的一个转移。但这里把这个环境进行了简化，从六号往上走，它就到了二号。\n",
    "* 所以直接用这个迭代来解它，因为我们已经知道每一个概率以及它的这个概率转移，那么就直接可以进行一个简短的迭代，这样就会算出它每一个状态的价值。\n",
    "\n",
    "![](img/2.36.png)\n",
    "\n",
    "我们再来看一个动态的例子，首先推荐斯坦福大学的一个网站：[GridWorld: Dynamic Programming Demo](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html) ，这个网站模拟了单步更新的过程中，所有格子的一个状态价值的变化过程。\n",
    "\n",
    "![](img/2.37.png ':size=550')\n",
    "\n",
    "这里有很多格子，每个格子都代表了一个状态。在每个格子里面有一个初始值零。然后在每一个状态，它还有一些箭头，这个箭头就是说它在当前这个状态应该采取什么样的策略。我们这里采取一个随机的策略，不管它在哪一个状态，它上下左右的概率都是相同的。比如在某个状态，它都有上下左右 0.25 的概率采取某一个动作，所以它的动作是完全随机的。\n",
    "\n",
    "在这样的环境里面，我们想计算它每一个状态的价值。我们也定义了它的 reward function，你可以看到有些状态上面有一个 R 的值。比如我们这边有些值是为负的，我们可以看到格子里面有几个 -1 的奖励，只有一个 +1 奖励的格子。在这个棋盘的中间这个位置，可以看到有一个 R 的值是 1.0，为正的一个价值函数。 所以每个状态对应了一个值，然后有一些状态没有任何值，就说明它的这个 reward function，它的奖励是为零的。\n",
    "\n",
    "![](img/2.38.png ':size=550')\n",
    "\n",
    "我们开始做这个 policy evaluation，policy evaluation 是一个不停迭代的过程。当我们初始化的时候，所有的 $v(s)$ 都是 0。我们现在迭代一次，迭代一次过后，你发现有些状态上面，值已经产生了变化。比如有些状态的值的 R 为 -1，迭代一次过后，它就会得到 -1 的这个奖励。对于中间这个绿色的，因为它的奖励为正，所以它是 +1 的状态。\n",
    "\n",
    "![](img/2.39.png ':size=550')\n",
    "\n",
    "所以当迭代第一次的时候，$v(s)$ 某些状态已经有些值的变化。\n",
    "\n",
    "![](img/2.40.png ':size=550')\n",
    "\n",
    "* 我们再迭代一次(one sweep)，然后发现它就从周围的状态也开始有值。因为周围状态跟之前有值的状态是临近的，所以它就相当于把旁边这个状态转移过来。所以当我们逐渐迭代的话，你会发现这个值一直在变换。\n",
    "\n",
    "* 等迭代了很多次过后，很远的这些状态的价值函数已经有些值了，而且你可以发现它这里整个过程呈现逐渐扩散开的一个过程，这其实也是 policy evaluation 的一个可视化。\n",
    "* 当我们每一步在进行迭代的时候，远的状态就会得到了一些值，就逐渐从一些已经有奖励的这些状态，逐渐扩散，当你 run 很多次过后，它就逐渐稳定下来，最后值就会确定不变，这样收敛过后，每个状态上面的值就是它目前得到的这个 value function 的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0139db-c4db-4e65-82c6-a14e753ae451",
   "metadata": {},
   "source": [
    "### MDP Control\n",
    "\n",
    "![](img/2.41.png)\n",
    "\n",
    "Policy evaluation 是说给定一个 MDP 和一个 policy，我们可以估算出它的价值函数。**还有问题是说如果我们只有一个 MDP，如何去寻找一个最佳的策略，然后可以得到一个`最佳价值函数(Optimal Value Function)`。**\n",
    "\n",
    "Optimal Value Function 的定义如下式所示：\n",
    "$$\n",
    "v^{*}(s)=\\max _{\\pi} v^{\\pi}(s)\n",
    "$$\n",
    "Optimal Value Function 是说，我们去搜索一种 policy $\\pi$ 来让每个状态的价值最大。$v^*$ 就是到达每一个状态，它的值的极大化情况。\n",
    "\n",
    "在这种极大化情况上面，我们得到的策略就可以说它是`最佳策略(optimal policy)`，如下式所示：\n",
    "$$\n",
    "\\pi^{*}(s)=\\underset{\\pi}{\\arg \\max }~ v^{\\pi}(s)\n",
    "$$\n",
    "Optimal policy 使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个 optimal value function，就可以说某一个 MDP 的环境被解。在这种情况下，它的最佳的价值函数是一致的，就它达到的这个上限的值是一致的，但这里可能有多个最佳的 policy，就是说多个 policy 可以取得相同的最佳价值。\n",
    "\n",
    "![](img/2.42.png)\n",
    "\n",
    "Q: 怎么去寻找这个最佳的 policy ？\n",
    "\n",
    "A: 当取得最佳的价值函数过后，我们可以通过对这个 Q 函数进行极大化，然后得到最佳策略。当所有东西都收敛过后，因为 Q 函数是关于状态跟动作的一个函数，所以在某一个状态采取一个动作，可以使得这个 Q 函数最大化，那么这个动作就应该是最佳的动作。所以如果我们能优化出一个 Q 函数，就可以直接在这个 Q 函数上面取一个让 Q 函数最大化的 action 的值，就可以提取出它的最佳策略。\n",
    "\n",
    "![](img/2.43.png)\n",
    "\n",
    "最简单的策略搜索办法就是`穷举`。假设状态和动作都是有限的，那么每个状态我们可以采取这个 A 种动作的策略，那么总共就是 $|A|^{|S|}$ 个可能的 policy。那我们可以把策略都穷举一遍，然后算出每种策略的 value function，对比一下就可以得到最佳策略。\n",
    "\n",
    "但是穷举非常没有效率，所以我们要采取其他方法。**搜索最佳策略有两种常用的方法：policy iteration 和  value iteration**。\n",
    "\n",
    "![](img/2.44.png)\n",
    "\n",
    "**寻找这个最佳策略的过程就是 MDP control 过程**。MDP control 说的就是怎么去寻找一个最佳的策略来让我们得到一个最大的价值函数，如下式所示：\n",
    "$$\n",
    "\\pi^{*}(s)=\\underset{\\pi}{\\arg \\max } ~ v^{\\pi}(s)\n",
    "$$\n",
    "对于一个事先定好的 MDP 过程，当 agent 去采取最佳策略的时候，我们可以说最佳策略一般都是确定的，而且是稳定的(它不会随着时间的变化)。但是不一定是唯一的，多种动作可能会取得相同的这个价值。\n",
    "\n",
    "**我们可以通过 policy iteration 和 value iteration 来解 MDP 的控制问题。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1fbc17-dd9f-4e47-ab85-e2b17dabfd03",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "![](img/2.45.png)\n",
    "\n",
    "**Policy iteration 由两个步骤组成：policy evaluation 和 policy improvement。**\n",
    "\n",
    "* **第一个步骤是 policy evaluation**，当前我们在优化这个 policy $\\pi$，在优化过程中得到一个最新的 policy。我们先保证这个 policy 不变，然后去估计它出来的这个价值。给定当前的 policy function 来估计这个 v 函数。\n",
    "* **第二个步骤是 policy improvement**，得到 v 函数过后，我们可以进一步推算出它的 Q 函数。得到 Q 函数过后，我们直接在 Q 函数上面取极大化，通过在这个 Q 函数上面做一个贪心的搜索来进一步改进它的策略。\n",
    "* 这两个步骤就一直是在迭代进行，所以在 policy iteration 里面，在初始化的时候，我们有一个初始化的 $V$ 和 $\\pi$ ，然后就是在这两个过程之间迭代。\n",
    "* 左边这幅图上面的线就是我们当前 v 的值，下面的线是 policy 的值。\n",
    "  * 跟踢皮球一样，我们先给定当前已有的这个 policy function，然后去算它的 v。\n",
    "  * 算出 v 过后，我们会得到一个 Q 函数。Q 函数我们采取 greedy 的策略，这样就像踢皮球，踢回这个 policy 。\n",
    "  * 然后进一步改进那个 policy ，得到一个改进的 policy 过后，它还不是最佳的，我们再进行 policy evaluation，然后又会得到一个新的 value function。基于这个新的 value function 再进行 Q 函数的极大化，这样就逐渐迭代，然后就会得到收敛。\n",
    "\n",
    "![](img/2.46.png)\n",
    "\n",
    "这里再来看一下第二个步骤： `policy improvement`，我们是如何改进它的这个策略。得到这个 v 值过后，我们就可以通过这个 reward function 以及状态转移把它的这个 Q-function 算出来，如下式所示：\n",
    "$$\n",
    "q^{\\pi_{i}}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi_{i}}\\left(s^{\\prime}\\right)\n",
    "$$\n",
    "对于每一个状态，第二个步骤会得到它的新一轮的这个 policy ，就在每一个状态，我们去取使它得到最大值的 action，如下式所示：\n",
    "$$\n",
    "\\pi_{i+1}(s)=\\underset{a}{\\arg \\max } ~q^{\\pi_{i}}(s, a)\n",
    "$$\n",
    "**你可以把 Q 函数看成一个 Q-table:**\n",
    "\n",
    "* 横轴是它的所有状态，\n",
    "* 纵轴是它的可能的 action。\n",
    "\n",
    "得到 Q 函数后，`Q-table`也就得到了。\n",
    "\n",
    "那么对于某一个状态，每一列里面我们会取最大的那个值，最大值对应的那个 action 就是它现在应该采取的 action。所以 arg max 操作就说在每个状态里面采取一个 action，这个 action 是能使这一列的 Q 最大化的那个动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e7cf5-55d1-4e80-8e9a-7206b3c236a6",
   "metadata": {},
   "source": [
    "#### Bellman Optimality Equation\n",
    "\n",
    "![](img/2.47.png)\n",
    "\n",
    "当一直在采取 arg max 操作的时候，我们会得到一个单调的递增。通过采取这种 greedy，即 arg max 操作，我们就会得到更好的或者不变的 policy，而不会使它这个价值函数变差。所以当这个改进停止过后，我们就会得到一个最佳策略。\n",
    "\n",
    "当改进停止过后，我们取它最大化的这个 action，它直接就会变成它的价值函数，如下式所示：\n",
    "$$\n",
    "q^{\\pi}\\left(s, \\pi^{\\prime}(s)\\right)=\\max _{a \\in \\mathcal{A}} q^{\\pi}(s, a)=q^{\\pi}(s, \\pi(s))=v^{\\pi}(s)\n",
    "$$\n",
    "所以我们有了一个新的等式：\n",
    "$$\n",
    "v^{\\pi}(s)=\\max _{a \\in \\mathcal{A}} q^{\\pi}(s, a)\n",
    "$$\n",
    "上式被称为  `Bellman optimality equation`。从直觉上讲，Bellman optimality equation 表达了这样一个事实：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。 \n",
    "\n",
    "**当 MDP 满足 Bellman optimality equation 的时候，整个 MDP 已经到达最佳的状态。**它到达最佳状态过后，对于这个 Q 函数，取它最大的 action 的那个值，就是直接等于它的最佳的 value function。只有当整个状态已经收敛过后，得到一个最佳的 policy 的时候，这个条件才是满足的。\n",
    "\n",
    "![](img/2.49.png)\n",
    "\n",
    "最佳的价值函数到达过后，这个 Bellman optimlity equation 就会满足。\n",
    "\n",
    "满足过后，就有这个 max 操作，如第一个等式所示：\n",
    "$$\n",
    "v^{*}(s)=\\max _{a} q^{*}(s, a)\n",
    "$$\n",
    "当我们取最大的这个 action 的时候对应的值就是当前状态的最佳的价值函数。\n",
    "\n",
    "另外，我们给出第二个等式，即 Q 函数的 Bellman equation：\n",
    "$$\n",
    "q^{*}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{*}\\left(s^{\\prime}\\right)\n",
    "$$\n",
    "**我们可以把第一个等式插入到第二个等式里面去**，如下式所示：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q^{*}(s, a)&=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{*}\\left(s^{\\prime}\\right) \\\\\n",
    "&=R(s,a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\max _{a} q^{*}(s', a')\n",
    "\\end{aligned}\n",
    "$$\n",
    "我们就会得到 Q 函数之间的转移。它下一步这个状态，取了 max 这个值过后，就会跟它最佳的这个状态等价。\n",
    "\n",
    "Q-learning 是基于 Bellman Optimality Equation 来进行的，当取它最大的这个状态的时候（ $\\underset{a'}{\\max} q^{*}\\left(s^{\\prime}, a^{\\prime}\\right)$ ），它会满足下面这个等式：\n",
    "$$\n",
    "q^{*}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\max _{a^{\\prime}} q^{*}\\left(s^{\\prime}, a^{\\prime}\\right)\n",
    "$$\n",
    "\n",
    "我们还可以把第二个等式插入到第一个等式，如下式所示：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^{*}(s)&=\\max _{a} q^{*}(s, a) \\\\\n",
    "&=\\max_{a} \\mathbb{E}[G_t|s_t=s,a_t=a]\\\\  \n",
    "&=\\max_{a}\\mathbb{E}[R_{t+1}+\\gamma G_{t+1}|s_t=s,a_t=a]\\\\\n",
    "&=\\max_{a}\\mathbb{E}[R_{t+1}+\\gamma v^*(s_{t+1})|s_t=s,a_t=a]\\\\\n",
    "&=\\max_{a}\\mathbb{E}[R_{t+1}]+ \\max_a \\mathbb{E}[\\gamma v^*(s_{t+1})|s_t=s,a_t=a]\\\\\n",
    "&=\\max_{a} R(s,a) + \\max_a\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{*}\\left(s^{\\prime}\\right)\\\\\n",
    "&=\\max_{a} \\left(R(s,a) + \\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{*}\\left(s^{\\prime}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "我们就会得到状态-价值函数的一个转移。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094753c-e201-4ae8-8f9d-a9737f851fac",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8828c-a6b5-45f2-bcb0-1606c26d1163",
   "metadata": {},
   "source": [
    "#### Principle of Optimality\n",
    "\n",
    "我们从另一个角度思考问题，动态规划的方法将优化问题分成两个部分：\n",
    "\n",
    "* 第一步执行的是最优的 action；\n",
    "* 之后后继的状态每一步都按照最优的 policy 去做，那么我最后的结果就是最优的。\n",
    "\n",
    "**Principle of Optimality Theorem**:\n",
    "\n",
    "一个 policy $\\pi(s|a)$ 在状态 $s$ 达到了最优价值，也就是 $v^{\\pi}(s) = v^{*}(s)$ 成立，当且仅当：\n",
    "\n",
    "对于**任何**能够从 $s$ 到达的 $s'$，都已经达到了最优价值，也就是，对于所有的 $s'$，$v^{\\pi}(s') = v^{*}(s')$ 恒成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe006191-a7cc-4ec8-8f3d-9bd1564bd6a7",
   "metadata": {},
   "source": [
    "#### Deterministic Value Iteration\n",
    "\n",
    "![](img/2.50.png)\n",
    "\n",
    "\n",
    "\n",
    "**Value iteration 就是把 Bellman Optimality Equation 当成一个 update rule 来进行，**如下式所示：\n",
    "$$\n",
    "v(s) \\leftarrow \\max _{a \\in \\mathcal{A}}\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} P\\left(s^{\\prime} \\mid s, a\\right) v\\left(s^{\\prime}\\right)\\right)\n",
    "$$\n",
    "之前我们说上面这个等式只有当整个 MDP 已经到达最佳的状态时才满足。但这里可以把它转换成一个 backup 的等式。Backup 就是说一个迭代的等式。**我们不停地去迭代 Bellman Optimality Equation，到了最后，它能逐渐趋向于最佳的策略，这是 value iteration 算法的精髓。**\n",
    "\n",
    "为了得到最佳的 $v^*$ ，对于每个状态的 $v^*$，我们直接把这个 Bellman Optimality Equation 进行迭代，迭代了很多次之后，它就会收敛。\n",
    "\n",
    "![](img/2.51.png)\n",
    "\n",
    "* 我们使用 value iteration 算法是为了得到一个最佳的策略。\n",
    "* 解法：我们可以直接把 `Bellman Optimality backup` 这个等式拿进来进行迭代，迭代很多次，收敛过后得到的那个值就是它的最佳的值。\n",
    "* 这个算法开始的时候，它是先把所有值初始化，通过每一个状态，然后它会进行这个迭代。把等式 (22) 插到等式 (23) 里面，就是 Bellman optimality backup 的那个等式。有了等式 (22) 和等式 (23) 过后，然后进行不停地迭代，迭代过后，然后收敛，收敛后就会得到这个 $v^*$ 。当我们有了 $v^*$ 过后，一个问题是如何进一步推算出它的最佳策略。\n",
    "* 提取最佳策略的话，我们可以直接用 arg max。就先把它的 Q 函数重构出来，重构出来过后，每一个列对应的最大的那个 action 就是它现在的最佳策略。这样就可以从最佳价值函数里面提取出最佳策略。\n",
    "* 我们只是在解决一个 planning 的问题，而不是强化学习的问题，因为我们知道环境如何变化。\n",
    "\n",
    "![](img/2.52.png)\n",
    "\n",
    "* value function 做的工作类似于 value 的反向传播，每次迭代做一步传播，所以中间过程的 policy 和 value function 是没有意义的。不像是 policy iteration，它每一次迭代的结果都是有意义的，都是一个完整的 policy。\n",
    "* 上图是一个可视化的过程，在一个 gridworld 中，我们设定了一个终点(goal)，也就是左上角的点。不管你在哪一个位置开始，我们都希望能够到终点（实际上这个终点是在迭代过程中不必要的，只是为了更好的演示）。Value iteration 的迭代过程像是一个从某一个状态（这里是我们的 goal）反向传播其他各个状态的过程。因为每次迭代只能影响到与之直接相关的状态。\n",
    "* 让我们回忆下 `Principle of Optimality Theorem`：当你这次迭代求解的某个状态 s 的 value function $v_{k+1}(s)$ 是最优解，它的前提是能够从该状态到达的所有状态 s' 此时都已经得到了最优解；如果不是的话，它做的事情只是一个类似传递 value function 的过程。\n",
    "* 以上图为例，实际上，对于每一个状态，我们都可以看成一个终点。迭代由每一个终点开始，每次都根据 Bellman optimality equation 重新计算 value。如果它的相邻节点 value 发生变化，变得更好，那么它也会变得更好，一直到相邻节点都不变了。因此，**在我们迭代到** $v_7$ **之前，也就是还没将每个终点的最优的 value 传递给其他的所有状态之前，中间的几个 value function 只是一种暂存的不完整的数据，它不能代表每一个 state 的 value function，所以生成的 policy 是一个没有意义的 policy**。\n",
    "* 因为它是一个迭代过程，这里可视化了从  $v_1$ 到 $v_7$  每一个状态的值的变化，它的这个值逐渐在变化。而且因为它每走一步，就会得到一个负的值，所以它需要尽快地到达左上角，可以发现离它越远的，那个值就越小。\n",
    "* $v_7$ 收敛过后，右下角那个值是 -6，相当于它要走六步，才能到达最上面那个值。而且离目的地越近，它的价值越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc75cd-ca3d-4ea8-8835-81a09e7cae6f",
   "metadata": {},
   "source": [
    "### Difference between Policy Iteration and Value Iteration\n",
    "\n",
    "![](img/2.53.png)\n",
    "\n",
    "![](img/2.54.png ':size=550')\n",
    "\n",
    "**我们来看一个 MDP control 的 Demo。**\n",
    "\n",
    "* 首先来看 policy iteration。之前的例子在每个状态都是采取固定的随机策略，就每个状态都是 0.25 的概率往上往下往左往右，没有策略的改变。\n",
    "* 但是我们现在想做 policy iteration，就是每个状态的策略都进行改变。Policy iteration 的过程是一个迭代过程。\n",
    "\n",
    "![](img/2.55.png ':size=550')\n",
    "\n",
    "我们先在这个状态里面 run 一遍 policy  evaluation，就得到了一个 value function，每个状态都有一个 value function。\n",
    "\n",
    "![](img/2.56.png ':size=550')\n",
    "\n",
    "* **现在进行 policy improvement，点一下 policy update。**点一下 policy update 过后，你可以发现有些格子里面的 policy 已经产生变化。\n",
    "* 比如说对于中间这个 -1 的这个状态，它的最佳策略是往下走。当你到达这个状态后，你应该往下，这样就会得到最佳的这个值。\n",
    "* 绿色右边的这个方块的策略也改变了，它现在选取的最佳策略是往左走，也就是说在这个状态的时候，最佳策略应该是往左走。\n",
    "\n",
    "![](img/2.57.png ':size=550')\n",
    "\n",
    "我们再 run 下一轮的 policy evaluation，你发现它的值又被改变了，很多次过后，它会收敛。\n",
    "\n",
    "![](img/2.58.png ':size=550')\n",
    "\n",
    "我们再 run policy update，你发现每个状态里面的值基本都改变，它不再是上下左右随机在变了，它会选取一个最佳的策略。\n",
    "\n",
    "![](img/2.59.png ':size=550')\n",
    "\n",
    "我们再 run 这个 policy evaluation，它的值又在不停地变化，变化之后又收敛了。\n",
    "\n",
    "![](img/2.60.png ':size=550')\n",
    "\n",
    "\n",
    "我们再来 run 一遍 policy update。现在它的值又会有变化，就在每一个状态，它的这个最佳策略也会产生一些改变。\n",
    "\n",
    "![](img/2.61.png ':size=550')\n",
    "\n",
    "再来在这个状态下面进行改变，现在你看基本没有什么变化，就说明整个 MDP 已经收敛了。所以现在它每个状态的值就是它当前最佳的 value function 的值以及它当前状态对应的这个 policy 就是最佳的 policy。\n",
    "\n",
    "比如说现在我们在右上角 0.38 的这个位置，然后它说现在应该往下走，我们往下走一步。它又说往下走，然后再往下走。现在我们有两个选择：往左走和往下走。我们现在往下走，随着这个箭头的指示，我们就会到达中间 1.20 的一个状态。如果能达到这个状态，我们就会得到很多 reward 。\n",
    "\n",
    "这个 Demo 说明了 policy iteration 可以把 gridworld 解决掉。解决掉的意思是说，不管在哪个状态，都可以顺着状态对应的最佳的策略来到达可以获得最多奖励的一个状态。\n",
    "\n",
    "![](img/2.62.png ':size=550')\n",
    "\n",
    "**我们再用 value iteration 来解 MDP，点 Toggle value iteration。** \n",
    "\n",
    "* 当它的这个值确定下来过后，它会产生它的最佳状态，这个最佳状态提取的策略跟 policy iteration 得出来的最佳策略是一致的。\n",
    "* 在每个状态，我们跟着这个最佳策略走，就会到达可以得到最多奖励的一个状态。\n",
    "\n",
    "我们给出一个[ Demo](https://github.com/cuhkrlcourse/RLexample/tree/master/MDP)，这个 Demo 是为了解一个叫 `FrozenLake` 的例子，这个例子是 OpenAI Gym 里的一个环境，跟 gridworld 很像，不过它每一个状态转移是一个概率。\n",
    "\n",
    "**我们再来对比下 policy iteration 和 value iteration，这两个算法都可以解 MDP 的控制问题。**\n",
    "\n",
    "* Policy Iteration 分两步，首先进行 policy evaluation，即对当前已经搜索到的策略函数进行一个估值。得到估值过后，进行 policy improvement，即把 Q 函数算出来，我们进一步进行改进。不断重复这两步，直到策略收敛。\n",
    "* Value iteration 直接把 Bellman Optimality Equation 拿进来，然后去寻找最佳的 value function，没有 policy function 在这里面。当算出 optimal value function 过后，我们再来提取最佳策略。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031a01b-f297-4441-87db-7ddadf443d4d",
   "metadata": {},
   "source": [
    "### Summary for Prediction and Control in MDP\n",
    "\n",
    "![](img/2.65.png)\n",
    "\n",
    "总结如上表所示，就对于 MDP 里面的 prediction 和 control  都是用动态规划来解，我们其实采取了不同的 Bellman Equation。\n",
    "\n",
    "* 如果是一个 prediction 的问题，即 policy evaluation  的问题，直接就是不停地 run 这个 Bellman Expectation Equation，这样我们就可以去估计出给定的这个策略，然后得到价值函数。\n",
    "* 对于 control，\n",
    "  * 如果采取的算法是 policy  iteration，那这里用的是 Bellman Expectation Equation 。把它分成两步，先上它的这个价值函数，再去优化它的策略，然后不停迭代。这里用到的只是 Bellman Expectation Equation。\n",
    "  * 如果采取的算法是 value iteration，那这里用到的 Bellman Equation 就是 Bellman Optimality Equation，通过 arg max 这个过程，不停地去 arg max 它，最后它就会达到最优的状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53146ff-fd40-4fe7-963c-836c3e767615",
   "metadata": {},
   "source": [
    "## 马尔科夫决策过程(MDP)代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cacc38c-edc6-457a-91d2-c8458594f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据本序列计算得到回报为：-2.5。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# 定义状态转移概率矩阵P\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P = np.array(P)\n",
    "\n",
    "rewards = [-1, -2, -2, 10, 1, 0]  # 定义奖励函数\n",
    "gamma = 0.5  # 定义折扣因子\n",
    "\n",
    "\n",
    "# 给定一条序列,计算从某个索引（起始状态）开始到序列最后（终止状态）得到的回报\n",
    "def compute_return(start_index, chain, gamma):\n",
    "    G = 0\n",
    "    for i in reversed(range(start_index, len(chain))):\n",
    "        G = gamma * G + rewards[chain[i] - 1]\n",
    "    return G\n",
    "\n",
    "\n",
    "# 一个状态序列,s1-s2-s3-s6\n",
    "chain = [1, 2, 3, 6]\n",
    "start_index = 0\n",
    "G = compute_return(start_index, chain, gamma)\n",
    "print(\"根据本序列计算得到回报为：%s。\" % G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2c0068-cabb-4f77-87b8-4a25ce1ed919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRP中每个状态价值分别为\n",
      " [[-2.01950168]\n",
      " [-2.21451846]\n",
      " [ 1.16142785]\n",
      " [10.53809283]\n",
      " [ 3.58728554]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def compute(P, rewards, gamma, states_num):\n",
    "    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n",
    "    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n",
    "    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n",
    "                   rewards)\n",
    "    return value\n",
    "\n",
    "\n",
    "V = compute(P, rewards, gamma, 6)\n",
    "print(\"MRP中每个状态价值分别为\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605516af-62ea-45da-ac2a-d1fbbf29b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP中每个状态价值分别为\n",
      " [[-1.22555411]\n",
      " [-1.67666232]\n",
      " [ 0.51890482]\n",
      " [ 6.0756193 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
    "# 状态转移函数\n",
    "P = {\n",
    "    \"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4,\n",
    "}\n",
    "# 奖励函数\n",
    "R = {\n",
    "    \"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1,\n",
    "}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)\n",
    "\n",
    "# 策略1,随机策略\n",
    "Pi_1 = {\n",
    "    \"s1-保持s1\": 0.5,\n",
    "    \"s1-前往s2\": 0.5,\n",
    "    \"s2-前往s1\": 0.5,\n",
    "    \"s2-前往s3\": 0.5,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.5,\n",
    "    \"s4-概率前往\": 0.5,\n",
    "}\n",
    "# 策略2\n",
    "Pi_2 = {\n",
    "    \"s1-保持s1\": 0.6,\n",
    "    \"s1-前往s2\": 0.4,\n",
    "    \"s2-前往s1\": 0.3,\n",
    "    \"s2-前往s3\": 0.7,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.1,\n",
    "    \"s4-概率前往\": 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\n",
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2\n",
    "\n",
    "gamma = 0.5\n",
    "# 转化后的MRP的状态转移矩阵\n",
    "P_from_mdp_to_mrp = [\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.1, 0.2, 0.2, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\n",
    "R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n",
    "\n",
    "V = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\n",
    "print(\"MDP中每个状态价值分别为\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982f429f-6afb-4738-9ee0-22fdba36ad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条序列\n",
      " [('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n",
      "第二条序列\n",
      " [('s4', '概率前往', 1, 's4'), ('s4', '前往s5', 10, 's5')]\n",
      "第五条序列\n",
      " [('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n"
     ]
    }
   ],
   "source": [
    "def sample(MDP, Pi, timestep_max, number):\n",
    "    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n",
    "    S, A, P, R, gamma = MDP\n",
    "    episodes = []\n",
    "    for _ in range(number):\n",
    "        episode = []\n",
    "        timestep = 0\n",
    "        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n",
    "        # 当前状态为终止状态或者时间步太长时,一次采样结束\n",
    "        while s != \"s5\" and timestep <= timestep_max:\n",
    "            timestep += 1\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            # 在状态s下根据策略选择动作\n",
    "            for a_opt in A:\n",
    "                temp += Pi.get(join(s, a_opt), 0)\n",
    "                if temp > rand:\n",
    "                    a = a_opt\n",
    "                    r = R.get(join(s, a), 0)\n",
    "                    break\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            # 根据状态转移概率得到下一个状态s_next\n",
    "            for s_opt in S:\n",
    "                temp += P.get(join(join(s, a), s_opt), 0)\n",
    "                if temp > rand:\n",
    "                    s_next = s_opt\n",
    "                    break\n",
    "            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n",
    "            s = s_next  # s_next变成当前状态,开始接下来的循环\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "\n",
    "# 采样5次,每个序列最长不超过1000步\n",
    "episodes = sample(MDP, Pi_1, 20, 5)\n",
    "print('第一条序列\\n', episodes[0])\n",
    "print('第二条序列\\n', episodes[1])\n",
    "print('第五条序列\\n', episodes[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3375f13-2646-4ea8-91a3-b1ace4b61d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用蒙特卡洛方法计算MDP的状态价值为\n",
      " {'s1': -1.228923788722258, 's2': -1.6955696284402704, 's3': 0.4823809701532294, 's4': 5.967514743019431, 's5': 0}\n"
     ]
    }
   ],
   "source": [
    "# 对所有采样序列计算所有状态的价值\n",
    "def MC(episodes, V, N, gamma):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n",
    "            (s, a, r, s_next) = episode[i]\n",
    "            G = r + gamma * G\n",
    "            N[s] = N[s] + 1\n",
    "            V[s] = V[s] + (G - V[s]) / N[s]\n",
    "\n",
    "\n",
    "timestep_max = 20\n",
    "# 采样1000次,可以自行修改\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "MC(episodes, V, N, gamma)\n",
    "print(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb624139-3b2a-482a-9e73-654d56fe8220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.112567796310472 0.23199480615618912\n"
     ]
    }
   ],
   "source": [
    "def occupancy(episodes, s, a, timestep_max, gamma):\n",
    "    ''' 计算状态动作对（s,a）出现的频率,以此来估算策略的占用度量 '''\n",
    "    rho = 0\n",
    "    total_times = np.zeros(timestep_max)  # 记录每个时间步t各被经历过几次\n",
    "    occur_times = np.zeros(timestep_max)  # 记录(s_t,a_t)=(s,a)的次数\n",
    "    for episode in episodes:\n",
    "        for i in range(len(episode)):\n",
    "            (s_opt, a_opt, r, s_next) = episode[i]\n",
    "            total_times[i] += 1\n",
    "            if s == s_opt and a == a_opt:\n",
    "                occur_times[i] += 1\n",
    "    for i in reversed(range(timestep_max)):\n",
    "        if total_times[i]:\n",
    "            rho += gamma**i * occur_times[i] / total_times[i]\n",
    "    return (1 - gamma) * rho\n",
    "\n",
    "\n",
    "gamma = 0.5\n",
    "timestep_max = 1000\n",
    "\n",
    "episodes_1 = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "episodes_2 = sample(MDP, Pi_2, timestep_max, 1000)\n",
    "rho_1 = occupancy(episodes_1, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "rho_2 = occupancy(episodes_2, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "print(rho_1, rho_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169ff66-43d4-4e27-851c-4d6e1bccb88c",
   "metadata": {},
   "source": [
    "# 表格型方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef872f9-aa86-40ae-ad59-e19d0921e70b",
   "metadata": {},
   "source": [
    "## 3.1 马尔可夫决策过程\n",
    "\n",
    "强化学习是一个与时间相关的序列决策的问题。\n",
    "例如，如图 3.1 所示，在 $t-1$ 时刻，我看到熊对我招手，下意识的动作就是逃跑。熊看到有人逃跑，就可能觉得发现了猎物，并开始发动攻击。而在 $t$ 时刻，我如果选择装死的动作，可能熊咬咬我、摔几下就觉得挺无趣的，可能会走开。这个时候我再逃跑，可能就成功了，这就是一个序列决策过程。\n",
    "\n",
    "在输出每一个动作之前，我们可以选择不同的动作。比如在 $t$ 时刻，我选择逃跑的时候，可能熊已经追上来了。如果在 $t$ 时刻，我没有选择装死，而是选择逃跑，这个时候熊已经追上来了，那么我就会转移到不同的状态。有一定的概率我会逃跑成功，也有一定的概率我会逃跑失败。我们用状态转移概率 $p\\left[s_{t+1}, r_{t} \\mid s_{t}, a_{t}\\right]$ 来表示在状态 $s_t$ 选择动作 $a_t$ 的时候，转移到转态 $s_{t+1}$ ，而且得到奖励 $r_t$ 的概率是多少。状态转移概率是具有**马尔可夫性质**的（系统下一时刻的状态仅由当前时刻的状态决定，不依赖于以往任何状态）。因为在这个过程中，下一时刻的状态取决于当前的状态 $s_t$，它和之前的 $s_{t-1}$ 和 $s_{t-2}$ 没有关系。再加上这个过程也取决于智能体与环境交互的 $a_t$ ，所以包含了决策的过程，我们称这样的过程为马尔可夫决策过程。\n",
    "马尔可夫决策过程就是序列决策的经典的表现方式。马尔可夫决策过程也是强化学习里面一个非常基本的学习框架。状态、动作、状态转移概率和奖励 $(S$、$A$、$P$、$R)$，这4个合集就构成了强化学习马尔可夫决策过程的四元组，后面也可能会再加上折扣因子构成五元组。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.1.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.1 马尔可夫决策过程四元组</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772448d7-6fca-4c54-ad35-c0fdf4b5f03d",
   "metadata": {},
   "source": [
    "### 3.1.2 免模型 \n",
    "很多强化学习的经典算法都是免模型的，也就是环境是未知的。\n",
    "因为现实世界中人类第一次遇到熊时，我们根本不知道能不能逃脱，所以 0.1、0.9 的概率都是虚构出来的概率。熊到底在什么时候往什么方向转变，我们通常是不知道的。\n",
    "我们处在未知的环境里，也就是这一系列的决策的概率函数和奖励函数是未知的，这就是有模型与免模型的最大的区别。\n",
    "\n",
    "如图 3.3 所示，强化学习可以应用于完全未知的和随机的环境。强化学习像人类一样学习，人类通过尝试不同的路来学习，通过尝试不同的路，人类可以慢慢地了解哪个状态会更好。强化学习用价值函数 $V(S)$ 来表示状态是好的还是坏的，用 Q 函数来判断在什么状态下采取什么动作能够取得最大奖励，即用 Q 函数来表示状态-动作值。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.3.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.3 免模型试错探索</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d244b5-9a6e-4c07-b250-d779f602a4ae",
   "metadata": {},
   "source": [
    "### 3.1.3 有模型与免模型的区别 \n",
    "\n",
    "如图 3.4 所示，策略迭代和价值迭代都需要得到环境的转移和奖励函数，所以在这个过程中，智能体没有与环境进行交互。在很多实际的问题中，马尔可夫决策过程的模型有可能是未知的，也有可能因模型太大不能进行迭代的计算，比如雅达利游戏、围棋、控制直升飞机、股票交易等问题，这些问题的状态转移非常复杂。\n",
    "\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/model_free_1.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.4 有模型强化学习方法</div>\n",
    "\n",
    "\n",
    "如图 3.5 所示，当马尔可夫决策过程的模型未知或者模型很大时，我们可以使用免模型强化学习的方法。免模型强化学习方法没有获取环境的状态转移和奖励函数，而是让智能体与环境进行交互，采集大量的轨迹数据，智能体从轨迹中获取信息来改进策略，从而获得更多的奖励。\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/model_free_2.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.5 免模型强化学习方法</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd24a5f-9b5e-4fd5-a062-298193896a6c",
   "metadata": {},
   "source": [
    "## 3.2 Q 表格 \n",
    "\n",
    "在多次尝试和熊打交道之后，我们就可以对熊的不同的状态做出判断，用状态动作价值来表达在某个状态下某个动作的好坏。\n",
    "如图 3.6 所示，如果 **Q 表格**是一张已经训练好的表格，这张表格就像是一本生活手册。通过查看这本手册，我们就知道在熊发怒的时候，装死的价值会高一点；在熊离开的时候，我们偷偷逃跑会比较容易获救。\n",
    "这张表格里面 Q 函数的意义就是我们选择了某个动作后，最后能不能成功，就需要我们去计算在某个状态下选择某个动作，后续能够获得多少总奖励。如果可以预估未来的总奖励的大小，我们就知道在当前的状态下选择哪个动作价值更高。我们选择某个动作是因为这样未来可以获得的价值会更高。所以强化学习的目标导向性很强，环境给出的奖励是非常重要的反馈，它根据环境的奖励来做选择。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.4.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.6 Q表格</div>\n",
    "\n",
    "Q: 为什么我们可以用未来的总奖励来评价当前动作是好是坏?\n",
    "\n",
    "A: 例如，如图 3.7 所示，假设一辆车在路上，当前是红灯，我们直接闯红灯的奖励就很低，因为这违反了交通规则，我们得到的奖励是当前的单步奖励。可是如果我们的车是一辆救护车，我们正在运送病人，把病人快速送达医院的奖励非常高，而且越快奖励越高。在这种情况下，我们可能要闯红灯，因为未来的远期奖励太高了。这是因为在现实世界中奖励往往是延迟的，所以强化学习需要学习远期的奖励。我们一般会从当前状态开始，把后续有可能会收到的所有奖励加起来计算当前动作的 Q 值，让 Q 值可以真正代表当前状态下动作的真正价值。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.5.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.7 未来的总奖励示例</div>\n",
    "\n",
    "但有的时候我们把目光放得太长远并不好。如果任务很快就结束，那么考虑到最后一步的奖励无可厚非。但如果任务是一个持续的没有尽头的任务，即**持续式任务（continuing task）**，我们把未来的奖励全部相加作为当前的状态价值就很不合理。\n",
    "股票就是一个典型的例子，如图 3.8 所示，我们关注的是累积的股票奖励，可是如果10年之后股票才有一次大涨大跌，我们肯定不会把10年后的奖励也作为当前动作的考虑因素。这个时候，我们就可以引入折扣因子 $\\gamma$ 来计算未来总奖励，$\\gamma \\in [0,1]$，越往后 $\\gamma^n$ 就会越小，越后面的奖励对当前价值的影响就会越小。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.6.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.8 股票的例子</div>\n",
    "\n",
    "悬崖行走问题是强化学习的一个经典问题，如图 3.9 所示，\n",
    "该问题需要智能体从出发点 S 出发，到达目的地 G，同时避免掉进悬崖（cliff），每走一步就有 $-$1分 的惩罚，掉进悬崖会有 $-$100 分的惩罚，但游戏不会结束，智能体会回到出发点，游戏继续，直到到达目的地结束游戏。智能体需要尽快地到达目的地。\n",
    "为了到达目的地，智能体可以沿着例如蓝线和红线的路线行走。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.7.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.9 悬崖行走问题</div>\n",
    "\n",
    "在悬崖行走问题的环境中，我们怎么计算状态动作价值（未来的总奖励）呢？我们可以选择一条路线，计算出这条路线上每个状态动作的价值。在悬崖行走问题里面，智能体每走一步都会拿到 $-$1 分的奖励，只有到达目的地之后，智能体才会停止。\n",
    "* 如果 $\\gamma = 0$，如图 3.10a 所示，我们考虑的就是单步的奖励，我们可以认为它是目光短浅的计算的方法。\n",
    "* 如果 $\\gamma = 1$，如图 3.10b 所示，就等于把后续所有的奖励全部加起来，我们可以认为它是目光过于长远的方法。如果智能体走的不是红色的路线，而是蓝色的路线，算出来的 Q 值可能如图中所示。因此，我们就可以知道，当小乌龟在 $-$12 的时候，往右走是 $-$11，往上走是 $-$15，它知道往右走的价值更大，它就会往右走。\n",
    "* 如果 $\\gamma = 0.6$，如图 3.10c 所示，\n",
    "\t我们的目光没有放得太长远，计算结果如式(3.1)所示。我们可以利用公式 $G_{t}=r_{t+1}+\\gamma G_{t+1}$ 从后往前推。\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "G_{13}=0 \\\\\n",
    "G_{12}=r_{13}+\\gamma G_{13}=-1+0.6 \\times 0=-1 \\\\\n",
    "G_{11}=r_{12}+\\gamma G_{12}=-1+0.6 \\times(-1)=-1.6 \\\\\n",
    "G_{10}=r_{11}+\\gamma G_{11}=-1+0.6 \\times(-1.6)=-1.96 \\\\\n",
    "G_{9}=r_{10}+\\gamma G_{10}=-1+0.6 \\times(-1.96)=-2.176 \\approx-2.18 \\\\\n",
    "G_{8}=r_{9}+\\gamma G_{9}=-1+0.6 \\times(-2.176)=-2.3056 \\approx-2.3 \\\\\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.8.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.10 折扣因子</div>\n",
    "\n",
    "\n",
    "类似于图 3.11，最后我们要求解的就是一张 Q 表格，它的行数是所有状态的数量，一般可以用坐标来表示格子的状态，也可以用 1、2、3、4、5、6、7 来表示不同的位置。Q 表格的列表示上、下、左、右4个动作。\n",
    "最开始的时候，Q 表格会全部初始化为0。智能体会不断和环境交互得到不同的轨迹，当交互的次数足够多的时候，我们就可以估算出每一个状态下，每个动作的平均总奖励，进而更新 Q  表格。Q表格的更新就是接下来要引入的强化概念。\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.9.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.11 Q表格</div>\n",
    "\n",
    "\n",
    "**强化**是指我们可以用下一个状态的价值来更新当前状态的价值，其实就是强化学习里面自举的概念。在强化学习里面，我们可以每走一步更新一次 Q 表格，用下一个状态的 Q 值来更新当前状态的 Q 值，这种单步更新的方法被称为时序差分方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131af2bb-7b10-431e-94d3-81b94b58b402",
   "metadata": {},
   "source": [
    "## 3.3 免模型预测 \n",
    "\n",
    "在无法获取马尔可夫决策过程的模型情况下，我们可以通过蒙特卡洛方法和时序差分方法来估计某个给定策略的价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04118e1-dbe4-4dba-bb1b-b652d2a1623d",
   "metadata": {},
   "source": [
    "### 3.3.1 蒙特卡洛策略评估\n",
    "**蒙特卡洛**方法是基于采样的方法，给定策略 $\\pi$，我们让智能体与环境进行交互，可以得到很多轨迹。每个轨迹都有对应的回报：\n",
    "\n",
    "$$\n",
    "G_{t}=r_{t+1}+\\gamma r_{t+2}+\\gamma^{2} r_{t+3}+\\ldots\n",
    "$$\n",
    "\n",
    "我们求出所有轨迹的回报的平均值，就可以知道某一个策略对应状态的价值，即 \n",
    "$$\n",
    "V_{\\pi}(s)=\\mathbb{E}_{\\tau \\sim \\pi}\\left[G_{t} \\mid  s_{t}=s\\right]\n",
    "$$\n",
    "\n",
    "蒙特卡洛仿真是指我们可以采样大量的轨迹，计算所有轨迹的真实回报，然后计算平均值。\n",
    "蒙特卡洛方法使用经验平均回报（empirical mean return）的方法来估计，它不需要马尔可夫决策过程的状态转移函数和奖励函数，并且不需要像动态规划那样用自举的方法。此外，蒙特卡洛方法有一定的局限性，它只能用在有终止的马尔可夫决策过程中。\n",
    "\n",
    "接下来，我们对蒙特卡洛方法进行总结。为了得到评估 $V(s)$，我们采取了如下的步骤。\n",
    "\n",
    "（1）在每个回合中，如果在时间步 $t$ 状态 $s$ 被访问了，那么\n",
    "\n",
    "* 状态 $s$ 的访问数 $N(s)$ 增加 1，$N(s)\\leftarrow N(s)+1$。\n",
    "* 状态 $s$ 的总的回报 $S(s)$  增加 $G_t$，$S(s)\\leftarrow S(s)+G_t$。\n",
    "\n",
    "（2）状态 $s$ 的价值可以通过回报的平均来估计，即 $V(s)=S(s)/N(s)$。 \n",
    "\n",
    "根据大数定律，只要我们得到足够多的轨迹，就可以趋近这个策略对应的价值函数。当 $N(s)\\rightarrow \\infty$ 时，$V(s) \\rightarrow V_{\\pi}(s)$。\n",
    "\n",
    "假设现在有样本 $x_1,x_2,\\cdots, x_t$，我们可以把经验均值（empirical mean）转换成增量均值（incremental mean）的形式：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\mu_{t} &=\\frac{1}{t} \\sum_{j=1}^{t} x_{j} \\\\\n",
    "\t&=\\frac{1}{t}\\left(x_{t}+\\sum_{j=1}^{t-1} x_{j}\\right) \\\\\n",
    "\t&=\\frac{1}{t}\\left(x_{t}+(t-1) \\mu_{t-1}\\right) \\\\\n",
    "\t&=\\frac{1}{t}\\left(x_{t}+t \\mu_{t-1}-\\mu_{t-1}\\right) \\\\\n",
    "\t&=\\mu_{t-1}+\\frac{1}{t}\\left(x_{t}-\\mu_{t-1}\\right) \n",
    "\t\\end{aligned}\n",
    "$$\n",
    "通过这种转换，我们就可以把上一时刻的平均值与现在时刻的平均值建立联系，即\n",
    "$$\n",
    "\t\\mu_t = \\mu_{t-1}+\\frac{1}{t}(x_t-\\mu_{t-1})\n",
    "$$\n",
    "其中，$x_t- \\mu_{t-1}$ 是残差，$\\frac{1}{t}$ 类似于学习率（learning rate）。\n",
    "当我们得到 $x_t$时，就可以用上一时刻的值来更新现在的值。\n",
    "\n",
    "我们可以把蒙特卡洛方法更新的方法写成增量式蒙特卡洛（incremental MC）方法。我们采集数据，得到一个新的轨迹 $(s_1,a_1,r_1,\\dots,s_t)$。对于这个轨迹，我们采用增量的方法进行更新：\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "N\\left(s_{t}\\right) \\leftarrow N\\left(s_{t}\\right)+1 \\\\\n",
    "V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\frac{1}{N\\left(s_{t}\\right)}\\left(G_{t}-V\\left(s_{t}\\right)\\right)\n",
    "\\end{array}\t\n",
    "$$\n",
    "\n",
    "我们可以直接把 $\\frac{1}{N(s_t)}$ 变成 $\\alpha$（学习率），即\n",
    "$$\n",
    "\tV\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(G_{t}-V\\left(s_{t}\\right)\\right)\n",
    "$$\n",
    "其中，$\\alpha$ 代表更新的速率，我们可以对其进行设置。\n",
    "\n",
    "我们再来看一下动态规划方法和蒙特卡洛方法的差异。\n",
    "动态规划也是常用的估计价值函数的方法。在动态规划方法里面，我们使用了自举的思想。自举就是我们基于之前估计的量来估计一个量。\n",
    "此外，动态规划方法使用贝尔曼期望备份（Bellman expectation backup），通过上一时刻的值 $V_{i-1}(s')$ 来更新当前时刻的值 $V_i(s)$ ，即\n",
    "$$\n",
    "  \n",
    "V_{i}(s) \\leftarrow \\sum_{a \\in A} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) V_{i-1}\\left(s^{\\prime}\\right)\\right)\n",
    "$$\n",
    "将其不停迭代，最后可以收敛。如图 3.12 所示，贝尔曼期望备份有两层加和，即内部加和和外部加和，计算两次期望，得到一个更新。\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/MC_4.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.12 贝尔曼期望备份</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "蒙特卡洛方法通过一个回合的经验平均回报（实际得到的奖励）来进行更新，即\n",
    "$$\n",
    "\tV\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(G_{i, t}-V\\left(s_{t}\\right)\\right)\n",
    "$$\n",
    "\n",
    "如图 3.13 所示，我们使用蒙特卡洛方法得到的轨迹对应树上蓝色的轨迹，轨迹上的状态已经是决定的，采取的动作也是已经决定的。我们现在只更新这条轨迹上的所有状态，与这条轨迹没有关系的状态都不进行更新。\n",
    "\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/MC_5.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.13 蒙特卡洛方法更新</div>\n",
    "\n",
    "\n",
    "蒙特卡洛方法相比动态规划方法是有一些优势的。首先，蒙特卡洛方法适用于环境未知的情况，而动态规划是有模型的方法。\n",
    "蒙特卡洛方法只需要更新一条轨迹的状态，而动态规划方法需要更新所有的状态。状态数量很多的时候（比如100万个、200万个），我们使用动态规划方法进行迭代，速度是非常慢的。这也是基于采样的蒙特卡洛方法相对于动态规划方法的优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b134b1-6e57-4b8e-9081-9f936b07c82a",
   "metadata": {},
   "source": [
    "### 3.3.2 时序差分 \n",
    "\n",
    "为了让读者更好地理解时序差分这种更新方法，我们给出它的“物理意义”。我们先了解一下巴甫洛夫的条件反射实验，如图 3.14 所示，这个实验讲的是小狗会对盆里面的食物无条件产生刺激，分泌唾液。一开始小狗对于铃声这种中性刺激是没有反应的，可是我们把铃声和食物结合起来，每次先给它响一下铃，再给它喂食物，多次重复之后，当铃声响起的时候，小狗也会开始流口水。盆里的肉可以认为是强化学习里面那个延迟的奖励，声音的刺激可以认为是有奖励的那个状态之前的状态。多次重复实验之后，最后的奖励会强化小狗对于声音的条件反射，它会让小狗知道这个声音代表着有食物，这个声音对于小狗也就有了价值，它听到这个声音就会流口水。\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.10.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.14 强化概念：巴甫洛夫的条件反射实验</div>\n",
    "\n",
    "\n",
    "如图 3.15 所示，巴甫洛夫效应揭示的是，当中性刺激（铃声）与无条件刺激（食物）相邻反复出现的时候，中件刺激也可以引起无条件刺激引起的唾液分泌，然后形成条件刺激。\n",
    "我们称这种中性刺激与无条件刺激在时间上面的结合为强化，强化的次数越多，条件反射就会越巩固。小狗本来不觉得铃声有价值的，经过强化之后，小狗就会慢慢地意识到铃声也是有价值的，它可能带来食物。更重要的是当一种条件反射巩固之后，我们再用另外一种新的刺激和条件反射相结合，还可以形成第二级条件反射，同样地还可以形成第三级条件反射。\n",
    "\n",
    "在人的身上也可以建立多级的条件反射，例如，比如我们遇到熊可能是这样一个顺序：看到树上有熊爪，然后看到熊，突然熊发怒并扑过来了。经历这个过程之后，我们可能最开始看到熊才会害怕，后面可能看到树上有熊爪就已经有害怕的感觉了。在不断的重复实验后，下一个状态的价值可以不断地强化影响上一个状态的价值。\n",
    "\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.11.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.15 强化示例</div>\n",
    "\n",
    "\n",
    "为了让读者更加直观地感受下一个状态会如何影响上一个状态（状态价值迭代），我们推荐[时序差分学习网格世界演示](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html)。\n",
    "如图 3.16 所示，我们先初始化，然后开始时序差分方法的更新过程。\n",
    "在训练的过程中，小黄球在不断地试错，在探索中会先迅速地发现有奖励的格子。最开始的时候，有奖励的格子才有价值。当小黄球不断地重复走这些路线的时候，有价值的格子可以慢慢地影响它附近的格子的价值。\n",
    "反复训练之后，有奖励的格子周围的格子的状态就会慢慢被强化。强化就是价值最终收敛到最优的情况之后，小黄球就会自动往价值高的格子走，就可以走到能够拿到奖励的格子。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.13.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.16 时序差分学习网格世界演示</div>\n",
    "\n",
    "下面我们开始正式介绍时序差分方法。\n",
    "时序差分是介于蒙特卡洛和动态规划之间的方法，它是免模型的，不需要马尔可夫决策过程的转移矩阵和奖励函数。\n",
    "此外，时序差分方法可以从不完整的回合中学习，并且结合了自举的思想。\n",
    "\n",
    "接下来，我们对时序差分方法进行总结。时序差分方法的目的是对于某个给定的策略 $\\pi$，在线（online）地算出它的价值函数 $V_{\\pi}$，即一步一步地（step-by-step）算。\n",
    "最简单的算法是**一步时序差分（one-step TD）**，即TD(0)。每往前走一步，就做一步自举，用得到的估计回报（estimated return）$r_{t+1}+\\gamma V(s_{t+1})$ 来更新上一时刻的值 $V(s_t)$：\n",
    "$$\n",
    "  V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(r_{t+1}+\\gamma V\\left(s_{t+1}\\right)-V\\left(s_{t}\\right)\\right) \\tag{3.1}\n",
    "$$\n",
    "估计回报 $r_{t+1}+\\gamma V(s_{t+1})$ 被称为**时序差分目标（TD target）**，\n",
    "时序差分目标是带衰减的未来奖励的总和。时序差分目标由两部分组成：\n",
    "\n",
    "（1）我们走了某一步后得到的实际奖励$r_{t+1}$；\n",
    "\n",
    "（2）我们利用了自举的方法，通过之前的估计来估计 $V(s_{t+1})$  ，并且加了折扣因子，即 $\\gamma V(s_{t+1})$。\n",
    "\n",
    "\n",
    "时序差分目标是估计有两个原因：\n",
    "\n",
    "（1）时序差分方法对期望值进行采样；\n",
    "\n",
    "（2）时序差分方法使用当前估计的 $V$ 而不是真实的 $V_{\\pi}$。\n",
    "\n",
    "**时序差分误差（TD error）** $\\delta=r_{t+1}+\\gamma V(s_{t+1})-V(s_t)$。\n",
    "类比增量式蒙特卡洛方法，给定一个回合 $i$，我们可以更新 $V(s_t)$ 来逼近真实的回报 $G_t$，具体更新公式为\n",
    "$$\n",
    "V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(G_{i, t}-V\\left(s_{t}\\right)\\right)\n",
    "$$\n",
    "式(3.1)体现了强化的概念。\n",
    "\n",
    "我们对比一下蒙特卡洛方法和时序差分方法。在蒙特卡洛方法里面，$G_{i,t}$ 是实际得到的值（可以看成目标），因为它已经把一条轨迹跑完了，可以算出每个状态实际的回报。时序差分不等轨迹结束，往前走一步，就可以更新价值函数。 \n",
    "如图 3.17 所示，时序差分方法只执行一步，状态的值就更新。蒙特卡洛方法全部执行完之后，到了终止状态之后，再更新它的值。\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/TD_3.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.17 时序差分方法相比蒙特卡洛方法的优势</div>\n",
    "\n",
    "\n",
    "接下来，进一步比较时序差分方法和蒙特卡洛方法。\n",
    "\n",
    "（1）时序差分方法可以在线学习（online learning），每走一步就可以更新，效率高。蒙特卡洛方法必须等游戏结束时才可以学习。\n",
    "\n",
    "（2）时序差分方法可以从不完整序列上进行学习。蒙特卡洛方法只能从完整的序列上进行学习。\n",
    "\n",
    "（3）时序差分方法可以在连续的环境下（没有终止）进行学习。蒙特卡洛方法只能在有终止的情况下学习。\n",
    "\n",
    "（4）时序差分方法利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。\n",
    "\n",
    "例如来解释 时序差分方法和蒙特卡洛方法的区别。\n",
    "时序差分方法是指在不清楚马尔可夫状态转移概率的情况下，以采样的方式得到不完整的状态序列，估计某状态在该状态序列完整后可能得到的奖励，并通过不断地采样持续更新价值。蒙特卡洛则需要经历完整的状态序列后，再来更新状态的真实价值。\n",
    "例如，我们想获得开车去公司的时间，每天上班开车的经历就是一次采样。假设我们今天在路口 A 遇到了堵车，\n",
    "时序差分方法会在路口 A 就开始更新预计到达路口 B、路口 C $\\cdots \\cdots$，以及到达公司的时间；\n",
    "而蒙特卡洛方法并不会立即更新时间，而是在到达公司后，再更新到达每个路口和公司的时间。\n",
    "时序差分方法能够在知道结果之前就开始学习，相比蒙特卡洛方法，其更快速、灵活。\n",
    "\n",
    "\n",
    "如图 3.18 所示，我们可以把时序差分方法进行进一步的推广。之前是只往前走一步，即TD(0)。\n",
    "我们可以调整步数（step），变成 **$n$步时序差分（$n$-step TD）**。比如 TD(2)，即往前走两步，利用两步得到的回报，使用自举来更新状态的价值。\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/TD_5.png\"/>\n",
    "</div>\n",
    "\n",
    "$\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad$ 图 3.18 $n$步时序差分\n",
    "\n",
    "这样我们就可以通过步数来调整算法需要的实际奖励和自举。\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "n=1\\text{（TD）} &G_{t}^{(1)}&=r_{t+1}+\\gamma V\\left(s_{t+1}\\right) \\\\\n",
    "n=2 &G_{t}^{(2)}&= r_{t+1}+\\gamma r_{t+2}+\\gamma^{2} V\\left(s_{t+2}\\right) \\\\\n",
    "& &\\vdots \\\\\n",
    "n=\\infty\\text{（MC）} &G_{t}^{\\infty}&=r_{t+1}+\\gamma r_{t+2}+\\ldots+\\gamma^{T-t-1} r_{T}\n",
    "\\end{array} \\tag{3.2}\n",
    "$$\n",
    "\n",
    "如式(3.2)所示，通过调整步数，可以进行蒙特卡洛方法和时序差分方法之间的权衡。如果 $n=\\infty$， 即整个游戏结束后，再进行更新，时序差分方法就变成了蒙特卡洛方法。\n",
    "\n",
    "$n$步时序差分可写为\n",
    "\n",
    "$$\n",
    "G_{t}^{n}=r_{t+1}+\\gamma r_{t+2}+\\ldots+\\gamma^{n-1} r_{t+n}+\\gamma^{n} V\\left(s_{t+n}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "得到时序差分目标之后，我们用增量式学习（incremental learning）的方法来更新状态的价值：\n",
    "\n",
    "$$\n",
    "V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(G_{t}^{n}-V\\left(s_{t}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dadaf-87e9-42b2-bc9d-7dcd62ed79f3",
   "metadata": {},
   "source": [
    "### 3.3.3 动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样 \n",
    "自举是指更新时使用了估计。蒙特卡洛方法没有使用自举，因为它根据实际的回报进行更新。\n",
    "动态规划方法和时序差分方法使用了自举。\n",
    "\n",
    "采样是指更新时通过采样得到一个期望。\n",
    "蒙特卡洛方法是纯采样的方法。\n",
    "动态规划方法没有使用采样，它是直接用贝尔曼期望方程来更新状态价值的。\n",
    "时序差分方法使用了采样。时序差分目标由两部分组成，一部分是采样，一部分是自举。\n",
    "\n",
    "如图 3.19 所示，动态规划方法直接计算期望，它把所有相关的状态都进行加和，即\n",
    "$$\n",
    "\tV\\left(s_{t}\\right) \\leftarrow \\mathbb{E}_{\\pi}\\left[r_{t+1}+\\gamma V\\left(s_{t+1}\\right)\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/comparison_2.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.19 统一视角：动态规划方法备份</div>\n",
    "\n",
    "如图 3.20 所示，蒙特卡洛方法在当前状态下，采取一条支路，在这条路径上进行更新，更新这条路径上的所有状态，即\n",
    "$$\n",
    "\t\n",
    "V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(G_{t}-V\\left(s_{t}\\right)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/comparison_3.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.20 统一视角：蒙特卡洛备份</div>\n",
    "\n",
    "\n",
    "如图 3.20 所示，时序差分从当前状态开始，往前走了一步，关注的是非常局部的步骤，即\n",
    "$$\n",
    "\t\\text{TD}(0): V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(r_{t+1}+\\gamma V\\left(s_{t+1}\\right)-V\\left(s_{t}\\right)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/comparison_4.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.21 统一视角：时序差分方法备份</div>\n",
    "\n",
    "\n",
    "如图 3.22 所示，如果 时序差分方法需要更广度的更新，就变成了 动态规划方法（因为动态规划方法是把所有状态都考虑进去来进行更新）。如果时序差分方法需要更深度的更新，就变成了蒙特卡洛方法。图 3.22 右下角是穷举搜索的方法（exhaustive search），穷举搜索的方法不仅需要很深度的信息，还需要很广度的信息。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/comparison_5.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.22 强化学习的统一视角</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6e57e-54d0-4b1b-8e98-344eb32bad69",
   "metadata": {},
   "source": [
    "## 3.4 免模型控制 \n",
    "在我们不知道马尔可夫决策过程模型的情况下，如何优化价值函数，得到最佳的策略呢？我们可以把策略迭代进行广义的推广，使它能够兼容蒙特卡洛和时序差分的方法，即带有蒙特卡洛方法和时序差分方法的**广义策略迭代（generalized policy iteration，GPI）**。\n",
    "\n",
    "如图 3.23 所示，策略迭代由两个步骤组成。第一，我们根据给定的当前策略 $\\pi$ 来估计价值函数；第二，得到估计的价值函数后，我们通过贪心的方法来改进策略，即\n",
    "$$\n",
    "\\pi^{'}=\\text{贪心函数}(V_{\\pi})\n",
    "$$\n",
    "\n",
    "这两个步骤是一个互相迭代的过程。\n",
    "\n",
    "$$\n",
    "\\pi_{i+1}(s)=\\underset{a}{\\arg \\max } Q_{\\pi_{i}}(s, a) \\tag{3.3}\n",
    "$$\n",
    "\n",
    "我们可以计算出策略 $\\pi$ 的动作价值函数，并且可以根据式（3.3）来计算针对状态 $s \\in S$ 的新策略 $\\pi_{i+1}$。但得到状态价值函数后，我们并不知道奖励函数 $R(s,a)$ 和状态转移 $P(s'|s,a)$，所以就无法估计 Q 函数\n",
    "$$\n",
    "Q_{\\pi_{i}}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) V_{\\pi_{i}}\\left(s^{\\prime}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/model_free_control_1.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.23 策略迭代</div>\n",
    "\n",
    "\n",
    "这里有一个问题：当我们不知道奖励函数和状态转移时，如何进行策略的优化？\n",
    "\n",
    "如图 3.24 所示，针对上述情况，我们引入了广义的策略迭代的方法。\n",
    "我们对策略评估部分进行修改，使用蒙特卡洛的方法代替动态规划的方法估计 Q 函数。我们首先进行策略评估，使用蒙特卡洛方法来估计策略 $Q=Q_{\\pi}$，然后进行策略更新，即得到 Q 函数后，我们就可以通过贪心的方法去改进它：\n",
    "$$\n",
    "\\pi(s)=\\underset{a}{\\arg \\max} Q(s, a)\n",
    "$$\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/model_free_control_3.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.24 广义策略迭代</div>\n",
    "\n",
    "\n",
    "图 3.25 所示为蒙特卡洛方法估计 Q 函数的算法。\n",
    "一个保证策略迭代收敛的假设是回合有**探索性开始（exploring start）**。\n",
    "假设每一个回合都有一个探索性开始，探索性开始保证所有的状态和动作都在无限步的执行后能被采样到，这样才能很好地进行估计。\n",
    "算法通过蒙特卡洛方法产生很多轨迹，每条轨迹都可以算出它的价值。然后，我们可以通过平均的方法去估计 Q 函数。Q 函数可以看成一个Q表格，我们通过采样的方法把表格的每个单元的值都填上，然后使用策略改进来选取更好的策略。\n",
    "如何用蒙特卡洛方法来填 Q 表格是这个算法的核心。\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/model_free_control_4.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.25 基于探索性开始的蒙特卡洛方法</div>\n",
    "\n",
    "\n",
    "为了确保蒙特卡洛方法能够有足够的探索，我们使用了 $\\varepsilon$-贪心（$\\varepsilon\\text{-greedy}$）探索。\n",
    "$\\varepsilon$-贪心是指我们有 $1-\\varepsilon$ 的概率会按照 Q函数来决定动作，通常 $\\varepsilon$ 就设一个很小的值， $1-\\varepsilon$ 可能是 0.9，也就是 0.9 的概率会按照Q函数来决定动作，但是我们有 0.1 的概率是随机的。通常在实现上，$\\varepsilon$ 的值会随着时间递减。在最开始的时候，因为我们还不知道哪个动作是比较好的，所以会花比较多的时间探索。接下来随着训练的次数越来越多，我们已经比较确定哪一个动作是比较好的，就会减少探索，把 $\\varepsilon$ 的值变小。主要根据 Q函数来决定动作，比较少随机决定动作，这就是 $\\varepsilon$-贪心。\n",
    "\n",
    "当我们使用蒙特卡洛方法和 $\\varepsilon$-贪心探索的时候，可以确保价值函数是单调的、改进的。对于任何 $\\varepsilon$-贪心策略 $\\pi$，关于 $Q_{\\pi}$ 的 $\\varepsilon$-贪心策略 $\\pi^{\\prime}$ 都是一个改进，即 $V_{\\pi}(s) \\leqslant V_{\\pi^{\\prime}}(s)$，证明过程如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tQ_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) &=\\sum_{a \\in A} \\pi^{\\prime}(a \\mid s) Q_{\\pi}(s, a) \\\\\n",
    "\t&=\\frac{\\varepsilon}{|A|} \\sum_{a \\in A} Q_{\\pi}(s, a)+(1-\\varepsilon) \\max _{a} Q_{\\pi}(s, a) \\\\\n",
    "\t& \\geqslant \\frac{\\varepsilon}{|A|} \\sum_{a \\in A} Q_{\\pi}(s, a)+(1-\\varepsilon) \\sum_{a \\in A} \\frac{\\pi(a \\mid s)-\\frac{\\varepsilon}{|A|}}{1-\\varepsilon} Q_{\\pi}(s, a) \\\\\n",
    "\t&=\\sum_{a \\in A} \\pi(a \\mid s) Q_{\\pi}(s, a)=V_{\\pi}(s)\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "\n",
    "基于 $\\varepsilon$-贪心探索的蒙特卡洛方法如图 3.26 所示。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/model_free_control_7.png\"/>\n",
    "</div>\n",
    "$\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad$ 图 3.26 基于 $\\varepsilon$-贪心探索的蒙特卡洛方法\n",
    "\n",
    "与蒙特卡洛方法相比，时序差分方法有如下几个优势：低方差，能够在线学习，能够从不完整的序列中学习。\n",
    "所以我们可以把时序差分方法也放到控制循环（control loop）里面去估计Q表格，再采取 $\\varepsilon$-贪心探索改进。这样就可以在回合没结束的时候更新已经采集到的状态价值。  \n",
    "\n",
    ">偏差（bias）：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越高，越偏离真实数据，如图 3.27 第2行所示。\n",
    "方差（variance）：描述的是预测值的变化范围、离散程度，也就是离其期望值的距离。方差越高，数据的分布越分散，如图 3.27 右列所示。\n",
    "\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/bias_variance.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.27 偏差-方差</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6aa71-c274-4769-a65b-88d7b52d67c9",
   "metadata": {},
   "source": [
    "### 3.4.1 Sarsa：同策略时序差分控制 \n",
    "\n",
    "时序差分方法是给定一个策略，然后我们去估计它的价值函数。接着我们要考虑怎么使用时序差分方法的框架来估计Q函数，也就是 Sarsa 算法。\n",
    "\n",
    "\n",
    "Sarsa 所做出的改变很简单，它将原本时序差分方法更新 $V$ 的过程，变成了更新 $Q$，即\n",
    "$$\n",
    "\tQ\\left(s_{t}, a_{t}\\right) \\leftarrow Q\\left(s_{t}, a_{t}\\right)+\\alpha\\left[r_{t+1}+\\gamma Q\\left(s_{t+1}, a_{t+1}\\right)-Q\\left(s_{t}, a_{t}\\right)\\right] \\tag{3.4}\n",
    "$$\n",
    "\n",
    "式(3.4)是指我们可以用下一步的 Q 值 $Q(s_{t+_1},a_{t+1})$ 来更新这一步的 Q 值 $Q(s_t,a_t)$ 。\n",
    "Sarsa 直接估计 Q 表格，得到 Q 表格后，就可以更新策略。\n",
    "\n",
    "为了理解式(3.4)，\n",
    "如图 3.28 所示，我们先把 $r_{t+1}+\\gamma Q\\left(s_{t+1}, a_{t+1}\\right.)$ 当作目标值，即 $Q(s_t,a_t)$ 想要逼近的目标值。$r_{t+1}+\\gamma Q\\left(s_{t+1}, a_{t+1}\\right.)$ 就是时序差分目标。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.14.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.28 时序差分单步更新</div>\n",
    "\n",
    "\n",
    "我们想要计算的就是 $Q(s_t,a_t)$ 。因为最开始 Q 值都是随机初始化或者是初始化为0，所以它需要不断地去逼近它理想中真实的 Q 值（时序差分目标），$r_{t+1}+\\gamma Q\\left(s_{t+1}, a_{t+1}\\right)-Q\\left(s_{t}, a_{t}\\right)$ 就是时序差分误差。\n",
    "我们用 $Q(s_t,a_t)$ 来逼近 $G_t$，那么 $Q(s_{t+1},a_{t+1})$ 其实就是近似 $G_{t+1}$。我们就可以用  $Q(s_{t+1},a_{t+1})$ 近似 $G_{t+1}$，把  $r_{t+1}+\\gamma Q(s_{t+1},a_{t+1})$  当成目标值。\n",
    "$Q(s_t,a_t)$  要逼近目标值，我们用软更新的方式来逼近。软更新的方式就是每次我们只更新一点点，$\\alpha$ 类似于学习率。最终Q 值是可以慢慢地逼近真实的目标值的。这样更新公式只需要用到当前时刻的 $s_{t}$、$a_t$，还有获取的 $r_{t+1}$、$s_{t+1}$、$a_{t+1}$ 。\n",
    "\n",
    "该算法由于每次更新值函数时需要知道当前的状态（state）、当前的动作（action）、奖励（reward）、下一步的状态（state）、下一步的动作（action），即 $(s_{t}, a_{t}, r_{t+1}, s_{t+1}, a_{t+1})$ 这几个值 ，因此得名 **Sarsa** 算法。它走了一步之后，获取了 $(s_{t}, a_{t}, r_{t+1}, s_{t+1}, a_{t+1})$  之后，就可以做一次更新。\n",
    "\n",
    "如图 3.29 所示，Sarsa 的更新公式可写为\n",
    "$$\n",
    "\tQ(S, A) \\leftarrow Q(S, A)+\\alpha\\left(R+\\gamma Q\\left(S^{\\prime}, A^{\\prime}\\right)-Q(S, A)\\right)\n",
    "$$\n",
    "\n",
    "Sarsa的更新公式与时序差分方法的公式是类似的。$S'$ 就是 $s_{t+1}$ 。我们就是用下一步的 Q 值 $Q(S',A')$ 来更新这一步的 Q 值 $Q(S,A)$，不断地强化每一个 Q 值。\n",
    "$$\n",
    "\\begin{array}{lrl}\n",
    "\t{n=1}\\text {（Sarsa）} &Q_{t}^{1}&=r_{t+1}+\\gamma Q\\left(s_{t+1}, a_{t+1}\\right) \\\\\n",
    "\tn=2 &Q_{t}^{2}&=r_{t+1}+\\gamma r_{t+2}+\\gamma^{2} Q\\left(s_{t+2}, a_{t+2}\\right) \\\\\n",
    "\t&&\\vdots \\\\\n",
    "\tn=\\infty\\text{（MC）} \\quad &Q_{t}^{\\infty}&=r_{t+1}+\\gamma r_{t+2}+\\ldots+\\gamma^{T-t-1} r_{T}\n",
    "\t\\end{array} \\tag{3.5}\n",
    "$$\n",
    "\n",
    "我们考虑 $n$ 步的回报（$n=1,2,\\cdots,\\infty$），如式(3.5)所示。Sarsa 属于单步更新算法，每执行一个动作，就会更新一次价值和策略。如果不进行单步更新，而是采取 $n$ 步更新或者回合更新，即在执行 $n$ 步之后再更新价值和策略，这样我们就得到了 **$n$ 步 Sarsa（$n$-step Sarsa）**。\n",
    "\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.15.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.29 Sarsa算法</div>\n",
    "\n",
    "\n",
    "\n",
    "比如 2步 Sarsa 就是执行两步后再来更新 Q函数的值。\n",
    "对于 Sarsa，在 $t$ 时刻的价值为\n",
    "$$\n",
    "\tQ_{t}=r_{t+1}+\\gamma Q\\left(s_{t+1}, a_{t+1}\\right)\n",
    "$$\n",
    "而对于 $n$ 步 Sarsa，它的 $n$ 步 Q 回报为\n",
    "$$\n",
    "\tQ_{t}^{n}=r_{t+1}+\\gamma r_{t+2}+\\ldots+\\gamma^{n-1} r_{t+n}+\\gamma^{n} Q\\left(s_{t+n}, a_{t+n}\\right)\n",
    "$$\n",
    "如果给 $Q_t^{n}$ 加上资格迹衰减参数（decay-rate parameter for eligibility traces）$\\lambda$ 并进行求和，即可得到 Sarsa($\\lambda$) 的 Q 回报\n",
    "$$\n",
    "\tQ_{t}^{\\lambda}=(1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} Q_{t}^{n}\n",
    "$$\n",
    "因此，$n$ 步 Sarsa($\\lambda$) 的更新策略为\n",
    "$$\n",
    "\tQ\\left(s_{t}, a_{t}\\right) \\leftarrow Q\\left(s_{t}, a_{t}\\right)+\\alpha\\left(Q_{t}^{\\lambda}-Q\\left(s_{t}, a_{t}\\right)\\right)\n",
    "$$\n",
    "总之，Sarsa 和 Sarsa($\\lambda$) 的差别主要体现在价值的更新上。\n",
    "\n",
    "了解单步更新的基本公式之后，代码实现就很简单了。如图 3.30 所示，右边是环境，左边是智能体。智能体每与环境交互一次之后，就可以学习一次，向环境输出动作，从环境当中获取状态和奖励。智能体主要实现两个方法：\n",
    "\n",
    "（1）根据 Q 表格选择动作，输出动作；\n",
    "\n",
    "（2）获取 $(s_{t}, a_{t}, r_{t+1}, s_{t+1}, a_{t+1})$  这几个值更新 Q 表格。\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.16.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.30 Sarsa代码实现示意</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf56de-abf9-428e-9ec9-6515c90a4c28",
   "metadata": {},
   "source": [
    "### 3.4.2 Q学习：异策略时序差分控制 \n",
    "\n",
    "Sarsa 是一种**同策略（on-policy）**算法，它优化的是它实际执行的策略，它直接用下一步会执行的动作去优化 Q 表格。同策略在学习的过程中，只存在一种策略，它用一种策略去做动作的选取，也用一种策略去做优化。所以 Sarsa 知道它下一步的动作有可能会跑到悬崖那边去，它就会在优化自己的策略的时候，尽可能离悬崖远一点。这样子就会保证，它下一步哪怕是有随机动作，它也还是在安全区域内。\n",
    "\n",
    "Q学习是一种**异策略（off-policy）**算法。如图 3.31 所示，异策略在学习的过程中，有两种不同的策略：**目标策略（target policy）**和**行为策略（behavior policy）**。\n",
    "目标策略是我们需要去学习的策略，一般用 $\\pi$ 来表示。目标策略就像是在后方指挥战术的一个军师，它可以根据自己的经验来学习最优的策略，不需要去和环境交互。\n",
    "行为策略是探索环境的策略，一般用 $\\mu$ 来表示。行为策略可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，然后把采集到的数据“喂”给目标策略学习。而且“喂”给目标策略的数据中并不需要 $a_{t+1}$ ，而 Sarsa 是要有 $a_{t+1}$ 的。行为策略像是一个战士，可以在环境里面探索所有的动作、轨迹和经验，然后把这些经验交给目标策略去学习。比如目标策略优化的时候，Q学习不会管我们下一步去往哪里探索，它只选取奖励最大的策略。\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.17.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.21 异策略</div>\n",
    "\n",
    "\n",
    "再例如，如图 3.32 所示，比如环境是波涛汹涌的大海，但学习策略（learning policy）太“胆小”了，无法直接与环境交互学习，所以我们有了探索策略（exploratory policy），探索策略是一个不畏风浪的海盗，它非常激进，可以在环境中探索。因此探索策略有很多经验，它可以把这些经验“写成稿子”，然后“喂”给学习策略。学习策略可以通过稿子进行学习。\n",
    "\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/off_policy_learning.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.32 异策略例子</div>\n",
    "\n",
    "\n",
    "\n",
    "在异策略学习的过程中，轨迹都是行为策略与环境交互产生的，产生这些轨迹后，我们使用这些轨迹来更新目标策略 $\\pi$。\n",
    "异策略学习有很多好处。首先，我们可以利用探索策略来学到最佳的策略，学习效率高；\n",
    "其次，异策略学习可以让我们学习其他智能体的动作，进行模仿学习，学习人或者其他智能体产生的轨迹；\n",
    "最后，异策略学习可以让我们重用旧的策略产生的轨迹，探索过程需要很多计算资源，这样可以节省资源。\n",
    "\n",
    "\n",
    "Q学习有两种策略：行为策略和目标策略。\n",
    "目标策略 $\\pi$ 直接在 Q表格上使用贪心策略，取它下一步能得到的所有状态，即\n",
    "$$\n",
    "\t\\pi\\left(s_{t+1}\\right)=\\underset{a^{\\prime}}{\\arg \\max}~ Q\\left(s_{t+1}, a^{\\prime}\\right)\n",
    "$$\n",
    "行为策略 $\\mu$ 可以是一个随机的策略，但我们采取 $\\varepsilon$-贪心策略，让行为策略不至于是完全随机的，它是基于Q表格逐渐改进的。\n",
    "\n",
    "我们可以构造 Q学习 目标，Q学习的下一个动作都是通过 arg max 操作选出来的，于是我们可得\n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_{t+1}+\\gamma Q\\left(s_{t+1}, A^{\\prime}\\right) &=r_{t+1}+\\gamma Q\\left(s_{t+1},\\arg \\max ~Q\\left(s_{t+1}, a^{\\prime}\\right)\\right) \\\\\n",
    "&=r_{t+1}+\\gamma \\max _{a^{\\prime}} Q\\left(s_{t+1}, a^{\\prime}\\right)\n",
    "\\end{aligned}\t\n",
    "$$\n",
    "\n",
    "接着我们可以把 Q学习更新写成增量学习的形式，时序差分目标变成了$r_{t+1}+\\gamma \\max _{a} Q\\left(s_{t+1}, a\\right)$，即\n",
    "$$\n",
    "\tQ\\left(s_{t}, a_{t}\\right) \\leftarrow Q\\left(s_{t}, a_{t}\\right)+\\alpha\\left[r_{t+1}+\\gamma \\max _{a} Q\\left(s_{t+1}, a\\right)-Q\\left(s_{t}, a_{t}\\right)\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "如图 3.33 所示，我们再通过对比的方式来进一步理解 **Q学习**。Q学习是异策略的时序差分学习方法，Sarsa 是同策略的时序差分学习方法。\n",
    "Sarsa 在更新 Q 表格的时候，它用到的是 $A'$ 。我们要获取下一个 Q 值的时候，$A'$ 是下一个步骤一定会执行的动作，这个动作有可能是 $\\varepsilon$-贪心方法采样出来的动作，也有可能是最大化 Q 值对应的动作，也有可能是随机动作，但这是它实际执行的动作。\n",
    "但是 Q学习 在更新 Q 表格的时候，它用到的是 Q 值 $Q(S',a)$ 对应的动作 ，它不一定是下一个步骤会执行的实际的动作，因为我们下一个实际会执行的那个动作可能会探索。\n",
    "Q学习默认的下一个动作不是通过行为策略来选取的，Q学习直接看Q表格，取它的最大化的值，它是默认 $A'$ 为最佳策略选取的动作，所以 Q学习 在学习的时候，不需要传入 $A'$，即 $a_{t+1}$  的值。\n",
    "\n",
    ">事实上，Q学习算法被提出的时间更早，Sarsa 算法是Q学习算法的改进。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.18.png\"/>\n",
    "</div>\t\n",
    "<div align=center>图 3.33 Sarsa与Q学习的伪代码</div>\n",
    "\n",
    "\n",
    "Sarsa 和 Q学习 的更新公式是一样的，区别只在目标计算的部分，\n",
    "Sarsa 是 $r_{t+1}+\\gamma Q(s_{t+1}, a_{t+1})$， \n",
    "Q学习 是 $r_{t+1}+\\gamma  \\underset{a}{\\max} Q\\left(s_{t+1}, a\\right)$ 。\n",
    "\n",
    "如图 3.34a 所示，Sarsa 用自己的策略产生了 $S,A,R,S',A'$ 这条轨迹，然后用 $Q(s_{t+1},a_{t+1})$ 去更新原本的 Q 值 $Q(s_t,a_t)$。 \n",
    "但是 Q学习 并不需要知道我们实际上选择哪一个动作 ，它默认下一个动作就是 Q 值最大的那个动作。Q学习知道实际上行为策略可能会有 0.1 的概率选择别的动作，但 Q 学习并不担心受到探索的影响，它默认按照最佳的策略去优化目标策略，所以它可以更大胆地去寻找最优的路径，它表现得比 Sarsa 大胆得多。\n",
    "\n",
    "如图 3.34b 所示，我们对Q学习进行逐步拆解，Q学习与 Sarsa 唯一不一样的就是并不需要提前知道 $A_2$ ，就能更新 $Q(S_1,A_1)$ 。在一个回合的训练当中，Q学习 在学习之前也不需要获取下一个动作 $A'$，它只需要前面的 $(S,A,R,S')$ ，这与 Sarsa 很不一样。 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.19.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.34 Sarsa与Q学习的区别</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130edc77-b0c4-4574-9fc0-8ad12dd5a19f",
   "metadata": {},
   "source": [
    "### 3.4.3 同策略与异策略的区别 \n",
    "\n",
    "总结一下同策略和异策略的区别。\n",
    "* Sarsa 是一个典型的同策略算法，它只用了一个策略 $\\pi$，它不仅使用策略 $\\pi$ 学习，还使用策略 $\\pi$ 与环境交互产生经验。\n",
    "如果策略采用 $\\varepsilon$-贪心算法，它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点“胆小”。它在解决悬崖行走问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心探索了一点儿，也还是在安全区域内。此外，因为采用的是 $\\varepsilon$-贪心 算法，策略会不断改变（$\\varepsilon$ 值会不断变小），所以策略不稳定。\n",
    "* Q学习是一个典型的异策略算法，它有两种策略————目标策略和行为策略，它分离了目标策略与行为策略。Q学习可以大胆地用行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。行为策略可以采用 $\\varepsilon$-贪心 算法，但目标策略采用的是贪心算法，它直接根据行为策略采集到的数据来采用最佳策略，所以 Q学习 不需要兼顾探索。\n",
    "\n",
    "* 我们比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q学习是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。\n",
    "\n",
    "表格型方法总结如图 3.35 所示。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"./img/3.21.png\"/>\n",
    "</div>\n",
    "<div align=center>图 3.35 表格型方法总结</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db9fe8-f66f-4b32-8652-c7d2c73467a3",
   "metadata": {},
   "source": [
    "# 习题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cda4c-67e8-42ea-8f0b-26daba50ee6b",
   "metadata": {},
   "source": [
    "### 为什么在马尔可夫奖励过程（MRP）中需要有**discount factor**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d342e-71cd-49a8-92a0-d0da7aaaaf25",
   "metadata": {},
   "source": [
    "1. 首先，是有些马尔可夫过程是**带环**的，它并没有终结，然后我们想**避免这个无穷的奖励**；\n",
    "  2. 另外，我们是想把这个**不确定性**也表示出来，希望**尽可能快**地得到奖励，而不是在未来某一个点得到奖励；\n",
    "  3. 接上面一点，如果这个奖励是它是有实际价值的了，我们可能是更希望立刻就得到奖励，而不是我们后面再得到奖励。\n",
    "  4. 还有在有些时候，这个系数也可以把它设为 0。比如说，当我们设为 0 过后，然后我们就只关注了它当前的奖励。我们也可以把它设为 1，设为 1 的话就是对未来并没有折扣，未来获得的奖励跟我们当前获得的奖励是一样的。\n",
    "\n",
    "  所以，这个系数其实是应该可以作为强化学习 agent 的一个 hyperparameter 来进行调整，然后就会得到不同行为的 agent。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbad4fb-7802-4d1a-8ae8-766184504250",
   "metadata": {},
   "source": [
    "### 为什么矩阵形式的Bellman Equation的解析解比较难解？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb85bce-c6fb-4ffa-8402-50458ab4e14c",
   "metadata": {},
   "source": [
    "通过矩阵求逆的过程，就可以把这个 V 的这个价值的解析解直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是 $O(N^3)$。所以就当我们状态非常多的时候，比如说从我们现在十个状态到一千个状态，到一百万个状态。那么当我们有一百万个状态的时候，这个转移矩阵就会是个一百万乘以一百万的一个矩阵。这样一个大矩阵的话求逆是非常困难的，所以这种通过解析解去解，只能对于很小量的MRP。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8913d-7f38-4a60-af2b-6d149bb21d4c",
   "metadata": {},
   "source": [
    "### 计算贝尔曼等式（Bellman Equation）的常见方法以及区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e50e7-cb8c-482b-a4b9-23da87c85e5a",
   "metadata": {},
   "source": [
    " 1. **Monte Carlo Algorithm（蒙特卡罗方法）：** 可用来计算价值函数的值。通俗的讲，我们当得到一个MRP过后，我们可以从某一个状态开始，然后让它让把这个小船放进去，让它随波逐流，这样就会产生一个轨迹。产生了一个轨迹过后，就会得到一个奖励，那么就直接把它的 Discounted 的奖励 $g$ 直接算出来。算出来过后就可以把它积累起来，当积累到一定的轨迹数量过后，然后直接除以这个轨迹，然后就会得到它的这个价值。\n",
    "  2. **Iterative Algorithm（动态规划方法）：** 可用来计算价值函数的值。通过一直迭代对应的Bellman Equation，最后使其收敛。当这个最后更新的状态跟你上一个状态变化并不大的时候，通常是小于一个阈值 $\\gamma$ ，这个更新就可以停止。\n",
    "  3. **以上两者的结合方法：** 另外我们也可以通过 Temporal-Difference Learning 的那个办法。这个 `Temporal-Difference Learning` 叫 `TD Leanring`，就是动态规划和蒙特卡罗的一个结合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0ba8f-832e-454e-93d7-0f77835d0b51",
   "metadata": {},
   "source": [
    "### 马尔可夫奖励过程（MRP）与马尔可夫决策过程 （MDP）的区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a714f6c-5d11-4a5f-b9b4-2d92100da892",
   "metadata": {},
   "source": [
    "相对于 MRP，马尔可夫决策过程(Markov Decision Process)多了一个 decision，其它的定义跟 MRP 都是类似的。这里我们多了一个决策，多了一个 action ，那么这个状态转移也多了一个 condition，就是采取某一种行为，然后你未来的状态会不同。它不仅是依赖于你当前的状态，也依赖于在当前状态你这个 agent 它采取的这个行为会决定它未来的这个状态走向。对于这个价值函数，它也是多了一个条件，多了一个你当前的这个行为，就是说你当前的状态以及你采取的行为会决定你在当前可能得到的奖励多少。\n",
    "\n",
    "  另外，两者之间是有转换关系的。具体来说，已知一个 MDP 以及一个 policy $\\pi$ 的时候，我们可以把 MDP 转换成MRP。在 MDP 里面，转移函数 $P(s'|s,a)$  是基于它当前状态以及它当前的 action，因为我们现在已知它 policy function，就是说在每一个状态，我们知道它可能采取的行为的概率，那么就可以直接把这个 action 进行加和，那我们就可以得到对于 MRP 的一个转移，这里就没有 action。同样地，对于奖励，我们也可以把 action 拿掉，这样就会得到一个类似于 MRP 的奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45450a76-9e7c-49fc-b7cb-56018afdfb6e",
   "metadata": {},
   "source": [
    "### MDP 里面的状态转移跟 MRP 以及 MP 的结构或者计算方面的差异？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952defbb-7196-4eba-ac52-5c4593cc8ef5",
   "metadata": {},
   "source": [
    "- 对于之前的马尔可夫链的过程，它的转移是直接就决定，就从你当前是 s，那么就直接通过这个转移概率就直接决定了你下一个状态会是什么。\n",
    "  \n",
    "- 但是对于 MDP，它的中间多了一层这个行为 a ，就是说在你当前这个状态的时候，你首先要决定的是采取某一种行为。然后因为你有一定的不确定性，当你当前状态决定你当前采取的行为过后，你到未来的状态其实也是一个概率分布。所以你采取行为以及你决定，然后你可能有有多大的概率到达某一个未来状态，以及另外有多大概率到达另外一个状态。所以在这个当前状态跟未来状态转移过程中这里多了一层决策性，这是MDP跟之前的马尔可夫过程很不同的一个地方。在马尔科夫决策过程中，行为是由 agent 决定，所以多了一个 component，agent 会采取行为来决定未来的状态转移。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52f8e8-ed9f-45d0-bebf-3f8b9fcfced3",
   "metadata": {},
   "source": [
    "### 我们如何寻找最佳的policy，方法有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8deb4-9a51-4929-bc48-99630d01f78b",
   "metadata": {},
   "source": [
    "本质来说，当我们取得最佳的价值函数过后，我们可以通过对这个 Q 函数进行极大化，然后得到最佳的价值。然后，我们直接在这个Q函数上面取一个让这个action最大化的值，然后我们就可以直接提取出它的最佳的policy。\n",
    "\n",
    "  具体方法：\n",
    "\n",
    "  1. **穷举法（一般不使用）：**假设我们有有限多个状态、有限多个行为可能性，那么每个状态我们可以采取这个 A 种行为的策略，那么总共就是 $|A|^{|S|}$ 个可能的 policy。我们可以把这个穷举一遍，然后算出每种策略的 value function，然后对比一下可以得到最佳策略。但是效率极低。\n",
    "  2. **Policy iteration：** 一种迭代方法，有两部分组成，下面两个步骤一直在迭代进行，最终收敛：(有些类似于ML中EM算法（期望-最大化算法）)\n",
    "     - 第一个步骤是 **policy evaluation** ，即当前我们在优化这个 policy $\\pi$ ，所以在优化过程中得到一个最新的这个 policy 。\n",
    "     - 第二个步骤是 **policy improvement** ，即取得价值函数后，进一步推算出它的 Q 函数。得到 Q 函数过后，那我们就直接去取它的极大化。\n",
    "  3. **Value iteration:** 我们一直去迭代 Bellman Optimality Equation，到了最后，它能逐渐趋向于最佳的策略，这是 value iteration 算法的精髓，就是我们去为了得到最佳的 $v^*$ ，对于每个状态它的 $v^*$ 这个值，我们直接把这个 Bellman Optimality Equation 进行迭代，迭代了很多次之后它就会收敛到最佳的policy以及其对应的状态，这里面是没有policy function的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e3fea-51ba-4dfc-aabb-030bdd17e75d",
   "metadata": {},
   "source": [
    "### 构成强化学习MDP的四元组有哪些变量？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc28398-51db-4130-a56a-0b085fb20876",
   "metadata": {},
   "source": [
    "状态、动作、状态转移概率和奖励，分别对应（S，A，P，R），后面有可能会加上个衰减因子构成五元组。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebfc298-d99e-4c35-b01a-7c579a691147",
   "metadata": {},
   "source": [
    "### 基于以上的描述所构成的强化学习的“学习”流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f96815-9ff8-4048-84d4-44e76cc88c03",
   "metadata": {},
   "source": [
    "强化学习要像人类一样去学习了，人类学习的话就是一条路一条路的去尝试一下，先走一条路，我看看结果到底是什么。多试几次，只要能一直走下去的，我们其实可以慢慢的了解哪个状态会更好。我们用价值函数 $V(s)$ 来代表这个状态是好的还是坏的。然后用这个 Q 函数来判断说在什么状态下做什么动作能够拿到最大奖励，我们用 Q 函数来表示这个状态-动作值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f6195f-dfef-46b9-9970-e62c7da3cb0a",
   "metadata": {},
   "source": [
    "### 基于SARSA算法的agent的学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16e72e-ddba-4d7b-998f-53b742894cd4",
   "metadata": {},
   "source": [
    "我们现在有环境，有agent。每交互一次以后，我们的agent会向环境输出action，接着环境会反馈给agent当前时刻的state和reward。那么agent此时会实现两个方法：\n",
    "  \n",
    "  1.使用已经训练好的Q表格，对应环境反馈的state和reward选取对应的action进行输出。\n",
    "  \n",
    "  2.我们已经拥有了$(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$  这几个值，并直接使用 $A_{t+1}$ 去更新我们的Q表格。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8bd312-509e-43d4-bf67-331a8b3df2e1",
   "metadata": {},
   "source": [
    "### Q-learning和Sarsa的区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6373c-432e-4269-b954-766eeb8a0deb",
   "metadata": {},
   "source": [
    "Sarsa算法是Q-learning算法的改进。（这句话出自「神经网络与深度学习」的第 342 页）（可参考SARSA「on-line q-learning using connectionist systems」的 abstract 部分）\n",
    "\n",
    "  1. 首先，Q-learning 是 off-policy 的时序差分学习方法，Sarsa 是 on-policy 的时序差分学习方法。\n",
    "\n",
    "  2. 其次，Sarsa 在更新 Q 表格的时候，它用到的 A' 。我要获取下一个 Q 值的时候，A' 是下一个 step 一定会执行的 action 。这个 action 有可能是 $\\varepsilon$-greddy 方法 sample 出来的值，也有可能是 max Q 对应的 action，也有可能是随机动作。但是就是它实实在在执行了的那个动作。\n",
    "\n",
    "  3. 但是 Q-learning 在更新 Q 表格的时候，它用到这个的 Q 值 $Q(S',a')$ 对应的那个 action ，它不一定是下一个 step 会执行的实际的 action，因为你下一个实际会执行的那个 action 可能会探索。Q-learning 默认的 action 不是通过 behavior policy 来选取的，它是默认 A' 为最优策略选的动作，所以 Q-learning 在学习的时候，不需要传入 A'，即 $a_{t+1}$  的值。\n",
    "\n",
    "  4. 更新公式的对比（区别只在target计算这一部分）：\n",
    "\n",
    "     - Sarsa的公式： $R_{t+1}+\\gamma Q(S_{t+1}, A_{t+1})$ ；\n",
    "     - Q-learning的公式：$R_{t+1}+\\gamma  \\underset{a}{\\max} Q\\left(S_{t+1}, a\\right)$\n",
    "\n",
    "     Sarsa 实际上都是用自己的策略产生了 S,A,R,S',A' 这一条轨迹。然后拿着 $Q(S_{t+1},A_{t+1})$ 去更新原本的 Q 值 $Q(S_t,A_t)$。 但是 Q-learning 并不需要知道，我实际上选择哪一个 action ，它默认下一个动作就是 Q 最大的那个动作。所以基于此，Sarsa的action通常会更加“保守”、“胆小”，而对应的Q-Learning的action会更加“莽撞”、“激进”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc21713-adf1-4d70-8675-c872c6e36f10",
   "metadata": {},
   "source": [
    "### On-policy和 off-policy 的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afa52b-b942-4a20-82a3-2dc4193edc4f",
   "metadata": {},
   "source": [
    "1. Sarsa 就是一个典型的 on-policy 策略，它只用一个 $\\pi$ ，为了兼顾探索和利用，所以它训练的时候会显得有点胆小怕事。它在解决悬崖问题的时候，会尽可能地离悬崖边上远远的，确保说哪怕自己不小心探索了一点了，也还是在安全区域内不不至于跳进悬崖。\n",
    "\n",
    "2. Q-learning 是一个比较典型的 off-policy 的策略，它有目标策略 target policy，一般用 $\\pi$ 来表示。然后还有行为策略 behavior policy，用 $\\mu$ 来表示。它分离了目标策略跟行为策略。Q-learning 就可以大胆地用 behavior policy 去探索得到的经验轨迹来去优化我的目标策略。这样子我更有可能去探索到最优的策略。\n",
    "\n",
    "3. 比较 Q-learning 和 Sarsa 的更新公式可以发现，Sarsa 并没有选取最大值的 max 操作。因此，Q-learning 是一个非常激进的算法，希望每一步都获得最大的利益；而 Sarsa 则相对非常保守，会选择一条相对安全的迭代路线。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df77463-cdf0-409c-968f-31a3868af955",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77038a-5f28-46cf-aabe-665c37a3285b",
   "metadata": {},
   "source": [
    "- **马尔可夫性质(Markov Property):** 如果某一个过程未来的转移跟过去是无关，只由现在的状态决定，那么其满足马尔可夫性质。换句话说，一个状态的下一个状态只取决于它当前状态，而跟它当前状态之前的状态都没有关系。\n",
    "- **马尔可夫链(Markov Chain):** 概率论和数理统计中具有马尔可夫性质（Markov property）且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process）。\n",
    "- **状态转移矩阵(State Transition Matrix):** 状态转移矩阵类似于一个 conditional probability，当我们知道当前我们在 $s_t$ 这个状态过后，到达下面所有状态的一个概念，它每一行其实描述了是从一个节点到达所有其它节点的概率。\n",
    "- **马尔可夫奖励过程(Markov Reward Process, MRP)：** 即马尔可夫链再加上了一个奖励函数。在 MRP之中，转移矩阵跟它的这个状态都是跟马尔可夫链一样的，多了一个奖励函数(reward function)。奖励函数是一个期望，它说当你到达某一个状态的时候，可以获得多大的奖励。\n",
    "- **horizon:** 定义了同一个 episode 或者是整个一个轨迹的长度，它是由有限个步数决定的。\n",
    "- **return:** 把奖励进行折扣(discounted)，然后获得的对应的收益。\n",
    "- **Bellman Equation（贝尔曼等式）:** 定义了当前状态与未来状态的迭代关系，表示当前状态的值函数可以通过下个状态的值函数来计算。Bellman Equation 因其提出者、动态规划创始人 Richard Bellman 而得名 ，同时也被叫作“动态规划方程”。$V(s)=R(S)+ \\gamma \\sum_{s' \\in S}P(s'|s)V(s')$ ，特别地，矩阵形式：$V=R+\\gamma PV$。\n",
    "- **Monte Carlo Algorithm（蒙特卡罗方法）：** 可用来计算价值函数的值。通俗的讲，我们当得到一个MRP过后，我们可以从某一个状态开始，然后让它让把这个小船放进去，让它随波逐流，这样就会产生一个轨迹。产生了一个轨迹过后，就会得到一个奖励，那么就直接把它的 Discounted 的奖励 $g$ 直接算出来。算出来过后就可以把它积累起来，当积累到一定的轨迹数量过后，然后直接除以这个轨迹，然后就会得到它的这个价值。\n",
    "- **Iterative Algorithm（动态规划方法）：** 可用来计算价值函数的值。通过一直迭代对应的Bellman Equation，最后使其收敛。当这个最后更新的状态跟你上一个状态变化并不大的时候，这个更新就可以停止。\n",
    "- **Q函数 (action-value function)：** 其定义的是某一个状态某一个行为，对应的它有可能得到的 return 的一个期望（over policy function）。\n",
    "- **MDP中的prediction（即policy evaluation问题）：** 给定一个 MDP 以及一个 policy $\\pi$ ，去计算它的 value function，即每个状态它的价值函数是多少。其可以通过动态规划方法（Iterative Algorithm）解决。\n",
    "- **MDP中的control问题：** 寻找一个最佳的一个策略，它的 input 就是MDP，输出是通过去寻找它的最佳策略，然后同时输出它的最佳价值函数(optimal value function)以及它的这个最佳策略(optimal policy)。其可以通过动态规划方法（Iterative Algorithm）解决。\n",
    "- **最佳价值函数(Optimal Value Function)：** 我们去搜索一种 policy $\\pi$ ，然后我们会得到每个状态它的状态值最大的一个情况，$v^*$ 就是到达每一个状态，它的值的极大化情况。在这种极大化情况上面，我们得到的策略就可以说它是最佳策略(optimal policy)。optimal policy 使得每个状态，它的状态函数都取得最大值。所以当我们说某一个 MDP 的环境被解了过后，就是说我们可以得到一个 optimal value function，然后我们就说它被解了。\n",
    "\n",
    "- **P函数和R函数：** P函数反应的是状态转移的概率，即反应的环境的随机性，R函数就是Reward function。但是我们通常处于一个未知的环境（即P函数和R函数是未知的）。\n",
    "- **Q表格型表示方法：** 表示形式是一种表格形式，其中横坐标为 action（agent）的行为，纵坐标是环境的state，其对应着每一个时刻agent和环境的情况，并通过对应的reward反馈去做选择。一般情况下，Q表格是一个已经训练好的表格，不过，我们也可以每进行一步，就更新一下Q表格，然后用下一个状态的Q值来更新这个状态的Q值（即时序差分方法）。\n",
    "- **时序差分（Temporal Difference）：** 一种Q函数（Q值）的更新方式，也就是可以拿下一步的 Q 值 $Q(S_{t+_1},A_{t+1})$ 来更新我这一步的 Q 值 $Q(S_t,A_t)$ 。完整的计算公式如下：$Q(S_t,A_t) \\larr Q(S_t,A_t) + \\alpha [R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]$\n",
    "- **SARSA算法：** 一种更新前一时刻状态的单步更新的强化学习算法，也是一种on-policy策略。该算法由于每次更新值函数需要知道前一步的状态(state)，前一步的动作(action)、奖励(reward)、当前状态(state)、将要执行的动作(action)，即 $(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$ 这几个值，所以被称为SARSA算法。agent每进行一次循环，都会用 $(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$ 对于前一步的Q值（函数）进行一次更新。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
