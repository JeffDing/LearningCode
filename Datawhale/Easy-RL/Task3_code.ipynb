{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf851e15-586c-4bdf-8d61-6ee38a372f42",
   "metadata": {},
   "source": [
    "### Policy Gradient Pytorch实现\n",
    "\n",
    "代码来源（[https://github.com/JohnComeon/RL/blob/master/Policy%20Gradient/policy%20gradient.py](https://github.com/JohnComeon/RL/blob/master/Policy%20Gradient/policy%20gradient.py)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb2051-a87e-4aa2-b0b3-3d44d2b34abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61a169-4830-4719-b9f8-dc523b55b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import gym\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Pytorch REINFORCE example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',help='discount factor(default:0.99)')\n",
    "parser.add_argument('--seed',type=int, default=543, metavar='N',help='random seed (default: 543)')\n",
    "parser.add_argument('--render',action='store_false',help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='interval between training status logs (default: 10)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)    # 策略梯度算法方差很大，设置seed以保证复现性\n",
    "print('observation space:',env.observation_space)\n",
    "print('action space:',env.action_space)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    ##  离散空间采用了 softmax policy 来参数化策略\n",
    "    def __init__(self):\n",
    "        super(Policy,self).__init__()\n",
    "        self.affline1 = nn.Linear(4,128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affline2 = nn.Linear(128,2)  # 两种动作\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affline1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affline2(x)\n",
    "        return F.softmax(action_scores,dim=1)\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(),lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()  # 非负的最小值，使得归一化时分母不为0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    ## 选择动作，这个动作不是根据Q值来选择，而是使用softmax生成的概率来选\n",
    "    #  不需要epsilon-greedy，因为概率本身就具有随机性\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    #print(state.shape)   torch.size([1,4])\n",
    "    probs = policy(state)\n",
    "    # print(probs)\n",
    "    # print(probs.log())\n",
    "    m = Categorical(probs)      # 生成分布\n",
    "    action = m.sample()           # 从分布中采样\n",
    "    #print(m.log_prob(action))   # m.log_prob(action)相当于probs.log()[0][action.item()].unsqueeze(0)\n",
    "    policy.saved_log_probs.append(m.log_prob(action))    # 取对数似然 logπ(s,a)\n",
    "    return action.item()         # 返回一个元素值\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0,R)        # 将R插入到指定的位置0处\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)     # 归一化\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)          # 损失函数为交叉熵\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()          # 求和\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]          # 清空episode 数据\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in range(1000):        # 采集（训练）最多1000个序列\n",
    "        state, ep_reward = env.reset(),0    # ep_reward表示每个episode中的reward\n",
    "        #print(state.shape)\n",
    "        for t in range(1, 1000):\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:   # 大于游戏的最大阈值475时，退出游戏\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162d010-560e-4d8a-88ef-d185a8f010c2",
   "metadata": {},
   "source": [
    "### Policy Gradient TF2.0实现\n",
    "\n",
    "代码来源（[https://github.com/louisnino/RLcode/blob/master/tutorial_PG.py](https://github.com/louisnino/RLcode/blob/master/tutorial_PG.py)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ee9a8-d115-4c86-a683-22e9ba6bd6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vanilla Policy Gradient(VPG or REINFORCE)\n",
    "-----------------------------------------\n",
    "The policy gradient algorithm works by updating policy parameters via stochastic gradient ascent on policy performance.\n",
    "It's an on-policy algorithm can be used for environments with either discrete or continuous action spaces.\n",
    "Here is an example on discrete action space game CartPole-v0.\n",
    "To apply it on continuous action space, you need to change the last softmax layer and the choose_action function.\n",
    "Reference\n",
    "---------\n",
    "Cookbook: Barto A G, Sutton R S. Reinforcement Learning: An Introduction[J]. 1998.\n",
    "MorvanZhou's tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Environment\n",
    "-----------\n",
    "Openai Gym CartPole-v0, discrete action space\n",
    "Prerequisites\n",
    "--------------\n",
    "tensorflow >=2.0.0a0\n",
    "tensorflow-probability 0.6.0\n",
    "tensorlayer >=2.0.0\n",
    "To run\n",
    "------\n",
    "python tutorial_PG.py --train/test\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorlayer as tl\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "parser.add_argument('--train', dest='train', action='store_true', default=True)\n",
    "parser.add_argument('--test', dest='train', action='store_false')\n",
    "args = parser.parse_args()\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'    # 定义环境\n",
    "RANDOMSEED = 1              # 设置随机种子。建议大家都设置，这样试验可以重现。\n",
    "\n",
    "DISPLAY_REWARD_THRESHOLD = 400  # 如果奖励超过DISPLAY_REWARD_THRESHOLD，就开始渲染\n",
    "RENDER = False                  # 开始的时候，不渲染游戏。\n",
    "num_episodes = 2                # 游戏迭代次数\n",
    "\n",
    "###############################  PG  ####################################\n",
    "\n",
    "\n",
    "class PolicyGradient:\n",
    "    \"\"\"\n",
    "    PG class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, n_actions, learning_rate=0.01, reward_decay=0.95):\n",
    "        # 定义相关参数\n",
    "        self.n_actions = n_actions      #动作\n",
    "        self.n_features = n_features    #环境特征数量\n",
    "        self.lr = learning_rate         #学习率\n",
    "        self.gamma = reward_decay       #折扣\n",
    "\n",
    "        #用于保存每个ep的数据。\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "\n",
    "        def get_model(inputs_shape):\n",
    "            \"\"\"\n",
    "            创建一个神经网络\n",
    "            输入: state\n",
    "            输出: act\n",
    "            \"\"\"\n",
    "            with tf.name_scope('inputs'):\n",
    "                self.tf_obs = tl.layers.Input(inputs_shape, tf.float32, name=\"observations\")\n",
    "                #self.tf_acts = tl.layers.Input([None,], tf.int32, name=\"actions_num\")\n",
    "                #self.tf_vt = tl.layers.Input([None,], tf.float32, name=\"actions_value\")\n",
    "            # fc1\n",
    "            layer = tl.layers.Dense(\n",
    "                n_units=30, act=tf.nn.tanh, W_init=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "                b_init=tf.constant_initializer(0.1), name='fc1'\n",
    "            )(self.tf_obs)\n",
    "            # fc2\n",
    "            all_act = tl.layers.Dense(\n",
    "                n_units=self.n_actions, act=None, W_init=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "                b_init=tf.constant_initializer(0.1), name='all_act'\n",
    "            )(layer)\n",
    "            return tl.models.Model(inputs=self.tf_obs, outputs=all_act, name='PG model')\n",
    "\n",
    "        self.model = get_model([None, n_features])\n",
    "        self.model.train()\n",
    "        self.optimizer = tf.optimizers.Adam(self.lr)\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        \"\"\"\n",
    "        用神经网络输出的**策略pi**，选择动作。\n",
    "        输入: state\n",
    "        输出: act\n",
    "        \"\"\"\n",
    "        _logits = self.model(np.array([s], np.float32))     \n",
    "        _probs = tf.nn.softmax(_logits).numpy()             \n",
    "        return tl.rein.choice_action_by_probs(_probs.ravel())   #根据策略PI选择动作。\n",
    "\n",
    "    def choose_action_greedy(self, s):\n",
    "        \"\"\"\n",
    "        贪心算法：直接用概率最大的动作\n",
    "        输入: state\n",
    "        输出: act\n",
    "        \"\"\"\n",
    "        _probs = tf.nn.softmax(self.model(np.array([s], np.float32))).numpy()\n",
    "        return np.argmax(_probs.ravel())\n",
    "\n",
    "    def store_transition(self, s, a, r):\n",
    "        \"\"\"\n",
    "        保存数据到buffer中\n",
    "        \"\"\"\n",
    "        self.ep_obs.append(np.array([s], np.float32))\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        通过带权重更新方法更新神经网络\n",
    "        \"\"\"\n",
    "        # _discount_and_norm_rewards中存储的就是这一ep中，每个状态的G值。\n",
    "        discounted_ep_rs_norm = self._discount_and_norm_rewards()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # 把s放入神经网络，就算_logits\n",
    "            _logits = self.model(np.vstack(self.ep_obs))\n",
    "            \n",
    "            # 敲黑板\n",
    "            ## _logits和真正的动作的差距\n",
    "            # 差距也可以这样算,和sparse_softmax_cross_entropy_with_logits等价的:\n",
    "            # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1)\n",
    "            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=_logits, labels=np.array(self.ep_as))\n",
    "\n",
    "            # 在原来的差距乘以G值，也就是以G值作为更新\n",
    "            loss = tf.reduce_mean(neg_log_prob * discounted_ep_rs_norm)\n",
    "\n",
    "        grad = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n",
    "\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []  # empty episode data\n",
    "        return discounted_ep_rs_norm\n",
    "\n",
    "    def _discount_and_norm_rewards(self):\n",
    "        \"\"\"\n",
    "        通过回溯计算G值\n",
    "        \"\"\"\n",
    "        # 先创建一个数组，大小和ep_rs一样。ep_rs记录的是每个状态的收获r。\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "        # 从ep_rs的最后往前，逐个计算G\n",
    "        for t in reversed(range(0, len(self.ep_rs))):\n",
    "            running_add = running_add * self.gamma + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "\n",
    "        # 归一化G值。\n",
    "        # 我们希望G值有正有负，这样比较容易学习。\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)\n",
    "        return discounted_ep_rs\n",
    "\n",
    "    def save_ckpt(self):\n",
    "        \"\"\"\n",
    "        save trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if not os.path.exists('model'):\n",
    "            os.makedirs('model')\n",
    "        tl.files.save_weights_to_hdf5('model/pg_policy.hdf5', self.model)\n",
    "\n",
    "    def load_ckpt(self):\n",
    "        \"\"\"\n",
    "        load trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        tl.files.load_hdf5_to_weights_in_order('model/pg_policy.hdf5', self.model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # reproducible\n",
    "    np.random.seed(RANDOMSEED)\n",
    "    tf.random.set_seed(RANDOMSEED)\n",
    "\n",
    "    tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env.seed(RANDOMSEED)  # reproducible, general Policy gradient has high variance\n",
    "    env = env.unwrapped\n",
    "\n",
    "    print(env.action_space)\n",
    "    print(env.observation_space)\n",
    "    print(env.observation_space.high)\n",
    "    print(env.observation_space.low)\n",
    "\n",
    "    RL = PolicyGradient(\n",
    "        n_actions=env.action_space.n,\n",
    "        n_features=env.observation_space.shape[0],\n",
    "        learning_rate=0.02,\n",
    "        reward_decay=0.99,\n",
    "        # output_graph=True,\n",
    "    )\n",
    "\n",
    "    if args.train:\n",
    "        reward_buffer = []\n",
    "\n",
    "        #=====开始更新训练=====\n",
    "        for i_episode in range(num_episodes):\n",
    "\n",
    "            episode_time = time.time()\n",
    "            observation = env.reset()\n",
    "\n",
    "            while True:\n",
    "                if RENDER:\n",
    "                    env.render()\n",
    "\n",
    "                # 注意：这里没有用贪婪算法，而是根据pi随机动作，以保证一定的探索性。\n",
    "                action = RL.choose_action(observation)\n",
    "\n",
    "                observation_, reward, done, info = env.step(action)\n",
    "\n",
    "                # 保存数据\n",
    "                RL.store_transition(observation, action, reward)\n",
    "\n",
    "                # PG用的是MC，如果到了最终状态\n",
    "                if done:\n",
    "                    ep_rs_sum = sum(RL.ep_rs)\n",
    "\n",
    "                    if 'running_reward' not in globals():\n",
    "                        running_reward = ep_rs_sum\n",
    "                    else:\n",
    "                        running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "\n",
    "                    #如果超过DISPLAY_REWARD_THRESHOLD就开始渲染游戏吧。\n",
    "                    if running_reward > DISPLAY_REWARD_THRESHOLD:\n",
    "                        RENDER = True \n",
    "\n",
    "                    # print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "\n",
    "                    print(\n",
    "                        \"Episode [%d/%d] \\tsum reward: %d  \\trunning reward: %f \\ttook: %.5fs \" %\n",
    "                        (i_episode, num_episodes, ep_rs_sum, running_reward, time.time() - episode_time)\n",
    "                    )\n",
    "                    reward_buffer.append(running_reward)\n",
    "\n",
    "                    # 开始学习\n",
    "                    vt = RL.learn()\n",
    "\n",
    "                    # 画图\n",
    "                    plt.ion()\n",
    "                    plt.cla()\n",
    "                    plt.title('PG')\n",
    "                    plt.plot(reward_buffer, )  \n",
    "                    plt.xlabel('episode steps')\n",
    "                    plt.ylabel('normalized state-action value')\n",
    "                    plt.show()\n",
    "                    plt.pause(0.1)\n",
    "\n",
    "                    break\n",
    "                \n",
    "                # 开始新一步\n",
    "                observation = observation_\n",
    "        RL.save_ckpt()\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "\n",
    "    # =====test=====\n",
    "    RL.load_ckpt()\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = RL.choose_action(observation)      # 这里建议大家可以改贪婪算法获取动作，对比效果是否有不同。\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            observation = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
