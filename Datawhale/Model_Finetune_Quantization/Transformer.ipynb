{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cac8669",
   "metadata": {},
   "source": [
    "Transformeræ¶æ„\n",
    "\n",
    "é˜¶æ®µ                 è¾“å…¥ç»´åº¦              è¾“å‡ºç»´åº¦\n",
    "åµŒå…¥å±‚(Embedding)     (B,L)               (B,L,D)\n",
    "\n",
    "ä½ç½®ç¼–ç               (B,L,D)              (B,L,D)\n",
    "\n",
    "ä¸­é—´è¿ç®—å±‚             (B,L,D)              (B,L,D)\n",
    "\n",
    "åˆ†ç±»å±‚(Linear)        (B,L,D)              (B,L,V)\n",
    "\n",
    "æ¯ä¸ªæ¿å—çš„æ ¸å¿ƒæœ¬è´¨ï¼š\n",
    "- åµŒå…¥å±‚ï¼ˆEmbeddingï¼‰ï¼šç‰¹å¾æ˜ å°„å°†ç¦»æ•£çš„å•è¯IDæ˜ å°„ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤ºï¼Œä¸ºæ¨¡å‹æä¾›åˆå§‹çš„è¯­ä¹‰åæ ‡ï¼ˆè¿ç»­å‘é‡å¯ä»¥ç”¨äºåšå‘é‡è¿ç®—ï¼‰ã€‚\n",
    "- ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼šé€šè¿‡æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œè®©æ¨¡å‹çŸ¥é“åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ çš„ç›¸å¯¹ä½ç½®ï¼Œæ‰“ç ´è¯è¢‹æ¨¡å‹çš„å±€é™æ€§ã€‚\n",
    "- ä¸­é—´è¿ç®—å±‚ï¼ˆSelf-Attention & MLPï¼‰ï¼šé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿå…³æ³¨åºåˆ—ä¸­çš„ä¸åŒä½ç½®ï¼ŒåŠ¨æ€è°ƒæ•´ç‰¹å¾è¡¨ç¤ºã€‚\n",
    "- åˆ†ç±»å±‚ï¼ˆLinearï¼‰ï¼šå›å½’é¢„æµ‹å°†æ·±å±‚ç‰¹å¾æ˜ å°„å›åºå¤§çš„è¯è¡¨ç©ºé—´ï¼Œè®¡ç®—ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9ba45",
   "metadata": {},
   "source": [
    "ä¸€ã€åŸºæœ¬æ¿å—çš„æ„å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731dabe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. è‡ªå®šä¹‰çº¿æ€§å±‚çš„å®ç°ï¼ˆæ²¡æœ‰åç½®é¡¹ï¼‰\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        factory_kwargs = {'device':device, 'dtype':dtype}\n",
    "        self.weight = nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "\n",
    "        # æˆªæ–­æ­£æ€åˆ†å¸ƒä½œä¸ºåˆå§‹åŒ–æƒé‡ï¼ˆå¯ä»¥è‡ªå®šä¹‰ï¼‰\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # è®¡ç®—æ ‡å‡†å·®sigma\n",
    "        # sigma^2 = 2/(d_in + d_out)\n",
    "        sigma = math.sqrt(2.0 / (self.in_features + self.out_features))\n",
    "        \n",
    "        # è‡ªå®šä¹‰æˆªæ–­èŒƒå›´[-3sigma, 3sigma]\n",
    "        # trunc_normal_çš„å‚æ•°ï¼štensor, mean, std, a(min), b (max)\n",
    "        init.trunc_normal_(\n",
    "            self.weight,\n",
    "            mean = 0.0,\n",
    "            std = sigma,\n",
    "            a = -3.0*sigma,\n",
    "            b = 3.0*sigma\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # æƒé‡ä¸º(out, in)ï¼Œè¾“å…¥æ–‡æœ¬æ•°æ®ä¸º(batch, in)å…¶ä¸­batchè¡¨ç¤ºå¤„ç†å¤šå°‘æ ·æœ¬inè¡¨ç¤ºç‰¹å¾å‘é‡ä¹Ÿå°±æ˜¯æ¯å¥è¯çš„ç¼–ç é•¿åº¦ \n",
    "        # éœ€è¦ç”¨.t()å¯¹æƒé‡è¿›è¡Œè½¬ç½®\n",
    "        return x @ self.weight.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ca2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•è‡ªå®šä¹‰çº¿æ€§å±‚\n",
    "model = Linear(out_features=3, in_features=6)\n",
    "test_weights = torch.randn(3, 6)\n",
    "state_dict = {'weight': test_weights}\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "x = torch.randn(1,6)\n",
    "output = model(x)\n",
    "print(f\"æƒé‡ä¸ºï¼š{model.weight}\")\n",
    "print(f\"è¾“å…¥ä¸ºï¼š{x}\")\n",
    "print(f\"è¾“å‡ºä¸ºï¼š{output}\")\n",
    "\n",
    "manual = x @ test_weights.t()\n",
    "print(f\"éªŒè¯ç»“æœæ˜¯å¦ä¸€è‡´ï¼š{torch.allclose(output, manual)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147cdec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.è‡ªå®šä¹‰åµŒå…¥æ¨¡å—ï¼ˆEmbeddingï¼‰\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # æƒé‡çŸ©é˜µ(num_embeddings, embedding_dim)->(è¯æ±‡è¡¨å¤§å°ï¼ŒåµŒå…¥å‘é‡ç»´åº¦)\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.weights = nn.Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs))\n",
    "\n",
    "        # ä½¿ç”¨æ­£æ€å‡½æ•°æˆªæ–­åˆå§‹åŒ–æƒé‡\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        sigma = 1.0\n",
    "        init.trunc_normal_(\n",
    "            self.weights,\n",
    "            mean = 0.0,\n",
    "            std = sigma,\n",
    "            a = -3.0*sigma,\n",
    "            b = 3.0*sigma\n",
    "        )   \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # è¾“å…¥token_idsä¸ºï¼ˆå¤„ç†æ‰¹æ¬¡å¤§å°ï¼Œåºåˆ—é•¿åº¦ï¼‰\n",
    "        # è¾“å‡ºä¸ºï¼ˆå¤„ç†æ‰¹æ¬¡å¤§å°ï¼Œåºåˆ—é•¿åº¦ï¼ŒåµŒå…¥å‘é‡ç»´åº¦ï¼‰\n",
    "        return self.weights[token_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ace121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•Embeddingæ¨¡å—\n",
    "import random\n",
    "# run_embeddingå‡½æ•°ç›¸å½“äºæ˜ å°„token_idsåˆ°weightsçŸ©é˜µçš„è¡Œå‘é‡\n",
    "def run_embedding(num_embeddings, embedding_dim, weights,token_ids):\n",
    "    # æµ‹è¯•Embeddingæ¨¡å—\n",
    "    model = Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "\n",
    "    state_dict = {'weights': weights}\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(token_ids)\n",
    "    return output\n",
    "\n",
    "weights = torch.randn(10, 3)\n",
    "# æµ‹è¯•æ¡†æ¶æä¾›çš„è¾“å…¥(batch, seq_len),è¿™é‡Œæ¨¡æ‹Ÿè¾“å…¥ä¸¤å¥è¯æ¯å¥è¯æœ‰3ä¸ªtoken\n",
    "ids = torch.tensor([[2, 9, 5], [3, 2, 6]])\n",
    "f = run_embedding(10, 3, weights, ids)\n",
    "print(f\"è¾“å‡ºembeddingå‘é‡: {f}\")\n",
    "\n",
    "print(f\"æƒé‡çŸ©é˜µ: {weights}\")\n",
    "\n",
    "check_1 = torch.equal(f[0][0], weights[2])\n",
    "check_2 = torch.equal(f[1][2], weights[6])\n",
    "\n",
    "print(f\"Index 2åŒ¹é…: {check_1}\")\n",
    "print(f\"Index 6åŒ¹é…: {check_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18518f",
   "metadata": {},
   "source": [
    "Q: åå‘ä¼ æ’­ä¸­æ›´æ–°æ”¹å˜äº†ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "A: åœ¨åå‘ä¼ æ’­ä¸­ï¼Œæ›´æ–°æƒé‡æ”¹å˜çš„æ˜¯å•è¯çš„â€˜å†…æ¶µâ€™ï¼Œè€Œä¸æ˜¯â€˜åå­—â€™:\n",
    "    1. ä¸å˜å…³ç³»ï¼šè¯è¡¨é‡Œçš„token IDï¼ˆæ¯”å¦‚weights[2]å¯¹åº”çŒ«ï¼‰æ˜¯ç”±é¢„åˆ†è¯é˜¶æ®µå†³å®šçš„ï¼Œåœ¨Transformerè®­ç»ƒæ—¶ä¸ä¼šæ”¹åŠ¨è¿™ä¸ªç´¢å¼•å…³ç³»ã€‚\n",
    "    2. æ•°å€¼å˜æ¢ï¼šæƒé‡çŸ©é˜µçš„æ¯ä¸€è¡Œå°±åƒæ˜¯ä¸€å¼ tokenåæ ‡è®°å½•è¡¨ï¼Œtransformeråå‘ä¼ æ’­ä¿®æ”¹çš„æ˜¯è¡¨é‡Œé‚£äº›æµ®ç‚¹æ•°ï¼Œæ”¹å˜è¿™äº›æ•°å€¼æœ¬è´¨ä¸Šæ˜¯åœ¨æ”¹å˜è¿™ä¸ªtokenåœ¨è¯­ä¹‰ç©ºé—´é‡Œçš„ç²¾ç¡®å®šä½ï¼Œé€šè¿‡ä¸æ–­å¾®è°ƒè®©å«ä¹‰ç›¸è¿‘çš„è¯åœ¨åœ°å›¾ä¸Šé å¾—æ›´è¿‘ï¼Œä»è€Œè®©æ¨¡å‹â€œç†è§£â€è¯­è¨€ï¼ˆæ¯”å¦‚â€˜çŒ«â€™å’Œâ€˜ç‹—â€™åœ¨è¯­ä¹‰ç©ºé—´é‡Œçš„è·ç¦»æ›´è¿‘å³å‘é‡ç©ºé—´å¤¹è§’æ›´å°ï¼Œè¯´æ˜å®ƒä»¬çš„å«ä¹‰æ›´æ¥è¿‘ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. å®ç°RMSNormå½’ä¸€åŒ–å±‚\n",
    "# RMSNormçš„æ ¸å¿ƒæ€æƒ³æ˜¯å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œç¼©æ”¾è€Œä¸æ˜¯å¹³ç§»\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps=1e-5, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        # å½’ä¸€å±‚å¯å­¦ä¹ çš„â€œå¢ç›Šâ€å‚æ•°ï¼Œåˆå§‹åŒ–ä¸º1\n",
    "        self.weight = nn.Parameter(torch.ones(d_model, device=device, dtype=dtype))\n",
    "\n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        # è¾“å…¥xçš„å½¢çŠ¶ä¸ºï¼ˆå¤„ç†æ‰¹æ¬¡å¤§å°ï¼Œåºåˆ—é•¿åº¦ï¼ŒåµŒå…¥å‘é‡ç»´åº¦ï¼‰\n",
    "        origin_dtype = x.dtype\n",
    "\n",
    "        # xè½¬åŒ–ä¸ºfloat32ä¿è¯å½’ä¸€åŒ–çš„è¿‡ç¨‹æ²¡æœ‰æ•°å€¼ä¸‹æº¢\n",
    "        x_fp32 = x.to(torch.float32)\n",
    "\n",
    "        # mean(-1, keepdim=True)è¡¨ç¤ºé™¤ä»¥æœ€åä¸€ç»´æ•°å€¼å³åµŒå…¥å‘é‡ç»´åº¦d_model\n",
    "        # torch.rsqrt(x)è¡¨ç¤º1/torch.sqrt(x)\n",
    "        RMS = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True)+self.eps)\n",
    "        x_normed = x_fp32 * RMS * self.weight.to(torch.float32)\n",
    "        return x_normed.to(origin_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•å½’ä¸€åŒ–å±‚\n",
    "import random\n",
    "def run_rmsnorm(d_model: int, x: torch.Tensor, eps: float):\n",
    "    # å®ä¾‹åŒ–ä½ åˆšåˆšå†™å¥½çš„ç±»\n",
    "    norm_layer = RMSNorm(d_model=d_model, eps=eps, device=x.device, dtype=x.dtype)\n",
    "    return norm_layer(x)\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "f = run_rmsnorm(3, x, 1e-5)\n",
    "print(f\"è¾“å…¥ä¸ºï¼š{x}\")\n",
    "print(f\"å½’ä¸€åŒ–è¾“å‡ºä¸ºï¼š{f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0afae",
   "metadata": {},
   "source": [
    "Q: RMSNormçš„ä½œç”¨å’Œä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "A: \n",
    "* åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå½’ä¸€åŒ–çš„æ ¸å¿ƒå·®å¼‚åœ¨äºâ€œç»Ÿè®¡çš„ç»´åº¦â€ã€‚LLMé€‰æ‹©å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰è€Œä¸æ˜¯æ‰¹å½’ä¸€åŒ–ï¼ˆBatchNormï¼‰ï¼š\n",
    "\n",
    "    1. BatchNorm(æ‰¹å½’ä¸€åŒ–)ï¼šç»Ÿè®¡è·¨æ ·æœ¬çš„åŒä¸€ç‰¹å¾ã€‚LLMä¸­ï¼Œç”±äºåºåˆ—é•¿åº¦æ³¢åŠ¨å¤§ã€æ˜¾å­˜é™åˆ¶å¯¼è‡´Batch Sizeè¾ƒå°ï¼Œä¸”ä¸åŒæ ·æœ¬é—´çš„ç»Ÿè®¡ç‰¹æ€§å·®å¼‚å·¨å¤§ï¼ŒBatchNormä¼šç ´åç‰¹å¾çš„ä¸€è‡´æ€§ã€‚\n",
    "    2. LayerNormã€RMSNorm(å±‚å½’ä¸€åŒ–)ï¼šç»Ÿè®¡åŒä¸€æ ·æœ¬å†…çš„ä¸åŒç‰¹å¾ã€‚\n",
    "    3. è¯­ä¹‰å®Œæ•´æ€§ï¼šLLMçš„è¯­ä¹‰ä¿¡æ¯ç¼–ç åœ¨æ¯ä¸ªTokençš„å‘é‡ç©ºé—´åˆ†å¸ƒä¸­ã€‚å±‚å½’ä¸€åŒ–åœ¨å•ä¸ªæ ·æœ¬å†…éƒ¨è¿›è¡Œç»Ÿè®¡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å’Œä¿ç•™ç‰¹å®šä¸Šä¸‹æ–‡ï¼ˆContextï¼‰ä¸‹çš„è¯­ä¹‰ç‰¹å¾ï¼Œè€Œä¸å—å…¶ä»–æ ·æœ¬å¹²æ‰°ã€‚\n",
    "\n",
    "* RMSNorm vs. LayerNormï¼šä»â€œå…¨å¤„ç†â€åˆ°â€œç²¾ç®€åŒ–â€\n",
    "LayerNormåŒ…å«ä¸¤ä¸ªæ“ä½œï¼šå¹³ç§»ï¼ˆRe-centeringï¼‰å’Œç¼©æ”¾ï¼ˆRe-scalingï¼‰\n",
    "RMSNormåšäº†å‡æ³•ï¼šå®ƒå‡è®¾å‡å€¼åç§»å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®å¾®å°ï¼Œå› æ­¤å–æ¶ˆäº†å‡å€¼å‡æ³•ï¼Œä»…ä¿ç•™ç¼©æ”¾æ“ä½œã€‚\n",
    "\n",
    "* RMSNormçš„æ ¸å¿ƒä¼˜åŠ¿\n",
    "A. è®¡ç®—æ•ˆç‡çš„é£è·ƒ\n",
    "    1. é€»è¾‘ï¼šå‡å°‘äº†æ±‚å‡å€¼ã€å‡å‡å€¼çš„ä¸¤æ¬¡å…¨å±€éå†å’Œå‡æ³•è¿ç®—ã€‚\n",
    "    2. æ•ˆæœï¼š åœ¨å¤„ç†è¶…å¤§è§„æ¨¡å‚æ•°ï¼ˆå¦‚70B+ï¼‰æ—¶ï¼Œç´¯ç§¯å‡å°‘çš„è®¡ç®—å¼€é”€èƒ½æ˜¾è‘—æå‡æ¨¡å‹è®­ç»ƒä¸æ¨ç†çš„ååé‡ï¼ˆThroughputï¼‰ã€‚\n",
    "\n",
    "B. æ›´åŠ æ¸©å’Œçš„æ®‹å·®åˆ†æ”¯ä¿æŠ¤\n",
    "    1. é€»è¾‘ï¼šç°ä»£LLMæ™®éé‡‡ç”¨Pre-Normæ¶æ„ï¼ˆå½’ä¸€åŒ–åœ¨æ®‹å·®è¿æ¥ä¹‹å‰ï¼‰ã€‚\n",
    "    2. æ·±åº¦ç†è§£ï¼šLayerNormå¼ºåˆ¶å°†æ¯ä¸€å±‚æ•°æ®çš„åˆ†å¸ƒä¸­å¿ƒå¹³ç§»åˆ°0ï¼Œè€ŒRMSNormä¸æ”¹å˜æ•°æ®çš„ä¸­å¿ƒï¼Œå®ƒä»…å¯¹å‘é‡çš„å¹…å€¼è¿›è¡Œè§„èŒƒåŒ–ã€‚\n",
    "    3. ä¼˜åŠ¿ï¼šè¿™ç§â€œä¸å¼ºåˆ¶æ”¹å˜åˆ†å¸ƒä¸­å¿ƒâ€çš„æ“ä½œæ›´æ¸©å’Œåœ°ä¿ç•™äº†åŸå§‹æ®‹å·®åˆ†æ”¯çš„ä¿¡æ¯ã€‚åœ¨è¶…æ·±ç½‘ç»œä¸­ï¼Œè¿™æœ‰åŠ©äºç»´æŒæ¢¯åº¦çš„å¥åº·æµåŠ¨ï¼Œé˜²æ­¢æ¨¡å‹åœ¨æ·±å±‚å‡ºç°ç‰¹å¾é€€åŒ–ã€‚\n",
    "\n",
    "C. ä¿æŒç‰¹å¾è¡¨ç¤ºå¼ºåº¦\n",
    "    1. é€»è¾‘ï¼šRMSNormé€šè¿‡æ§åˆ¶å‘é‡çš„â€œæ¨¡é•¿â€ï¼ˆMagnitudeï¼‰æ¥é˜²æ­¢æ¿€æ´»å€¼çˆ†ç‚¸ã€‚\n",
    "    2. ä¼˜åŠ¿ï¼šå®ƒåœ¨ä¿è¯æ•°å€¼ç¨³å®šçš„åŒæ—¶ï¼Œä¿ç•™äº†å„ç‰¹å¾ç»´åº¦ä¹‹é—´çš„ç›¸å¯¹æ¯”ä¾‹å…³ç³»ï¼Œä»è€Œç¡®ä¿äº†æ¨¡å‹å¯¹Tokenç‰¹å¾è¡¨ç¤ºçš„å¼ºåº¦ã€‚\n",
    "\n",
    "ğŸ’¡å®šä¹‰ï¼šRMSNormæ˜¯ä¸€ç§è®¡ç®—æˆæœ¬æ›´ä½ã€æ›´å¥‘åˆå¤§è§„æ¨¡ç”Ÿæˆå¼ä»»åŠ¡çš„å½’ä¸€åŒ–æ–¹æ¡ˆã€‚å®ƒé€šè¿‡èˆå¼ƒLayerNormçš„å¹³ç§»ä¸å˜æ€§ï¼Œæ¢å–äº†æ›´é«˜çš„è®¡ç®—æ€§èƒ½å’Œå¯¹æ·±å±‚æ®‹å·®ä¿¡æ¯çš„å°Šé‡ã€‚è¿™æ˜¯å¤§æ¨¡å‹åœ¨æ€§èƒ½ï¼ˆSpeedï¼‰ä¸ç¨³å®šæ€§ï¼ˆStabilityï¼‰ä¹‹é—´å–å¾—çš„å“è¶Šå¹³è¡¡ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0425965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. å®ç°SwiGLUæ¿€æ´»å‡½æ•° <- SiLU+é—¨æ§æœºåˆ¶GLU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # è®¡ç®—éšè—å±‚ç»´åº¦d_ffï¼Œè®¾ä¸º8/3 * d_modelï¼Œå¹¶å‘ä¸Šå–æ•´åˆ°64çš„å€æ•°\n",
    "        d_ff = int(8 / 3 * d_model)\n",
    "        d_ff = (d_ff + 63) // 64 * 64\n",
    "        \n",
    "        # å®šä¹‰çº¿æ€§å±‚ï¼ŒGateæŠ•å½±ï¼ˆWï¼‰å’ŒUpï¼ˆVï¼‰æŠ•å½±å¾€å¾€åˆå¹¶æˆ–å¹¶è¡Œå®šä¹‰\n",
    "        # è¾“å…¥(in_features)å³è¯åµŒå…¥å‘é‡ç»´åº¦ï¼Œè¾“å‡º(out_features)å³éšè—å±‚ç»´åº¦d_ff\n",
    "        self.w_gate = Linear(d_model, d_ff)\n",
    "        self.w_up = Linear(d_model, d_ff)\n",
    "        # DownæŠ•å½±å°†ç»´åº¦è½¬å›d_model\n",
    "        self.w_down = Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # å»ºè®®ä½¿ç”¨torch.sigmoidæ¥å®ç°SiLUï¼Œå³SiLU(x)=x*sigmoid(x)\n",
    "        gate = self.w_gate(x)\n",
    "        swish_gate = gate * torch.sigmoid(gate)\n",
    "        intermediate = swish_gate * self.w_up(x)\n",
    "        \n",
    "        # æœ€åæŠ•å½±å›åŸå§‹ç»´åº¦\n",
    "        return self.w_down(intermediate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•SwiGLU\n",
    "def run_swiglu(d_model: int, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    æµ‹è¯•é€‚é…å™¨ï¼šå®ä¾‹åŒ–SwiGLUå¹¶è¿è¡Œå‰å‘ä¼ æ’­\n",
    "    \"\"\"\n",
    "    model = SwiGLU(d_model).to(x.device).to(x.dtype)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "    return output   \n",
    "d_model = 3\n",
    "x = torch.randn(1, 5, d_model)\n",
    "output = run_swiglu(d_model, x)\n",
    "print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "print(f\"è¾“å…¥ï¼š{x}\")\n",
    "print(f\"è¾“å‡ºï¼š{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. å®ç°RoPEä½ç½®ç¼–ç  <- æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆç›¸å¯¹ä½ç½®ç¼–ç å¯¹é•¿ä¸Šä¸‹æ–‡å‹å¥½ï¼‰\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, device=None):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.theta = theta\n",
    "    \n",
    "        # åˆå§‹åŒ–ä¸ºç©ºï¼Œæ–¹ä¾¿forwardåŠ¨æ€ç”Ÿæˆï¼ˆé¢„è®¡ç®—å¹¶ç¼“å­˜coså’Œsinå€¼ï¼‰\n",
    "        self.register_buffer(\"cos\", None, persistent=False)\n",
    "        self.register_buffer(\"sin\", None, persistent=False)\n",
    "\n",
    "    def _build_cache(self, seq_len, device, dtype):\n",
    "        # åªæœ‰åœ¨seq_lenè¶…è¿‡å½“å‰ç¼“å­˜æ—¶æ‰é‡æ–°è®¡ç®—\n",
    "        if self.cos is not None and seq_len <= self.cos.shape[0]:\n",
    "            return \n",
    "        powers = torch.arange(0, self.d_k, 2, device=device).float()\n",
    "        inv_freq = self.theta ** (-powers / self.d_k)\n",
    "\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.outer(t, inv_freq)  # freqs.shape:(seq_len, d_k/2)\n",
    "\n",
    "        # ç¼“å­˜coså’Œsinå€¼\n",
    "        self.register_buffer(\"cos\", torch.cos(freqs).to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin\", torch.sin(freqs).to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.Tensor:\n",
    "        # x.shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        seq_len = x.shape[-2]\n",
    "\n",
    "        # åŠ¨æ€åˆ¤å®šå¹¶ç”Ÿæˆç¼“å­˜\n",
    "        self._build_cache(seq_len, x.device, x.dtype)\n",
    "\n",
    "        # åˆ‡ç‰‡è·å–å½“å‰é•¿åº¦æ‰€éœ€çš„ç¼“å­˜\n",
    "        # d_model/num_headsä¹Ÿå¿…é¡»ä¸ºå¶æ•°\n",
    "        assert self.d_k % 2 == 0, \"RoPEéœ€è¦d_kæ˜¯ä¸€ä¸ªå¶æ•°\"\n",
    "        cos = self.cos[:seq_len, :] # (seq_len, d_k/2)\n",
    "        sin = self.sin[:seq_len, :] # (seq_len, d_k/2)\n",
    "\n",
    "        # å®ç°æ—‹è½¬é€»è¾‘å¹¶æŠŠd_kç»´æ‹†åˆ†ä¸ºä¸¤åŠå¤„ç†\n",
    "        # cosã€sinå½¢çŠ¶ä»(S, D/2)->(1, 1, S, D/2)ä½¿ç”¨unsqueezeé€‚é…å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ç»´åº¦å¹¿æ’­\n",
    "        cos = cos.view(1, seq_len, -1)\n",
    "        sin = sin.view(1, seq_len, -1)\n",
    "        \n",
    "        x1 = x[..., 0::2]\n",
    "        x2 = x[..., 1::2]\n",
    "        \n",
    "        # æ—‹è½¬çŸ©é˜µå…¬å¼ï¼š[x1*cos - x2*sin, x1*sin + x2*cos] \n",
    "        # x_rot.shape:(B, H, S, D/2, 2)\n",
    "        x_rot = torch.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "\n",
    "        return x_rot.flatten(-2) # æœ€åå±•å¹³å›åˆ°(B, H, S, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•RoPE\n",
    "import random\n",
    "def run_rope(theta: float, d_k: int, x: torch.Tensor) -> torch.Tensor:\n",
    "    # å®ä¾‹åŒ–å¹¶è¿è¡ŒRoPE\n",
    "    model = RotaryPositionalEmbedding(theta, d_k, device=x.device)\n",
    "    return model(x)\n",
    "\n",
    "x = torch.randn(1, 2, 5, 8)\n",
    "print(f\"è¾“å…¥ï¼š{x}\")\n",
    "print(f\"è¾“å‡ºï¼š{run_rope(10000.0, 8, x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd10f8",
   "metadata": {},
   "source": [
    "Q: æ—‹è½¬ä½ç½®ç¼–ç çš„ä¼˜åŠ¿åœ¨äºä»€ä¹ˆï¼Œä»¥åŠthetaå¦‚ä½•é€‰æ‹©ï¼Ÿ\n",
    "\n",
    "A: æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºï¼šå®ƒé€šè¿‡å¯¹Attentionä¸­çš„Queryå’ŒKeyå‘é‡æ–½åŠ ä¸ä½ç½®ç›¸å…³çš„æ—‹è½¬ï¼Œå°†ç›¸å¯¹ä½ç½®ä¿¡æ¯ç›´æ¥èå…¥æ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹ã€‚è¯¥æ—‹è½¬è§„åˆ™åœ¨æ•´ä¸ªè®­ç»ƒä¸æ¨ç†è¿‡ç¨‹ä¸­æ˜¯å›ºå®šçš„ï¼Œå› æ­¤å¯¹åº”çš„cosã€sinå€¼å¯ä»¥é¢„å…ˆè®¡ç®—å¹¶ä»¥bufferçš„å½¢å¼ç¼“å­˜å’Œå¤ç”¨ã€‚ä»æœ¬è´¨ä¸Šçœ‹ï¼ŒRoPEä¸ºAttentionå¼•å…¥äº†ä¸€ç§å‡ ä½•çº¦æŸæ¡†æ¶ï¼Œä½¿å¾—æ³¨æ„åŠ›åˆ†æ•°æ˜¾å¼ä¾èµ–äºtokenä¹‹é—´çš„ç›¸å¯¹ä½ç½®å·®ï¼Œè€Œéç»å¯¹ä½ç½®ã€‚ç”±äºäºŒç»´æ—‹è½¬çŸ©é˜µçš„ç‚¹ç§¯æ€§è´¨ï¼Œç»è¿‡æ—‹è½¬åçš„ç­‰ä»·äºåœ¨ç‚¹ç§¯ä¸­å¼•å…¥ä¸ğ‘—âˆ’ğ‘–ç›¸å…³çš„ç›¸ä½åç§»ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶åœ°å»ºæ¨¡ç›¸å¯¹è·ç¦»å…³ç³»ï¼Œè€Œæ— éœ€é¢å¤–çš„ç›¸å¯¹ä½ç½®åç½®æˆ–å¯å­¦ä¹ å‚æ•°ã€‚è¿™ä¸€ç‰¹æ€§ä½¿å¾—RoPEåœ¨é•¿ä¸Šä¸‹æ–‡å¤–æ¨ä»¥åŠæ¨ç†é˜¶æ®µï¼ˆå¦‚ä½¿ç”¨KV cacheï¼‰å…·æœ‰è‰¯å¥½çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n",
    "\n",
    "å…¶ä¸­ï¼Œthetaæ˜¯æ§åˆ¶æ—‹è½¬é¢‘ç‡åˆ†å¸ƒçš„è¶…å‚æ•°ï¼šthetaè¶Šå¤§ï¼Œæ•´ä½“æ—‹è½¬é¢‘ç‡è¶Šä½ï¼Œæ›´æœ‰åˆ©äºå»ºæ¨¡é•¿è·ç¦»ï¼ˆå…¨å±€ï¼‰ä¾èµ–ï¼›thetaè¶Šå°ï¼Œé«˜é¢‘æˆåˆ†å æ¯”è¶Šå¤§ï¼Œæ›´å¼ºè°ƒå±€éƒ¨ç›¸å¯¹ä½ç½®å…³ç³»ã€‚åœ¨å®è·µä¸­ï¼Œtheta = 10000.0 æ˜¯è¢«å¹¿æ³›éªŒè¯çš„é»˜è®¤é€‰æ‹©ï¼Œå®ƒåœ¨åŒä¸€è¡¨ç¤ºç©ºé—´ä¸­åŒæ—¶è¦†ç›–äº†å±€éƒ¨ä¸å…¨å±€ç›¸å¯¹ä½ç½®å»ºæ¨¡éœ€æ±‚ï¼Œå› æ­¤è¢«å¤§å¤šæ•°ä¸»æµå¤§è¯­è¨€æ¨¡å‹é‡‡ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b64e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. å®ç°softmaxå‡½æ•°\n",
    "def Softmax(x: torch.Tensor, dim:int) -> torch.Tensor:\n",
    "    # å‡å»æœ€å¤§å€¼æé«˜ç¨³å®šæ€§,å–æŒ‡å®šç»´åº¦dimçš„æœ€å¤§å€¼,ä¿æŒå–å‡ºç»´åº¦ä¸€è‡´\n",
    "    max_val = torch.max(x, dim=dim, keepdim=True)[0]\n",
    "    e_x = torch.exp(x - max_val)\n",
    "    return e_x / e_x.sum(dim=dim, keepdim=True)   # æ‰€æœ‰æ•°å€¼éƒ½åœ¨[0,1)ä¹‹é—´,ä¸”å’Œä¸º1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6280ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•softmaxå‡½æ•°\n",
    "def run_softmax(x: torch.Tensor, dim: int):\n",
    "    return softmax(x, dim)\n",
    "test_input = torch.randn(2, 3)\n",
    "output = Softmax(test_input, dim=-1)\n",
    "print(f\"è¾“å…¥: {test_input}\")\n",
    "print(f\"è¾“å‡º: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. å®ç°ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶\n",
    "import math\n",
    "\n",
    "def scaled_dot_product_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask= None):\n",
    "    \"\"\"\n",
    "    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶\n",
    "    Args:\n",
    "        q: æŸ¥è¯¢,å½¢çŠ¶ä¸º(batch_size, num_heads, seq_len, d_k)\n",
    "        k: é”®,å½¢çŠ¶ä¸º(batch_size, num_heads, seq_len, d_k)\n",
    "        v: å€¼,å½¢çŠ¶ä¸º(batch_size, num_heads, seq_len, d_v)\n",
    "        mask: æ©ç å½¢çŠ¶ä¸º(seq_len, seq_len)å¸ƒå°”å¼ é‡, Trueè¡¨ç¤ºä¿ç•™è¯¥ä½ç½®çš„æ³¨æ„åŠ›æƒé‡ï¼ŒFalseè¡¨ç¤ºå±è”½è¯¥ä½ç½®çš„æ³¨æ„åŠ›æƒé‡\n",
    "    \"\"\"\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # åº”ç”¨æ©ç mask\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == False, float('-inf'))\n",
    "    attn_weights = Softmax(scores, dim=-1)\n",
    "\n",
    "    # å¾—åˆ†æƒé‡ä¹˜ä»¥vå¾—åˆ°æœ€ååŠ æƒç»“æœè¾“å‡º\n",
    "    output = torch.matmul(attn_weights, v)\n",
    "\n",
    "    return output, attn_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a40b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ç‚¹ç§¯å‡½æ•°çš„å®ç°\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def test_implementation():\n",
    "    # è®¾ç½®éšæœºç§å­ä¿è¯ç»“æœå¯å¤ç°\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # ç»´åº¦å®šä¹‰\n",
    "    batch_size = 2\n",
    "    heads = 1\n",
    "    seq_len = 3\n",
    "    d_k = 5\n",
    "    d_v = 5\n",
    "\n",
    "    # 3é˜¶å¼ é‡è¾“å…¥(batch_size, seq_len, d_k)\n",
    "    q3 = torch.randn(batch_size, seq_len, d_k)\n",
    "    k3 = torch.randn(batch_size, seq_len, d_k)\n",
    "    v3 = torch.randn(batch_size, seq_len, d_v)\n",
    "    \n",
    "    out3, attn_weights = scaled_dot_product_attention(q3, k3, v3)\n",
    "    print(f\"3é˜¶å¼ é‡è¾“å‡º:\\n{out3}\")\n",
    "    assert out3.shape == (batch_size, seq_len, d_v), f\"3Då½¢çŠ¶é”™è¯¯: {out3.shape}\"\n",
    "    print(\"3é˜¶å¼ é‡å½¢çŠ¶æµ‹è¯•é€šè¿‡\")\n",
    "\n",
    "    # 4é˜¶å¼ é‡è¾“å…¥(batch_size, heads, seq_len, d_k)\n",
    "    q4 = torch.randn(batch_size, heads, seq_len, d_k)\n",
    "    k4 = torch.randn(batch_size, heads, seq_len, d_k)\n",
    "    v4 = torch.randn(batch_size, heads, seq_len, d_v)\n",
    "    \n",
    "    out4, attn_weights = scaled_dot_product_attention(q4, k4, v4)\n",
    "    print(f\"4é˜¶å¼ é‡è¾“å‡º:\\n{out4}\")\n",
    "    assert out4.shape == (batch_size, heads, seq_len, d_v), f\"4Då½¢çŠ¶é”™è¯¯: {out4.shape}\"\n",
    "    print(\"4é˜¶å¼ é‡å½¢çŠ¶æµ‹è¯•é€šè¿‡\")\n",
    "\n",
    "    # æ©ç æµ‹è¯•\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
    "    print(f\"æ©ç çŸ©é˜µmask:\\n{mask}\")\n",
    "\n",
    "    # å¦‚æœæ©ç ç”Ÿæ•ˆï¼Œç¬¬ä¸€è¡Œè¾“å‡ºåº”è¯¥åªä¸ç¬¬ä¸€ä¸ªValueæœ‰å…³\n",
    "    out_masked, attn_weights = scaled_dot_product_attention(q3, k3, v3, mask=mask)\n",
    "    print(f\"æ³¨æ„åŠ›æƒé‡attn_weights:\\n{attn_weights}\")\n",
    "    \n",
    "    # éªŒè¯æ©ç é€»è¾‘ï¼šåœ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«NaNæˆ–Infæ¥åˆæ­¥åˆ¤æ–­\n",
    "    print(f\"æ©ç åçš„è¾“å‡ºout_masked:\\n{out_masked}\")\n",
    "    assert not torch.isnan(out_masked).any(), \"è¾“å‡ºåŒ…å«NaN\"\n",
    "    print(\"æ©ç åŠŸèƒ½æµ‹è¯•é€šè¿‡\")\n",
    "\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        test_implementation()\n",
    "    except NameError:\n",
    "        print(\"é”™è¯¯ï¼šè¯·å…ˆå®šä¹‰scaled_dot_product_attentionå‡½æ•°å†è¿è¡Œæµ‹è¯•ã€‚\")\n",
    "    except Exception as e:\n",
    "        print(f\"æµ‹è¯•å¤±è´¥{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e28049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. å› æœå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_modelå¿…é¡»èƒ½è¢«num_headsæ•´é™¤\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.w_q = Linear(d_model, d_model)\n",
    "        self.w_k = Linear(d_model, d_model)\n",
    "        self.w_v = Linear(d_model, d_model)\n",
    "        self.w_o = Linear(d_model, d_model)\n",
    "\n",
    "        # å¯¹Qã€kä½¿ç”¨RoPE\n",
    "        self.rope = RotaryPositionalEmbedding(10000.0, d_k=self.d_k)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x.shape->(batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # çº¿æ€§å˜æ¢å¹¶æ‹†åˆ†ä¸ºå¤šå¤´ (B, L, D) -> (B, S, H, D_K) -> (B, H, S, D_K)\n",
    "        q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "        \n",
    "        # è®¡ç®—ç¼©æ”¾ç‚¹ç§¯å¾—åˆ†\n",
    "        scores, _ = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "        \n",
    "        # åŠ æƒæ±‚å’Œå¹¶æ‹¼æ¥å¤´ (B, H, S, D_K)->(B, S, H, D_K)->(B, S, D),å…¶ä¸­D=D_K*H=d_model\n",
    "        out = scores.transpose(1, 2).contiguous()\n",
    "        out = out.view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        return self.w_o(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5695bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶\n",
    "import torch\n",
    "\n",
    "def run_multihead_self_attention(d_model, num_heads, x):\n",
    "    \"\"\"\n",
    "    æµ‹è¯•é€‚é…å™¨,x:è¾“å…¥å¼ é‡(batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    model = MultiHeadAttention(d_model, num_heads)\n",
    "    model.eval()\n",
    "    return model(x)\n",
    "\n",
    "x = torch.randn(1, 2048, 8)\n",
    "out = run_multihead_self_attention(8, 2, x)\n",
    "print(f\"è¾“å‡ºå½¢çŠ¶: {out.shape}\")\n",
    "print(f\"è¾“å‡º: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12731f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. å®ç°Transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        # å‚æ•°ï¼š\n",
    "        # d_modelï¼šè¾“å…¥ç»´åº¦ï¼ˆè¯å‘é‡åµŒå…¥ç»´åº¦ï¼‰\n",
    "        # num_headsï¼šå¤´æ•°\n",
    "        # d_ffï¼šå‰é¦ˆç¥ç»ç½‘ç»œç»´åº¦\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            Linear(d_model, d_ff),\n",
    "            SwiGLU(d_ff),\n",
    "            Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        original_x1 = x\n",
    "\n",
    "        # Pre-Norm + Multihead Attention + Residual Connection\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = x + original_x1\n",
    "        \n",
    "        # Pre-Norm + FFN + Residual Connection\n",
    "        original_x2 = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + original_x2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•TransformerBlock\n",
    "def run_transformerBlock(d_model, num_heads, d_ff):\n",
    "    model = TransformerBlock(d_model=d_model, num_heads=num_heads, d_ff=d_ff)\n",
    "    model.eval()\n",
    "    return model(x)\n",
    "\n",
    "# æµ‹è¯•å‚æ•°\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_ff = 2\n",
    "\n",
    "# ç”Ÿæˆéšæœºè¾“å…¥\n",
    "x = torch.randn(1, 5, d_model)\n",
    "\n",
    "# è¿è¡Œæ¨¡å‹\n",
    "output = run_transformerBlock(d_model, num_heads, d_ff)\n",
    "\n",
    "print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "print(f\"è¾“å…¥ç¤ºä¾‹: {x}\")\n",
    "\n",
    "print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "print(f\"è¾“å‡º: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f04901",
   "metadata": {},
   "source": [
    "äºŒã€ æ”¯æŒTransformerè®­ç»ƒçš„æ¿å—æ„å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å®ç°äº¤å‰ç†µæŸå¤±å‡½æ•°\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy(logits, targets):\n",
    "    # logits:æ¨¡å‹è¾“å‡ºçš„ logitsï¼Œå½¢çŠ¶ä¸º(batch_size, Vocab)\n",
    "    # targets: çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(batch_size,)\n",
    "\n",
    "    # ä¸ºäº†ä¿æŒæ•°å€¼ç¨³å®šæ€§ï¼Œæˆ‘ä»¬ä»æ¯ä¸ªæ ·æœ¬çš„logitsä¸­å‡å»æœ€å¤§å€¼\n",
    "    max_val = np.max(logits, axis=-1, keepdims=True)\n",
    "    shifted_logits = logits - max_val\n",
    "\n",
    "    log_sum_exp = np.log(\n",
    "        np.sum(np.exp(shifted_logits), axis=-1, keepdims=False)\n",
    "    )\n",
    "\n",
    "    target_logits = np.take_along_axis(shifted_logits, targets[..., None], axis=-1).squeeze(-1)\n",
    "    loss_i = log_sum_exp - target_logits\n",
    "    loss = np.mean(loss_i)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def calculate_perplexity(logits, targets):\n",
    "    # ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è®¡ç®—å›°æƒ‘åº¦\n",
    "    loss = cross_entropy(logits, targets)\n",
    "    \n",
    "    # å›°æƒ‘åº¦ = exp(å¹³å‡æŸå¤±)\n",
    "    perplexity = np.exp(loss)\n",
    "\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cf68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. è‡ªå®šä¹‰å®ç°AdamWä¼˜åŒ–å™¨\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "            params: å¾…ä¼˜åŒ–çš„å‚æ•°è¿­ä»£å™¨(å¯å­¦ä¹ å‚æ•°)\n",
    "            lr: å­¦ä¹ ç‡\n",
    "            betas: ç”¨äºè®¡ç®—æ¢¯åº¦åŠå…¶å¹³æ–¹çš„è¿è¡Œå¹³å‡å€¼çš„ç³»æ•°(ä¸€é˜¶çŸ©, äºŒé˜¶çŸ©)\n",
    "            eps: æ·»åŠ åˆ°åˆ†æ¯ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§\n",
    "            weight_decay: æƒé‡è¡°å‡ç³»æ•° (L2æ­£åˆ™åŒ–è§£è€¦)\n",
    "        \"\"\"\n",
    "        # å‚æ•°æ ¡éªŒ\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if eps <= 0.0:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta1: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta2: {betas[1]}\")\n",
    "\n",
    "        # å°†è¶…å‚æ•°æ‰“åŒ…è¿›dictï¼Œæ–¹ä¾¿çˆ¶ç±»Optimizerç®¡ç†\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–æ›´æ–°\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        # éå†æ‰€æœ‰çš„å‚æ•°ç»„ï¼ˆä¾‹å¦‚æ¨¡å‹ä¸åŒå±‚å¯ä»¥æœ‰ä¸åŒçš„å­¦ä¹ ç‡ï¼‰\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            beta1, beta2 = group[\"betas\"]\n",
    "            eps = group[\"eps\"]\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "\n",
    "            for param in group[\"params\"]:\n",
    "                # å¦‚æœå‚æ•°æ²¡æœ‰æ¢¯åº¦ï¼Œåˆ™è·³è¿‡\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = param.grad\n",
    "                state = self.state[param]\n",
    "\n",
    "                # åˆå§‹åŒ–çŠ¶æ€å˜é‡ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶æ‰§è¡Œï¼‰\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼šä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(param)\n",
    "                    # æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼šäºŒé˜¶çŸ©ï¼ˆæ¢¯åº¦çš„å¹³æ–¹ï¼‰\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(param)\n",
    "\n",
    "                exp_avg = state[\"exp_avg\"]\n",
    "                exp_avg_sq = state[\"exp_avg_sq\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "                t = state[\"step\"]\n",
    "\n",
    "                # æ›´æ–°ä¸€é˜¶çŸ©ï¼šm_t = Î²1 * m_{t-1} + (1 - Î²1) * g_t\n",
    "                # mul_ æ˜¯åŸåœ°ä¹˜æ³•ï¼Œadd_ æ˜¯åŸåœ°åŠ æ³•\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                # æ›´æ–°äºŒé˜¶çŸ©ï¼šv_t = Î²2 * v_{t-1} + (1 - Î²2) * g_t^2\n",
    "                # addcmul_è®¡ç®—ï¼šexp_avg_sq = exp_avg_sq + (1 - beta2) * (grad * grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # è®¡ç®—åç½®ä¿®æ­£\n",
    "                # ç”±äºmå’Œvåˆå§‹åŒ–ä¸º0ï¼Œæ—©æœŸè¿­ä»£ä¼šå‘0åç§»ï¼Œéœ€è¦ä¿®æ­£\n",
    "                bias_correction1 = 1 - beta1 ** t\n",
    "                bias_correction2 = 1 - beta2 ** t\n",
    "\n",
    "                # è®¡ç®—æœ€ç»ˆçš„æ­¥é•¿ï¼šä¿®æ­£åçš„å­¦ä¹ ç‡\n",
    "                step_size = lr * (bias_correction2 ** 0.5) / bias_correction1\n",
    "\n",
    "                # æ›´æ–°å‚æ•°ï¼šÎ¸ = Î¸ âˆ’ step_size * m / (sqrt(v) + Îµ)\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "                # è§£è€¦æƒé‡è¡°å‡\n",
    "                # Î¸ = Î¸ - lr * weight_decay * Î¸\n",
    "                if weight_decay != 0:\n",
    "                    param.add_(param, alpha=-lr * weight_decay)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab5b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. å®ç°LLamAç±»ä¼¼çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "import math\n",
    "\n",
    "def get_lr_cosine_schedule(t, alpha_max, alpha_min, T_w, T_c):\n",
    "    \"\"\"\n",
    "    å‚æ•°:\n",
    "        t: å½“å‰è¿­ä»£æ­¥æ•°\n",
    "        alpha_max: æœ€å¤§å­¦ä¹ ç‡\n",
    "        alpha_min: æœ€å°ï¼ˆæœ€ç»ˆï¼‰å­¦ä¹ ç‡\n",
    "        T_w: é¢„çƒ­è¿­ä»£æ¬¡æ•°\n",
    "        T_c: ä½™å¼¦é€€ç«è¿­ä»£æ€»æ¬¡æ•°\n",
    "    \"\"\"\n",
    "    \n",
    "    if t < T_w:\n",
    "        return (t / T_w) * alpha_max\n",
    "    \n",
    "    elif T_w <= t <= T_c:\n",
    "        progress = (t - T_w) / (T_c - T_w)\n",
    "        cosine_out = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return alpha_min + cosine_out * (alpha_max - alpha_min)\n",
    "    else:\n",
    "        return alpha_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e47a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä½™å¼¦é€€ç«å­¦ä¹ è°ƒåº¦å™¨çš„ä»£ç \n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# æ¨¡æ‹Ÿæµ‹è¯•\n",
    "A_MAX = 1e-3\n",
    "A_MIN = 1e-5\n",
    "TW = 20    # ç¬¬20æ­¥å®Œæˆé¢„çƒ­\n",
    "TC = 100   # 100æ­¥å®Œæˆé€€ç«\n",
    "TOTAL_STEPS = 120\n",
    "\n",
    "steps = list(range(TOTAL_STEPS))\n",
    "lrs = [get_lr_cosine_schedule(t, A_MAX, A_MIN, TW, TC) for t in steps]\n",
    "\n",
    "# æ‰“å°å…³é”®èŠ‚ç‚¹æ•°å€¼\n",
    "print(f\"Step 0(å¼€å§‹): {lrs[0]:.6f}\")\n",
    "print(f\"Step {TW}(é¢„çƒ­ç»“æŸ): {lrs[TW]:.6f} (é¢„æœŸåº”æ¥è¿‘{A_MAX})\")\n",
    "print(f\"Step {(TW+TC)//2}(é€€ç«ä¸­ç‚¹): {lrs[(TW+TC)//2]:.6f}\")\n",
    "print(f\"Step {TC}(é€€ç«ç»“æŸ): {lrs[TC]:.6f} (é¢„æœŸåº”ä¸º{A_MIN})\")\n",
    "print(f\"Step {TOTAL_STEPS-1}(é€€ç«å): {lrs[-1]:.6f} (é¢„æœŸåº”ä¿æŒ{A_MIN})\")\n",
    "\n",
    "plt.plot(steps, lrs)\n",
    "plt.axvline(x=TW, color='r', linestyle='--', label='Warm-up End')\n",
    "plt.axvline(x=TC, color='g', linestyle='--', label='Annealing End')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. æ¢¯åº¦è£å‰ª\n",
    "import torch\n",
    "\n",
    "def run_gradient_clipping(params, max_norm, eps=1e-6):\n",
    "    \"\"\"\n",
    "    å‚æ•°:\n",
    "        params: åŒ…å«å¾…è£å‰ªæ¢¯åº¦çš„å‚æ•°åˆ—è¡¨æˆ–ç”Ÿæˆå™¨\n",
    "        max_norm: å…è®¸çš„æœ€å¤§L2èŒƒæ•°(M)\n",
    "        eps: ç”¨äºæ•°å€¼ç¨³å®šæ€§çš„å¾®å°å€¼\n",
    "    \"\"\"\n",
    "    params_with_grad = [p for p in params if p.grad is not None]\n",
    "    if len(params_with_grad) == 0:\n",
    "        return\n",
    "\n",
    "    total_norm = torch.norm(\n",
    "        torch.stack([torch.norm(p.grad.detach(), 2) for p in params_with_grad]), 2\n",
    "    )\n",
    "\n",
    "    clip_coeff = max_norm / (total_norm + eps)\n",
    "\n",
    "    if clip_coeff < 1.0:\n",
    "        for p in params_with_grad:\n",
    "            p.grad.detach().mul_(clip_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ¢¯åº¦è£å‰ªä»£ç \n",
    "import torch\n",
    "\n",
    "def test_gradient_clipping_logic():\n",
    "    print(\"å¼€å§‹æ¢¯åº¦è£å‰ªæ¨¡æ‹Ÿæµ‹è¯•...\")\n",
    "    \n",
    "    # åˆ›å»ºä¸¤ä¸ªå‚æ•°ï¼Œæ‰‹åŠ¨è®¾ç½®æ¢¯åº¦\n",
    "    p1 = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "    p2 = torch.tensor([2.0, 2.0], requires_grad=True)\n",
    "    \n",
    "    # æ‰‹åŠ¨èµ‹äºˆè¾ƒå¤§çš„æ¢¯åº¦ï¼šg1=[3.0, 0.0], g2=[0.0, 4.0]\n",
    "    p1.grad = torch.tensor([3.0, 0.0])\n",
    "    p2.grad = torch.tensor([0.0, 4.0])\n",
    "    \n",
    "    # è®¡ç®—å½“å‰å…¨å±€L2èŒƒæ•°ï¼šsqrt(3^2 + 4^2) = 5.0\n",
    "    max_norm = 1.0  # è®¾ç½®æœ€å¤§èŒƒæ•°ä¸º1.0\n",
    "    print(f\"\\nåœºæ™¯ 1(è§¦å‘è£å‰ª):\")\n",
    "    print(f\"è£å‰ªå‰ - åŸå§‹æ¢¯åº¦èŒƒæ•°: 5.0, ç›®æ ‡æœ€å¤§èŒƒæ•°M: {max_norm}\")\n",
    "    \n",
    "    run_gradient_clipping([p1, p2], max_norm)\n",
    "    \n",
    "    # è£å‰ªåçš„æ€»èŒƒæ•°åº”æ¥è¿‘1.0(ç”±äºepsçš„å­˜åœ¨ï¼Œä¼šç•¥å°äº1.0)\n",
    "    new_norm = torch.norm(torch.stack([torch.norm(p1.grad, 2), torch.norm(p2.grad, 2)]), 2)\n",
    "    print(f\"è£å‰ªå - å‚æ•°1 æ¢¯åº¦: {p1.grad}\")\n",
    "    print(f\"è£å‰ªå - å‚æ•°2 æ¢¯åº¦: {p2.grad}\")\n",
    "    print(f\"è£å‰ªå - å…¨å±€æ¢¯åº¦èŒƒæ•°: {new_norm.item():.6f} (é¢„æœŸ<= {max_norm})\")\n",
    "\n",
    "    # æ¢¯åº¦å°äºæœ€å¤§èŒƒæ•° (ä¸åº”è£å‰ª)\n",
    "    p3 = torch.tensor([0.1, 0.1], requires_grad=True)\n",
    "    p3.grad = torch.tensor([0.1, 0.2])\n",
    "    \n",
    "    # å½“å‰èŒƒæ•°çº¦ä¸º0.2236\n",
    "    max_norm_large = 10.0\n",
    "    original_grad = p3.grad.clone()\n",
    "    \n",
    "    print(f\"\\nåœºæ™¯ 2(ä¸è§¦å‘è£å‰ª):\")\n",
    "    print(f\"è£å‰ªå‰ - åŸå§‹æ¢¯åº¦èŒƒæ•°: 0.2236, ç›®æ ‡æœ€å¤§èŒƒæ•°M: {max_norm_large}\")\n",
    "    \n",
    "    run_gradient_clipping([p3], max_norm_large)\n",
    "    \n",
    "    print(f\"è£å‰ªå - æ¢¯åº¦æ˜¯å¦ä¿æŒä¸å˜: {torch.equal(p3.grad, original_grad)}\")\n",
    "    print(f\"è£å‰ªå - æ¢¯åº¦èŒƒæ•°: {torch.norm(p3.grad, 2).item():.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_gradient_clipping_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. æ•°æ®åŠ è½½ï¼ˆç»™å®šå‰é¢çš„tokené¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼‰\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "def get_batch(x, batch_size, context_length, device):\n",
    "    \"\"\"\n",
    "    x: numpyæ•°ç»„(token ID)\n",
    "    batch_size: æ‰¹å¤§å°\n",
    "    context_length: ä¸Šä¸‹æ–‡é•¿åº¦(m)\n",
    "    device: è®¾å¤‡å­—ç¬¦ä¸² ('cpu', 'cuda', 'mps')\n",
    "    \"\"\"\n",
    "\n",
    "    # ç¡®ä¿i + context_lengthä¹‹åè¿˜æœ‰ä¸€ä¸ªä½ç½®ç•™ç»™ç›®æ ‡åºåˆ—çš„æœ€åä¸€ä¸ªä»¤ç‰Œ\n",
    "    max_idx = len(x) - context_length\n",
    "    \n",
    "    # éšæœºç”Ÿæˆbatch_sizeä¸ªèµ·å§‹ä½ç½®\n",
    "    ix = torch.randint(0, max_idx, (batch_size,))\n",
    "    \n",
    "    # æ ¹æ®ç´¢å¼•æå–è¾“å…¥å’Œç›®æ ‡\n",
    "    # x_batch: å½¢çŠ¶ä¸º(batch_size, context_length)\n",
    "    # y_batch: å½¢çŠ¶ä¸º(batch_size, context_length)\n",
    "    x_batch = torch.stack([torch.from_numpy(x[i : i + context_length].astype(np.int64)) for i in ix])\n",
    "    y_batch = torch.stack([torch.from_numpy(x[i + 1 : i + context_length + 1].astype(np.int64)) for i in ix])\n",
    "    \n",
    "    # å°†å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "    \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ•°æ®åŠ è½½å‡½æ•°\n",
    "import numpy as np\n",
    "import torch\n",
    "def run_get_batch(x_list, batch_size, context_length, device_str):\n",
    "    \"\"\"\n",
    "    æµ‹è¯•æ¡†æ¶å¯èƒ½ä¼šä»¥åˆ—è¡¨å½¢å¼ä¼ å…¥æ•°æ®ï¼Œéœ€è¦ç¡®ä¿å®ƒæ˜¯numpyæ•°ç»„\n",
    "    \"\"\"\n",
    "    # å°†è¾“å…¥è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆå¦‚æœå®ƒè¿˜ä¸æ˜¯çš„è¯ï¼‰\n",
    "    x = np.array(x_list)\n",
    "\n",
    "    # è°ƒç”¨å®ç°çš„get_batchå‡½æ•°\n",
    "    x_batch, y_batch = get_batch(x, batch_size, context_length, device_str)\n",
    "    \n",
    "    return x_batch, y_batch\n",
    "\n",
    "x = np.array([1, 2, 3, 2, 5, 9, 10, 11, 12])\n",
    "print(f'è¾“å…¥x:\\n{x}')\n",
    "x_batch, y_batch = run_get_batch(x, batch_size=2, context_length=2, device_str='cpu')\n",
    "print(f'è¾“å‡ºx_batch:\\n{x_batch}')\n",
    "print(f'è¾“å‡ºy_batch:\\n{y_batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d8a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ä¿å­˜å’ŒåŠ è½½checkpoint\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(model, optimizer, iteration, out):\n",
    "    \"\"\"\n",
    "    å°†æ¨¡å‹ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œè¿­ä»£æ¬¡æ•°ä¿å­˜åˆ°ç›®æ ‡ä½ç½®ã€‚\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºåŒ…å«æ‰€æœ‰å¿…è¦çŠ¶æ€çš„å­—å…¸\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'iteration': iteration\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, out)  # ä½¿ç”¨torch.saveè¿›è¡ŒæŒä¹…åŒ–å­˜å‚¨\n",
    "\n",
    "def load_checkpoint(src, model, optimizer):\n",
    "    \"\"\"\n",
    "    ä»æºä½ç½®åŠ è½½æ£€æŸ¥ç‚¹ï¼Œæ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚\n",
    "    è¿”å›ä¿å­˜æ—¶çš„è¿­ä»£æ¬¡æ•°ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint = torch.load(src)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return checkpoint['iteration']   # è¿”å›è¿­ä»£æ¬¡æ•°ï¼Œä»¥ä¾¿æ¢å¤è®­ç»ƒè¿›åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•save_checkpointå’Œload_checkpointå‡½æ•°\n",
    "import torch\n",
    "import os\n",
    "import typing\n",
    "\n",
    "def run_save_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, iteration: int, out: typing.Union[str, os.PathLike, typing.BinaryIO]):\n",
    "\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'iteration': iteration\n",
    "    }\n",
    "    torch.save(checkpoint, out)\n",
    "\n",
    "\n",
    "def run_load_checkpoint(src: typing.Union[str, os.PathLike, typing.BinaryIO], model: torch.nn.Module, optimizer: torch.optim.Optimizer) -> int:\n",
    "\n",
    "    # map_locationç¡®ä¿äº†å³ä½¿æ˜¯åœ¨æ²¡æœ‰GPUçš„æœºå™¨ä¸Šä¹Ÿèƒ½åŠ è½½GPUä¿å­˜çš„æ¨¡å‹\n",
    "    checkpoint = torch.load(src, map_location='cpu')\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return checkpoint['iteration']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
