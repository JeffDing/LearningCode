{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四章 基础实战——FashionMNIST时装分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fashion-mnist-sprite.png)\n",
    "\n",
    "经过前面三章内容的学习，我们完成了以下的内容：  \n",
    "- 对PyTorch有了初步的认识\n",
    "- 学会了如何安装PyTorch以及对应的编程环境\n",
    "- 学习了PyTorch最核心的理论基础（张量&自动求导）\n",
    "- 梳理了利用PyTorch完成深度学习的主要步骤和对应实现方式  \n",
    "  \n",
    "现在，我们通过一个基础实战案例，将第一部分所涉及的PyTorch入门知识串起来，便于大家加深理解。同时为后续的进阶学习打好基础。 \n",
    "  \n",
    "我们这里的任务是对10个类别的“时装”图像进行分类，使用FashionMNIST数据集（https://github.com/zalandoresearch/fashion-mnist ）。上图给出了FashionMNIST中数据的若干样例图，其中每个小图对应一个样本。  \n",
    "FashionMNIST数据集中包含已经预先划分好的训练集和测试集，其中训练集共60,000张图像，测试集共10,000张图像。每张图像均为单通道黑白图像，大小为32\\*32pixel，分属10个类别。  \n",
    "  \n",
    "下面让我们一起将第三章各部分内容逐步实现，来跑完整个深度学习流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**安装pytorch**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先导入必要的包**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**配置训练环境和超参数**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置GPU，这里有两种方式\n",
    "## 方案一：使用os.environ\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## 配置其他超参数，如batch_size, num_workers, learning rate, 以及总的epochs\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "lr = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据读入和加载**  \n",
    "这里同时展示两种方式:  \n",
    "- 下载并使用PyTorch提供的内置数据集  \n",
    "- 从网站下载以csv格式存储的数据，读入并转成预期的格式    \n",
    "第一种数据读入方式只适用于常见的数据集，如MNIST，CIFAR10等，PyTorch官方提供了数据下载。这种方式往往适用于快速测试方法（比如测试下某个idea在MNIST数据集上是否有效）  \n",
    "第二种数据读入方式需要自己构建Dataset，这对于PyTorch应用于自己的工作中十分重要  \n",
    "  \n",
    "同时，还需要对数据进行必要的变换，比如说需要将图片统一为一致的大小，以便后续能够输入网络训练；需要将数据格式转为Tensor类，等等。\n",
    "  \n",
    "这些变换可以很方便地借助torchvision包来完成，这是PyTorch官方用于图像处理的工具库，上面提到的使用内置数据集的方式也要用到。PyTorch的一大方便之处就在于它是一整套“生态”，有着官方和第三方各个领域的支持。这些内容我们会在后续课程中详细介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先设置数据变换\n",
    "from torchvision import transforms\n",
    "\n",
    "image_size = 28\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    #transforms.ToPILImage(),   # 这一步取决于后续的数据读取方式，如果使用内置数据集则不需要\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 读取方式一：使用torchvision自带数据集，下载可能需要一段时间\n",
    "from torchvision import datasets\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='./', train=True, download=True, transform=data_transform)\n",
    "test_data = datasets.FashionMNIST(root='./', train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 读取方式二：读入csv格式的数据，自行构建Dataset类\n",
    "class FMDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.images = df.iloc[:,1:].values.astype(np.uint8)\n",
    "        self.labels = df.iloc[:, 0].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28,28,1)\n",
    "        label = int(self.labels[idx])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image/255., dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "train_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_test.csv\")\n",
    "train_data = FMDataset(train_df, data_transform)\n",
    "test_data = FMDataset(test_df, data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在构建训练和测试数据集完成后，需要定义DataLoader类，以便在训练和测试时加载数据  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入后，我们可以做一些数据可视化操作，主要是验证我们读入的数据是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f731c8b49b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = next(iter(train_loader))\n",
    "print(image.shape, label.shape)\n",
    "plt.imshow(image[0][0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型设计**  \n",
    "由于任务较为简单，这里我们手搭一个CNN，而不考虑当下各种模型的复杂结构  \n",
    "模型构建完成后，将模型放到GPU上用于训练  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*4*4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = self.fc(x)\n",
    "        # x = nn.functional.normalize(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model = model.cuda()\n",
    "# model = nn.DataParallel(model).cuda()   # 多卡训练时的写法，之后的课程中会进一步讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设定损失函数**  \n",
    "使用torch.nn模块自带的CrossEntropy损失  \n",
    "PyTorch会自动把整数型的label转为one-hot型，用于计算CE loss  \n",
    "这里需要确保label是从0开始的，同时模型不加softmax层（使用logits计算）,这也说明了PyTorch训练中各个部分不是独立的，需要通盘考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss(weight=[1,1,1,1,3,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.\n",
       "\n",
       "It is useful when training a classification problem with `C` classes.\n",
       "If provided, the optional argument :attr:`weight` should be a 1D `Tensor`\n",
       "assigning weight to each of the classes.\n",
       "This is particularly useful when you have an unbalanced training set.\n",
       "\n",
       "The `input` is expected to contain scores for each class.\n",
       "\n",
       "`input` has to be a Tensor of size either :math:`(minibatch, C)` or\n",
       ":math:`(minibatch, C, d_1, d_2, ..., d_K)`\n",
       "with :math:`K \\geq 2` for the `K`-dimensional case (described later).\n",
       "\n",
       "This criterion expects a class index (0 to `C-1`) as the\n",
       "`target` for each value of a 1D tensor of size `minibatch`\n",
       "\n",
       "The loss can be described as:\n",
       "\n",
       ".. math::\n",
       "    \\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
       "                   = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\n",
       "\n",
       "or in the case of the `weight` argument being specified:\n",
       "\n",
       ".. math::\n",
       "    \\text{loss}(x, class) = weight[class] \\left(-x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\\right)\n",
       "\n",
       "The losses are averaged across observations for each minibatch.\n",
       "\n",
       "Can also be used for higher dimension inputs, such as 2D images, by providing\n",
       "an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 2`,\n",
       "where :math:`K` is the number of dimensions, and a target of appropriate shape\n",
       "(see below).\n",
       "\n",
       "\n",
       "Args:\n",
       "    weight (Tensor, optional): a manual rescaling weight given to each class.\n",
       "        If given, has to be a Tensor of size `C`\n",
       "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
       "        the losses are averaged over each loss element in the batch. Note that for\n",
       "        some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
       "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
       "        when reduce is ``False``. Default: ``True``\n",
       "    ignore_index (int, optional): Specifies a target value that is ignored\n",
       "        and does not contribute to the input gradient. When `size_average` is\n",
       "        ``True``, the loss is averaged over non-ignored targets.\n",
       "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
       "        losses are averaged or summed over observations for each minibatch depending\n",
       "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
       "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
       "    reduction (string, optional): Specifies the reduction to apply to the output:\n",
       "        'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n",
       "        'mean': the sum of the output will be divided by the number of\n",
       "        elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
       "        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
       "        specifying either of those two args will override :attr:`reduction`. Default: 'mean'\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C)` where `C = number of classes`, or\n",
       "        :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 2`\n",
       "        in the case of `K`-dimensional loss.\n",
       "    - Target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\n",
       "        :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 2` in the case of\n",
       "        K-dimensional loss.\n",
       "    - Output: scalar. If reduce is ``False``, then the same size\n",
       "        as the target: :math:`(N)`, or\n",
       "        :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 2` in the case\n",
       "        of K-dimensional loss.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> loss = nn.CrossEntropyLoss()\n",
       "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
       "    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
       "    >>> output = loss(input, target)\n",
       "    >>> output.backward()\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages/torch/nn/modules/loss.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?nn.CrossEntropyLoss # 这里方便看一下weighting等策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设定优化器**  \n",
    "这里我们使用Adam优化器  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练和测试（验证）**  \n",
    "各自封装成函数，方便后续调用  \n",
    "关注两者的主要区别：  \n",
    "- 模型状态设置  \n",
    "- 是否需要初始化优化器\n",
    "- 是否需要将loss传回到网络\n",
    "- 是否需要每步更新optimizer  \n",
    "  \n",
    "此外，对于测试或验证过程，可以计算分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):       \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    gt_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            gt_labels.append(label.cpu().data.numpy())\n",
    "            pred_labels.append(preds.cpu().data.numpy())\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "    val_loss = val_loss/len(test_loader.dataset)\n",
    "    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)\n",
    "    acc = np.sum(gt_labels==pred_labels)/len(pred_labels)\n",
    "    print('Epoch: {} \\tValidation Loss: {:.6f}, Accuracy: {:6f}'.format(epoch, val_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.663844\n",
      "Epoch: 1 \tValidation Loss: 0.457308, Accuracy: 0.835200\n",
      "Epoch: 2 \tTraining Loss: 0.419207\n",
      "Epoch: 2 \tValidation Loss: 0.369370, Accuracy: 0.868100\n",
      "Epoch: 3 \tTraining Loss: 0.360339\n",
      "Epoch: 3 \tValidation Loss: 0.347108, Accuracy: 0.873100\n",
      "Epoch: 4 \tTraining Loss: 0.327296\n",
      "Epoch: 4 \tValidation Loss: 0.313740, Accuracy: 0.884800\n",
      "Epoch: 5 \tTraining Loss: 0.305447\n",
      "Epoch: 5 \tValidation Loss: 0.301968, Accuracy: 0.890400\n",
      "Epoch: 6 \tTraining Loss: 0.290955\n",
      "Epoch: 6 \tValidation Loss: 0.293701, Accuracy: 0.891400\n",
      "Epoch: 7 \tTraining Loss: 0.274523\n",
      "Epoch: 7 \tValidation Loss: 0.273335, Accuracy: 0.900000\n",
      "Epoch: 8 \tTraining Loss: 0.263639\n",
      "Epoch: 8 \tValidation Loss: 0.278038, Accuracy: 0.899200\n",
      "Epoch: 9 \tTraining Loss: 0.253378\n",
      "Epoch: 9 \tValidation Loss: 0.259808, Accuracy: 0.902900\n",
      "Epoch: 10 \tTraining Loss: 0.240073\n",
      "Epoch: 10 \tValidation Loss: 0.259348, Accuracy: 0.906000\n",
      "Epoch: 11 \tTraining Loss: 0.233204\n",
      "Epoch: 11 \tValidation Loss: 0.252927, Accuracy: 0.908000\n",
      "Epoch: 12 \tTraining Loss: 0.223957\n",
      "Epoch: 12 \tValidation Loss: 0.244853, Accuracy: 0.910000\n",
      "Epoch: 13 \tTraining Loss: 0.218368\n",
      "Epoch: 13 \tValidation Loss: 0.240558, Accuracy: 0.912600\n",
      "Epoch: 14 \tTraining Loss: 0.211391\n",
      "Epoch: 14 \tValidation Loss: 0.230351, Accuracy: 0.918300\n",
      "Epoch: 15 \tTraining Loss: 0.199662\n",
      "Epoch: 15 \tValidation Loss: 0.228290, Accuracy: 0.918800\n",
      "Epoch: 16 \tTraining Loss: 0.197178\n",
      "Epoch: 16 \tValidation Loss: 0.232498, Accuracy: 0.914800\n",
      "Epoch: 17 \tTraining Loss: 0.191481\n",
      "Epoch: 17 \tValidation Loss: 0.228378, Accuracy: 0.917300\n",
      "Epoch: 18 \tTraining Loss: 0.186178\n",
      "Epoch: 18 \tValidation Loss: 0.240717, Accuracy: 0.912000\n",
      "Epoch: 19 \tTraining Loss: 0.180610\n",
      "Epoch: 19 \tValidation Loss: 0.219800, Accuracy: 0.920100\n",
      "Epoch: 20 \tTraining Loss: 0.177247\n",
      "Epoch: 20 \tValidation Loss: 0.229094, Accuracy: 0.916400\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    val(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型保存**  \n",
    "训练完成后，可以使用torch.save保存模型参数或者整个模型，也可以在训练过程中保存模型  \n",
    "这部分会在后面的课程中详细介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./FahionModel.pkl\"\n",
    "torch.save(model, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
